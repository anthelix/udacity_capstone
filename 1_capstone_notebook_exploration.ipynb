{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* [Step 1: Scope the Project and Gather Data](#Step-1:-Scope-the-Project-and-Gather-Data)\n",
    "* [Step 2: Explore and Assess the Data](#Step-2:-Explore-and-Assess-the-Data)\n",
    "    * [I94 Description Labels](#I94-Descrition-Labels)\n",
    "    * [Immigration data](#Immigration-data)\n",
    "    * [Global Land Temperature Data](#Global-Land-Temperature-Data)\n",
    "    * [Global Airports Data](#Global-Airports-Data)\n",
    "    * [Airports Data](#Airports-Data)\n",
    "    \n",
    "* [Step 3: Define the Data Model](#Step-3:-Define-the-Data-Model)\n",
    "* [Step 4: Run ETL to Model the Data](#Step-4:-Run-ETL-to-Model-the-Data)\n",
    "* [Step 5: Complete Project Write Up](#Step-5:-Complete-Project-Write-Up)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les bases de données multidimensionnelles considèrent chaque attribut d’une donnée comme une dimension « séparée ». Le logiciel peut ensuite localiser l’intersection des dimensions et les afficher. Il est ainsi possible d’analyser et de comparer les données de différentes façons. Les attributs peuvent aussi être séparés en plusieurs sous-attributs. Les bases de données multi-dimensionnelles s’opposent aux bases de données relationnelles à deux dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Finish here yesterday](#workflow-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!{sys.executable} -m pip install -U ray\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "pd.set_option(\"display.precision\", 2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "# set AWS variables\n",
    "os.environ['AWS_ACCESS_KEY_ID']    = config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['SECRET']\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "download_url = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/nba-elo/nbaallelo.csv\"\n",
    "target_csv_path = \"nba_all_elo.csv\"\n",
    "\n",
    "response = requests.get(download_url)\n",
    "response.raise_for_status()    # Check that the request was successful\n",
    "with open(target_csv_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "print(\"Download ready.\")\n",
    ">>> import pandas as pd\n",
    ">>> nba = pd.read_csv(\"nba_all_elo.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1,3G\r\n",
      "510M airports_us.csv\r\n",
      "509M GlobalLandTemperaturesByCity.csv\r\n",
      "205M WDIData.csv\r\n",
      "5,8M airport-codes_csv.csv\r\n",
      "1,5M airports-extended.csv\r\n",
      "248K us-cities-demographics.csv\r\n",
      "144K immigration_data_sample.csv\r\n",
      " 36K I94_SAS_Labels_Descriptions.SAS\r\n",
      " 12K i94port.csv\r\n",
      "8,0K i94cit_i94res.csv\r\n",
      "4,0K ./\r\n",
      "4,0K ../\r\n",
      "4,0K 20-years-us-university-dataset/\r\n",
      "4,0K airline-delay-and-cancellation-data-2009-2018/\r\n",
      "4,0K education-statistics/\r\n",
      "4,0K sas_data/\r\n",
      "4,0K i94addr.csv\r\n",
      "4,0K i94mode.csv\r\n",
      "4,0K i94visa.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Read in the data here\n",
    "!ls -1FSash ./dataset\n",
    "#!ls -tRFh ./dataset/\n",
    "path = './dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope TODO\n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "> The main dataset includes data on immigration to the United State, and other datasets. In this work book, the data is transforming and cleasning.  \n",
    "> How many students arrived in US in April?  \n",
    "> Which Airline bring the most student in April?  \n",
    "> What are the top city for alien studies?    \n",
    "> what are the student profils (age, country born, country indicators)?  \n",
    "\n",
    "\n",
    "### Describe and Gather Data\n",
    "\n",
    "[Datactionnary](2_data_dictionnary.ipynb) is provided a dictionnary abou dataset and tables used.\n",
    "\n",
    "\n",
    "**change name _immigration_data_sample.csv_ for  _data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat_**\n",
    "\n",
    "#### I94 Immigration data  Description: \n",
    "Each line of immigration_data_sample.csv correspond to a record of I-94 Form from the U.S. immigration officers. It's provide information about Arrival/Departure to foreign visitors. Some explanation about the [Visitor Arrivals Program (I-94 Form)](https://travel.trade.gov/research/programs/i94/description.asp).\n",
    "\n",
    "Dataset information: There is a file per month for 2016, storage format is sas7bdat. These records are described according to 28 variables.   \n",
    "A small description is provided [here](2_data_dictionnary.ipynb)  \n",
    "I keep this variables for this project:\n",
    "    \n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**cicid**|     ID uniq per record in the dataset | 4.08e+06 | float64\n",
    "**i94yr**|     4 digit year  | 2016.0 | float64\n",
    "**i94mon**|    Numeric month |  4.0 | float64      \n",
    "**i94cit**|     3 digit code of source city for immigration (Born country) | 209.0 | float64\n",
    "**i94res**|    3 digit code of source country for immigration\n",
    "**i94port**|   Port addmitted through | HHW | object\n",
    "**arrdate**|   Arrival date in the USA | 20566.0 | float64\n",
    "**i94mode**|   Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) | 1.0 | float\n",
    "**i94addr**|   State of arrival | HI | object\n",
    "**i94bir**|    Age in years | 61.0 | float\n",
    "**i94visa**|   Visa Code - 1 = Business / 2 = Pleasure / 3 = Student |2.0 | float\n",
    "**dtadfile**|  Date Field in I94 files |20160422| int 64\n",
    "**admnum**|    Admission number, should be unique and not nullable |5.66e+10| float\n",
    "**gender**|    Gender|M| object\n",
    "**visatype**|  Class of admission legally admitting the non-immigrant to temporarily stay in U.S.|WT|object\n",
    "\n",
    "\n",
    "Additional files of this dataset are provide to give more desciption about this dataset\n",
    "\n",
    "\n",
    "#### I94 Description Labels  Description\n",
    "The I94_SAS_Labels_Description.SAS file is provide to add explanations  about code used in _data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat._ \n",
    "I parse this file, save the result in 5 .csv files. \n",
    "    * i94visa Data\n",
    "    * i94country and i94residence Data\n",
    "    * i94port Data\n",
    "    * i94mode Data\n",
    "    * i94addr\n",
    "A small description is provided [here](2_data_dictionnary.ipynb)\n",
    "\n",
    "####  Global Land Temperature Data  Description\n",
    "The Berkeley Earth Surface Temperature Study provide climate information. Each line correspond to a record of temperature per day from city around the world. \n",
    "Dataset information: the GlobalLandTemperaturesByCity.csv has 7 variables.    \n",
    "A small description is provided [here](2_data_dictionnary.ipynb)\n",
    "I keep this variables for this project:\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**dt**|Date format YYYY-MM-DD| 1743-11-01| object\n",
    "**AverageTemperature**|Average Temperature for the city to th date dt|6.07|float64\n",
    "**City**| City name| Århus| object\n",
    "**Country**| Country name | Denmark | object\n",
    "\n",
    "#### Global Airports Data\n",
    "This is a database of airports, train stations, and ferry terminals around the world. Some of the data come from public sources and some of it comes from OpenFlights.org user contributions\n",
    "Dataset information: \n",
    "A small description is provided [here](2_data_dictionnary.ipynb)\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "\n",
    "\n",
    "#### Airports Data Description\n",
    "The airport code refers to the IATA airport code, 3 letters code unique for all airports in the world. It's a code used in passenger reservation, ticket and baggage-handling too. \n",
    "Dataset information: The airport-codes_csv.csv provides informations about aiports and have 12 variables.    \n",
    "A small description is provided [here](2_data_dictionnary.ipynb)\n",
    "I keep this variables for this project:\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**ident**| Unique identifier Airport code| 00AK| object \n",
    "**type**| Type of airport | small_airport |object\n",
    "**name**| Name of the airport | Lowell Field | object\n",
    "**continent**| Continent | | object\n",
    "**iso_country**| ISO code of airport country |US| object\n",
    "**iso_region**| ISO code of the region airport | US-KS|object\n",
    "**municipality**| City name where the airport is located | Anchor Point|object\n",
    "**iata_code**| IATA code of the airport| | object\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df(df, description):\n",
    "    nRow, nCol = df.shape\n",
    "    print(\"There are {} rows and {} columns in **** {}. ****\".format(nRow, nCol, description))\n",
    "    #print(df.head(3))\n",
    "    #print(n_df.info())\n",
    "    #print(df.nunique)\n",
    "    # check null value\n",
    "    print(       )\n",
    "    print(\"---------   Check null values\")\n",
    "    tab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\n",
    "    tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "    tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100).T.rename(index={0:'null values (%)'}))\n",
    "    display(tab_info)\n",
    "    print(       )\n",
    "    print(\"---------   Check unique values\")\n",
    "    cols = [col for col in df.columns if df[col].isnull().any()]\n",
    "    df_miss = df[cols]    \n",
    "    display(pd.DataFrame(df.nunique()).T.rename(index={0:'Unique values in columns'}))\n",
    "    #print(df_miss.head())\n",
    "    print(       )\n",
    "    print(\"---------   Check duplicated value\")\n",
    "    n_df = df.iloc[:, 2:]\n",
    "    df_dup = df[df.duplicated(keep=False)]\n",
    "    display(pd.DataFrame(df_dup.count()).T.rename(index={0:'Duplicate values in columns'}))\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I94_SAS_Labels_Description.SAS\n",
    "#def SAS_parser(file_parse, item, columns):\n",
    "import re\n",
    "import io\n",
    "\n",
    "def parse_file(path_file, key):\n",
    "    \"\"\"\n",
    "    fonction to parse file and create csv file\n",
    "    return dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    file_parse = path+'I94_SAS_Labels_Descriptions.SAS'\n",
    "    with open(file_parse, 'r') as f:\n",
    "        file = f.read()\n",
    "    sas_dict={}\n",
    "    key_name = ''\n",
    "\n",
    "    for line in file.split(\"\\n\"):\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        if '/* I94' in line :         \n",
    "            line = line.strip('/* ')\n",
    "            key_name = line.split('-')[0].replace(\"&\", \"_\").replace(\" \", \"\").strip(\" \").lower() \n",
    "            sas_dict[key_name] = []\n",
    "        elif '=' in line and key_name != '' :\n",
    "            #line_trans = re.sub(\"([A-Z]*?),(\\s*?[A-Z]{2}\\s)\",\"\\\\1=\\\\2\", line)\n",
    "            #print(line_trans)\n",
    "            sas_dict[key_name].append([item.strip(' ').strip(\" ';\") for item in line.split('=')])\n",
    "        \n",
    "\n",
    "    if key is \"i94port\":\n",
    "        columns = [\"Port_id\", \"Port_city\", \"State_id\"]\n",
    "        swap = sas_dict[key]\n",
    "        sas_dict[key] = []\n",
    "        for x in swap:\n",
    "            if \",\" in x[1]:\n",
    "                mylist=[]\n",
    "                a = x[1].rsplit(\",\", 1)\n",
    "                b = a[0]\n",
    "                c = a[1].strip()\n",
    "                mylist.extend([x[0], b, c])\n",
    "                sas_dict[key].append(item for item in mylist)\n",
    "                \n",
    "                \n",
    "    if key is \"i94cit_i94res\":\n",
    "        columns = [\"Country_id\", \"Country\"]       \n",
    "    if key is \"i94mode\":\n",
    "        columns = [\"Mode_id\", \"Mode\"]\n",
    "    if key is \"i94addr\":\n",
    "        columns = [\"State_id\", \"State\"]\n",
    "    if key is \"i94visa\":\n",
    "            columns = [\"Code_visa\", \"Visa\"]\n",
    "    df = \"\"           \n",
    "            \n",
    "\n",
    "    if key in sas_dict.keys():\n",
    "        if len(sas_dict[key]) > 0:\n",
    "            df = pd.DataFrame(sas_dict[key], columns = columns)\n",
    "        with io.open(f\"./dataset/{key}.csv\", \"w\") as f:\n",
    "            df.to_csv(f, index=False) \n",
    "           \n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Description Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = \"path+'I94_SAS_Labels_Descriptions.SAS'\"\n",
    "key = \"i94port\"\n",
    "i94_port = parse_file(path_file, key) \n",
    "i94_port.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = \"path+'I94_SAS_Labels_Descriptions.SAS'\"\n",
    "key = \"i94cit_i94res\"\n",
    "df_i94 = parse_file(path_file, key) \n",
    "df_i94.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = \"path+'I94_SAS_Labels_Descriptions.SAS'\"\n",
    "key = \"i94addr\"\n",
    "df_i94 = parse_file(path_file, key) \n",
    "df_i94.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = \"path+'I94_SAS_Labels_Descriptions.SAS'\"\n",
    "key = \"i94mode\"\n",
    "df_i94 = parse_file(path_file, key) \n",
    "df_i94.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = \"path+'I94_SAS_Labels_Descriptions.SAS'\"\n",
    "key = \"i94visa\"\n",
    "df_i94 = parse_file(path_file, key) \n",
    "df_i94.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "* immigration_data_sample\n",
    "    * revoir l'orignine du fichier. chercher data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat dans udacity workspace\n",
    "    (pd.read_sas(immigration_fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "    ==> 1000 rows fourni par udac, beaucoup plus dans 1 seul mois\n",
    "    * DONE: faire un dictionnaire, recuperer les colonnes. \n",
    "* DONE chercher les valeurs manquantes, les valeurs dupliquees, data cleanning\n",
    "    * chamger les formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRowsRead = None # change and set to None for the whole data\n",
    "description = \"dataset immigration provide by Udacity\"\n",
    "name = \"df_immigration\"\n",
    "file = \"immigration_data_sample.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(path+file, nrows = nRowsRead)\n",
    "\n",
    "df_drop = df_raw.drop([\"count\", \"visapost\", \"occup\", \"entdepa\", \"depdate\", \"entdepd\", \"entdepu\", \"biryear\", \\\n",
    "                       \"dtaddto\", \"matflag\", \"insnum\", \"airline\", \"fltno\"], axis=1)\n",
    "df_immigration = check_df(df_drop, description).sort_values(by = ['cicid', 'admnum'])\n",
    "df_immigration.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing and Duplicate\n",
    "* somme value missing in 1000 rows\n",
    "    * i94addr # US State of arrival,  59 null values and 51 unique values\n",
    "        * map with i94port.csv\n",
    "    * gender  # Gender, 141 null values and 3 unique values \n",
    "        * 'M', nan, 'F', 'X'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration['i94addr'] = df_immigration[\"i94port\"].map(dict(zip(i94_port[\"Port_id\"], i94_port[\"State_id\"]))).fillna(df_immigration.i94addr)\n",
    "df = df_immigration[df_immigration[\"gender\"].isnull()]\n",
    "df_immigration.dropna(inplace = True)\n",
    "df_immigration = check_df(df_drop, description).sort_values(by = ['cicid', 'admnum'])\n",
    "df_immigration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c/c: we have an ID for each record, `admnum` should be none null as `i94addr` for the analytic questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Land Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from kaggle the GlobalLandTemperaturesByCity.csv KAGGLE/UDACITY\n",
    "nRowsRead = None # change and set to None for the whole data\n",
    "description = \"download from kaggle the GlobalLandTemperaturesByCity.csv KAGGLE/UDACITY\"\n",
    "name = \"df_temperature\"\n",
    "file = \"GlobalLandTemperaturesByCity.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "df_raw = df_raw.drop([\"AverageTemperatureUncertainty\"], axis=1)\n",
    "\n",
    "\n",
    "df_temp = check_df(df_raw, description).sort_values(by = [\"dt\", \"Country\",\"City\"], ascending=True)\n",
    "\n",
    "\n",
    "print(\"The date of the first record is {}.\".format(df_temp[\"dt\"].min()))\n",
    "print(\"The date of the first record is {}.\".format(df_temp[\"dt\"].max()))\n",
    "\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing and Duplicate\n",
    "* no duplicate for the whole Temperature dataset\n",
    "* 364130 none value for AverageTemperature, so remove this rows.\n",
    "* begin in 1743 and finis in 2013 so agregation by City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_temp = df_temp.groupby(['Country', 'City']) \\\n",
    "                            .agg({\"AverageTemperature\": \"mean\", \n",
    "                                  \"Latitude\": \"first\", \n",
    "                                  \"Longitude\": \"first\"}).reset_index()\n",
    "                            \n",
    "global_temp.sort_values([\"AverageTemperature\"], ascending=True, inplace=True)\n",
    "df_drop = global_temp.drop([\"Latitude\", \"Longitude\"], axis=1)\n",
    "\n",
    "df_temperature = check_df(df_drop, description).sort_values(by = [\"Country\",\"City\"], ascending=True)\n",
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c/c: Temperature from 1743 to 2013, useful if we want look for raison of immigration. People have needed bad weather conditions for a long time to leave their country**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Airports Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7750 rows and 5 columns in **** download from kaggle the airports-extended.csv. ****\n",
      "\n",
      "---------   Check null values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_name</th>\n",
       "      <th>airport_city</th>\n",
       "      <th>airport_country</th>\n",
       "      <th>airport_iata</th>\n",
       "      <th>airport_icao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>column type</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null values (nb)</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1665</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null values (%)</th>\n",
       "      <td>0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 airport_name airport_city airport_country airport_iata  \\\n",
       "column type            object       object          object       object   \n",
       "null values (nb)            0           44               0         1665   \n",
       "null values (%)             0         0.57               0           21   \n",
       "\n",
       "                 airport_icao  \n",
       "column type            object  \n",
       "null values (nb)          478  \n",
       "null values (%)           6.2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------   Check unique values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_name</th>\n",
       "      <th>airport_city</th>\n",
       "      <th>airport_country</th>\n",
       "      <th>airport_iata</th>\n",
       "      <th>airport_icao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unique values in columns</th>\n",
       "      <td>7664</td>\n",
       "      <td>6953</td>\n",
       "      <td>237</td>\n",
       "      <td>6085</td>\n",
       "      <td>7272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          airport_name  airport_city  airport_country  \\\n",
       "Unique values in columns          7664          6953              237   \n",
       "\n",
       "                          airport_iata  airport_icao  \n",
       "Unique values in columns          6085          7272  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------   Check duplicated value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_name</th>\n",
       "      <th>airport_city</th>\n",
       "      <th>airport_country</th>\n",
       "      <th>airport_iata</th>\n",
       "      <th>airport_icao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Duplicate values in columns</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             airport_name  airport_city  airport_country  \\\n",
       "Duplicate values in columns            13            13               13   \n",
       "\n",
       "                             airport_iata  airport_icao  \n",
       "Duplicate values in columns             0             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_name</th>\n",
       "      <th>airport_city</th>\n",
       "      <th>airport_country</th>\n",
       "      <th>airport_iata</th>\n",
       "      <th>airport_icao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goroka Airport</td>\n",
       "      <td>Goroka</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>GKA</td>\n",
       "      <td>AYGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Madang Airport</td>\n",
       "      <td>Madang</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>MAG</td>\n",
       "      <td>AYMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mount Hagen Kagamuga Airport</td>\n",
       "      <td>Mount Hagen</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>HGU</td>\n",
       "      <td>AYMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nadzab Airport</td>\n",
       "      <td>Nadzab</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>LAE</td>\n",
       "      <td>AYNZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Port Moresby Jacksons International Airport</td>\n",
       "      <td>Port Moresby</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>POM</td>\n",
       "      <td>AYPY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  airport_name  airport_city  \\\n",
       "0                               Goroka Airport        Goroka   \n",
       "1                               Madang Airport        Madang   \n",
       "2                 Mount Hagen Kagamuga Airport   Mount Hagen   \n",
       "3                               Nadzab Airport        Nadzab   \n",
       "4  Port Moresby Jacksons International Airport  Port Moresby   \n",
       "\n",
       "    airport_country airport_iata airport_icao  \n",
       "0  Papua New Guinea          GKA         AYGA  \n",
       "1  Papua New Guinea          MAG         AYMD  \n",
       "2  Papua New Guinea          HGU         AYMH  \n",
       "3  Papua New Guinea          LAE         AYNZ  \n",
       "4  Papua New Guinea          POM         AYPY  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download from kaggle the airports-extended.csv KAGGLE\n",
    "nRowsRead = None # change and set to None for the whole data\n",
    "description = \"download from kaggle the airports-extended.csv\"\n",
    "name = \"df_global_airports\"\n",
    "file = \"airports-extended.csv\"\n",
    "\n",
    "df_raw = (pd.read_csv(path+file, \n",
    "                     nrows = nRowsRead,\n",
    "                     names=['id', 'name', 'city', 'country', 'iata', 'icao', 'latitude', 'longitude', 'altitude', \n",
    "                            'timezone', 'dst', 'tz_timezone', 'type', 'data_source'],\n",
    "                     na_values=['\\\\N', '-', 'NAN', 'unknown'])\n",
    "           .set_index(\"id\")[lambda df: df.type == 'airport']\n",
    "           .reset_index(drop=True)\n",
    "           .drop(columns=['type', 'timezone', 'tz_timezone', 'data_source', 'dst', 'latitude', 'longitude', 'altitude'])\n",
    "           .rename(columns=lambda col:'airport_'+ col)\n",
    "        )\n",
    "#df_raw = df_raw.drop([\"AverageTemperatureUncertainty\"], axis=1)\n",
    "\n",
    "df_global_airports = check_df(df_raw, description)\n",
    "#.sort_values(by = [\"dt\", \"Country\",\"City\"], ascending=True)\n",
    "\n",
    "\n",
    "df_global_airports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%who_ls DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel global_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### workflow-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airport-codes_csv UDACITY\n",
    "nRowsRead = None # change and set to None for the whole data\n",
    "description = \"airport-codes_csv provide by UDACITY\"\n",
    "name = \"df_airport_world\"\n",
    "file = \"airport-codes_csv.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "df_raw = df_raw.drop([\"elevation_ft\", \"continent\",\"gps_code\", \"local_code\", \"coordinates\"], axis=1)\n",
    "\n",
    "df_airports = check_df(df_raw, description).sort_values(by = [\"iata_code\"], ascending=True)\n",
    "df_airports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing and Duplicate\n",
    "* no duplicate for the whole Airports dataset\n",
    "* column `ident` has no missing value and unique.\n",
    "* none value 45886 in iata_code\n",
    "* Type port are 'small_airport' 'medium_airport' 'large_airport' 'closed' 'seaplane_base' 'balloonport' do decide too drop ballonport, heliport and closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = df_airports[\"type\"].unique()\n",
    "#print(unique)\n",
    "indexNames = df_airports[df_airports['type'].str.contains(r'\\bheliport\\b' or 'closed' or 'ballonport')].index\n",
    "df_airports.drop(indexNames , inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_airports.loc[df_airports[\"iata_code\"].notnull(), [\"type\", \"iata_code\"]]\n",
    "df_airports['i94addr'] = df_immigration[\"i94port\"].map(dict(zip(i94_port[\"Port_id\"], i94_port[\"State_id\"]))).fillna(df_immigration.i94addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c/c : It seems to have no data in common apart from the regions of the united states with the 1st dataset.The columns `ident` contains unique value for airport, digit letter with zero before, sometimes 1 or 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Cities Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* from the US census bureau's\n",
    "* demographics of all US cities > 65 000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us-cities-demographics USACITY\n",
    "nRowsRead = None # change and set to None for the whole data\n",
    "description   = \"us-cities-demographics provide by UDACITY\"\n",
    "name          = \"df_demograph\"\n",
    "file          = \"us-cities-demographics.csv\"\n",
    "\n",
    "n_df          = pd.read_csv(path+file, sep=\";\", nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_demograph  = n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_df.info())\n",
    "n_df.head(10)\n",
    "# dic_5 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### missing and duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup = n_df[n_df.duplicated(keep=False)].sort_values(\"City\")\n",
    "df_dup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in n_df.columns if n_df[col].isnull().any()]\n",
    "df_miss = n_df[cols]\n",
    "df_miss.head()\n",
    "df_miss[pd.isnull(df_miss).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateRowsDF = n_df[n_df.duplicated()]\n",
    "duplicateRowsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airports_us.csv KAGGLE\n",
    "nRowsRead = 1000 # change and set to None for the whole data\n",
    "description = \" Dataset from KAGGLE, usefule to follow where go aliens\"\n",
    "name = \"df_airport_us\"\n",
    "file = \"airports_us.csv\"\n",
    "n_df = pd.read_csv(path+file, nrows = nRowsRead)\n",
    "nRow, nCol = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, file))\n",
    "df_airport_us = n_df\n",
    "df_airport_us.head()\n",
    "#print(n_df.head(1))\n",
    "#print(n_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_df.sort_values([\"Fly_date\"], ascending=True, inplace=True)\n",
    "print(n_df[\"Fly_date\"].min())\n",
    "print(n_df[\"Fly_date\"].max())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "global_temp = df_temp.groupby(['Country']) \\\n",
    "                            .agg({\"AverageTemperature\": \"mean\", \n",
    "                                  \"Latitude\": \"first\", \n",
    "                                  \"Longitude\": \"first\"}).reset_index()\n",
    "                            \n",
    "global_temp.sort_values([\"AverageTemperature\"], ascending=True, inplace=True)\n",
    "global_temp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WDIData.csv Indicators developpement KAGGLE\n",
    "description      = \"WDIData.csv country Indicators developpment KAGGLE\"\n",
    "name             = \"df_indicator_dev\"\n",
    "file             = \"WDIData.csv\"\n",
    "\n",
    "n_df             = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol       = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_indicator_dev = n_df\n",
    "n_df.head()\n",
    "#dic_3 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./dataset/airline-delay-and-cancellation-data-2009-2018/2016.csv\n",
    "nRowsRead = 1000 # change and set to None for the whole data\n",
    "description   = \".Data about us flight in 2016\"\n",
    "name          = \"df_airline_delay\"\n",
    "file          = \"airline-delay-and-cancellation-data-2009-2018/2016.csv\"\n",
    "\n",
    "n_df          = pd.read_csv(path+file, nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_airline_delay       = n_df\n",
    "n_df.head()\n",
    "\n",
    "#dic_10 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_10\n",
    "#type(dic_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /dataset/education-statistics \n",
    "\n",
    "description           = \"Data from education-statistics\"\n",
    "name                  = \"df_Educ_country_series\"\n",
    "file                  = \"education-statistics/EdStatsCountry-Series.csv\"\n",
    "n_df                  = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol            = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_country_series = n_df\n",
    "dic_11 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_11\n",
    "\n",
    "description   = \"Data from education-statistics\"\n",
    "name          = \"df_Educ_country\"\n",
    "file          = \"education-statistics/EdStatsCountry.csv\"\n",
    "n_df          = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_country = n_df\n",
    "dic_12 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_12\n",
    "\n",
    "description   = \"Data from education-statistics\"\n",
    "name          = \"df_Educ_data\"\n",
    "file          = \"education-statistics/EdStatsData.csv\"\n",
    "n_df          = pd.read_csv(path+file, nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_data  = n_df\n",
    "dic_13 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_13\n",
    "\n",
    "description       = \"Data from education-statistics\"\n",
    "name              = \"df_Educ_foot_note\"\n",
    "file              = \"education-statistics/EdStatsFootNote.csv\"\n",
    "n_df              = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol        = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_foot_note = n_df\n",
    "dic_14 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_14\n",
    "\n",
    "description    = \"Data from education-statistics\"\n",
    "name           = \"df_Educ_series\"\n",
    "file           = \"education-statistics/EdStatsSeries.csv\"\n",
    "n_df           = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol     = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_series = n_df\n",
    "dic_15 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%xdel n_df\n",
    "%who_ls dict\n",
    "%who_ls DataFrame\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TOOLS BOX\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "# some information about data loaded\n",
    "df = pd.read_csv('../input/<>.csv', low_memory=False)\n",
    "print('Dataframe dimensions:', df.shape)\n",
    "#____________________________________________________________\n",
    "# gives some infos on columns types and number of null values\n",
    "tab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100)\n",
    "                         .T.rename(index={0:'null values (%)'}))\n",
    "tab_info\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# airport-codes_csv UDACITY\n",
    "nRowsRead = None # change and set to None for the whole data\n",
    "description      = \"airport-codes_csv provide by UDACITY\"\n",
    "name             = \"df_airport_world\"\n",
    "file             = \"airport-codes_csv.csv\"\n",
    "n_df             = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol       = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_airport_world = n_df\n",
    "#dic_4 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "\n",
    "print(n_df.info())\n",
    "n_df.head(10)\n",
    "----------------------------------------\n",
    "\n",
    "### Data dictionary\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "----------------------------------------\n",
    "# RETROUVER UNE CHAINE DE CARACTERE\n",
    "n_df[n_df['iata_code'].str.contains(r'\\b00A\\b')]\n",
    "----------------------------------------\n",
    "# PRETTY PRINT\n",
    "n_df = pd.read_csv(path+file, nrows = nRowsRead)\n",
    "nRow, nCol = n_df.shape\n",
    "print(\"There are {} rows and {} columns in {}.\".format(nRow, nCol, description))\n",
    "df_immigration = n_d\n",
    "----------------------------------------\n",
    "# DATAFRAME AVEC LES COLNNES AVEC NULL VALUE\n",
    "cols = [col for col in n_df.columns if n_df[col].isnull().any()]\n",
    "df_miss = n_df[cols]\n",
    "df_miss.nunique()\n",
    "df_miss[pd.isnull(df_miss).any(axis=1)]\n",
    "----------------------------------------\n",
    "df.apply(lambda row: row.astype(str).str.contains('TEST').any(), axis=1)\n",
    "----------------------------------------\n",
    "n = n_df.dropna()\n",
    "----------------------------------------\n",
    "df = n_df[n_df.apply(lambda x: x.astype(str).str.contains(r'\\bHHW\\b')).any(axis=1)]\n",
    "----------------------------------------\n",
    "duplicateRowsDF = n_df[n_df.duplicated()]\n",
    "----------------------------------------\n",
    "#preplace null value in dataframe from an other dataframe\n",
    "#df2['B']=df2['A'].map(dict(zip(df1['A'],df1['B']))).fillna(df2.B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%whos DataFrame\n",
    "all_df = %who_ls DataFrame\n",
    "all_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_sas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_indicator_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_country_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_foot_note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_df_demograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_airport_world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_airport_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_airline_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df_airline_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_airline_delay.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_airline_delay.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Engineering Capstone Project\n",
    "\n",
    "# US Student Immigration Part 3\n",
    "> The purpose of this project is to study the foreign students. The goal is to offer Data teams Analysts a selection of data concerning immigration to the United States.\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* [Step 1: Scope the Project and Gather Data](#Step-1:-Scope-the-Project-and-Gather-Data)\n",
    "\n",
    "* [Step 2: Explore and Assess the Data](#Step-2:-Explore-Assess-the-Data) \n",
    "\n",
    "* [Step 3: Define the Data Model](#Step-3:-Define-the-Data-Model)\n",
    "* [Step 4: Run ETL to Model the Data](#Step-4:-Run-ETL-to-Model-the-Data)\n",
    "* [Step 5: Complete Project Write Up](#Step-5:-Complete-Project-Write-Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType, StringType, DecimalType\n",
    "\n",
    "import datetime as dt\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'pyspark.sql.types' in sys.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "\n",
    "Data warehouse allow us to collect, transform and manage data from varied sources. Then, Data Team Business connect to it and analyse data. \n",
    "Apache Spark has been used to gather data\n",
    "Amazon S3 buckets store the data in parquet files for the Data teams.\n",
    "The main dataset includes data on immigration to the United State.\n",
    "The questions about foreign students and their choice to come to US may be useful to propose services.   \n",
    "How many students arrived in US in April?    \n",
    "Which Airline bring the most student in April?    \n",
    "What are the top city to arrive in the USA?   \n",
    "Where are from?   \n",
    "what are the student profils (age, country born, country indicators)? \n",
    "\n",
    "The main dataset includes data on immigration to the United State, and other datasets. In this work book, the data is transforming and cleasning.  \n",
    "\n",
    "\n",
    "### Describe and Gather Data\n",
    "\n",
    "[Data dictionnary](2_data_dictionnary.ipynb) provide informations abou dataset and tables used.\n",
    "\n",
    "#### Data Source\n",
    "\n",
    "Data |File |Data Source\n",
    "-|-|-|\n",
    "I94 Immigration | immigration_data_sample.csv| [US National Tourism and Trade Office](https://travel.trade.gov/research/programs/i94/description.asp)\n",
    "I94 Description Labels  Description|I94_SAS_Labels_Descriptions.SAS |US National Tourism and Trade Office\n",
    "Global Land Temperature|GlobalLandTemperaturesByCity.csv| [Berkeley Earth](http://berkeleyearth.org/)\n",
    "Global Airports|airports-extended.csv| [OpenFlights.org and user contributions](https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals)\n",
    "Airports codes |airport-codes_csv.csv| provide by Udacity\n",
    "Iso country | wikipedia-iso-country-codes.csv|[Kaggle](https://www.kaggle.com/juanumusic/countries-iso-codes)\n",
    "US Cities Demographic| us-cities-demographics.csv|provide by Udacity\n",
    "Indicators developpment| WDIData.csv| [Kaggle](https://www.kaggle.com/xavier14/wdidata)\n",
    "Education-statistics| EdStatsData.csv|provide by Kaggle [World Bank](https://www.kaggle.com/kostya23/worldbankedstatsunarchived) # Edit: not used\n",
    "\n",
    "\n",
    "#### I94 Immigration data  Description: \n",
    "Each line of immigration_data_sample.csv correspond to a record of I-94 Form from the U.S. immigration officers. It's provide information about Arrival/Departure to foreign visitors. Some explanation about the [Visitor Arrivals Program (I-94 Form)](https://travel.trade.gov/research/programs/i94/description.asp).  \n",
    "\n",
    "Dataset information: There is a file per month for 2016, storage format is sas7bdat. These records are described according to 28 variables.   \n",
    "A small description is provided [here](2_data_dictionnary.ipynb)  \n",
    "I keep this variables for this project( _df_immigration_ ):\n",
    "    \n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**cicid**|     ID uniq per record in the dataset | 4.08e+06 | float64\n",
    "**i94yr**|     4 digit year  | 2016.0 | float64\n",
    "**i94mon**|    Numeric month |  4.0 | float64      \n",
    "**i94cit**|     3 digit code of source city for immigration (Born country) | 209.0 | float64\n",
    "**i94res**|    3 digit code of source country for immigration |209.0 | float64\n",
    "**i94port**|   Port addmitted through | HHW | object\n",
    "**arrdate**|   Arrival date in the USA | 20566.0 | float64\n",
    "**i94mode**|   Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) | 1.0 | float\n",
    "**i94addr**|   State of arrival | HI | object\n",
    "**i94bir**|    Age in years | 61.0 | float\n",
    "**i94visa**|   Visa Code - 1 = Business / 2 = Pleasure / 3 = Student |2.0 | float\n",
    "**dtadfile**|  Date Field in I94 files |20160422| int 64\n",
    "**gender**|    Gender|M| object\n",
    "**visatype**|  Class of admission legally admitting the non-immigrant to temporarily stay in U.S.|WT|object\n",
    "**airline**|Airline used to arrive in U.S.|MU|Object\n",
    "\n",
    "\n",
    "\n",
    "df_immigration   \n",
    "Additional files of this dataset are provide to give more desciption about this dataset\n",
    "\n",
    "\n",
    "#### I94 Description Labels  Description\n",
    "The I94_SAS_Labels_Description.SAS file is provide to add explanations  about code used in _data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat._ \n",
    "I parse this file, save the result in 5 .csv files. \n",
    "    * i94visa Data\n",
    "    * i94country and i94residence Data\n",
    "    * i94port Data\n",
    "    * i94mode Data\n",
    "    * i94addr\n",
    "A small description is provided [here](2_data_dictionnary.ipynb)\n",
    "\n",
    "####  Global Land Temperature Data  Description\n",
    "The Berkeley Earth Surface Temperature Study provide climate information. Each line correspond to a record of temperature per day from city around the world.     \n",
    "Dataset information: the GlobalLandTemperaturesByCity.csv has 7 variables. A small description is provided [here](2_data_dictionnary.ipynb). I keep this variables for this project ( _df_temperature_ ):\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**dt**|Date format YYYY-MM-DD| 1743-11-01| object\n",
    "**AverageTemperature**|Average Temperature for the city to th date dt|6.07|float64\n",
    "**City**| City name| Ã…rhus| object\n",
    "**Country**| Country name | Denmark | object\n",
    "\n",
    "#### Global Airports Data\n",
    "This is a database of airports, train stations, and ferry terminals around the world. Some of the data come from public sources and some of it comes from OpenFlights.org user contributions.      \n",
    "Dataset information: A small description is provided [here](2_data_dictionnary.ipynb). I give name and keep this variables ( _df_global_airports_ ):\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**airport_ID**|Id in the table|1| Int\n",
    "**airport_name**|Name of airport|Nadzab Airport|Object\n",
    "**airport_city**|Main city served by airport|Nadzab|Object\n",
    "**airport_country**|Country or territory where airport is located|Papua New Guinea|Object\n",
    "**airport_iata**|3-letter IATA code|LAE|Object\n",
    "\n",
    "\n",
    "#### Airports Data Description\n",
    "The airport code refers to the IATA airport code, 3 letters code unique for all airports in the world. It's a code used in passenger reservation, ticket and baggage-handling too.     \n",
    "Dataset information: The airport-codes_csv.csv provides informations about aiports and have 12 variables. A small description is provided [here](2_data_dictionnary.ipynb). I keep this variables for this project ( _df_airport_code_ ):\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**ident**| Unique identifier Airport code| 00AK| object \n",
    "**type**| Type of airport | small_airport |object\n",
    "**name**| Name of the airport | Lowell Field | object\n",
    "**iso_country**| ISO code of airport country |US| object\n",
    "**iso_region**| ISO code of the region airport | US-KS|object\n",
    "**municipality**| City name where the airport is located | Anchor Point|object\n",
    "**iata_code**| IATA code of the airport| | object\n",
    "\n",
    "#### Iso country\n",
    "This is a database about the different code useful to identify country.        \n",
    "Datasset information: A small description is provided [here](2_data_dictionnary.ipynb). This table gives us informations about Country codes used to identify each country and contains 4 variables. I keep this variables for this project ( _df_iso_country_ ):\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**Country_name**|Country Name in English|Wallis and Futuna|Object\n",
    "**Alpha2_code**|code 2 letter code for the country|WF|Object\n",
    "**Alpha3_code**|code 3 letter code for the country|WLF|Object\n",
    "**Numeric_code**|ISO 3166-2 code|876|Int\n",
    "\n",
    "#### US cities Demographics\n",
    "This dataset contains information about the demographics of all US cities and come from the US Census Bureau.     \n",
    "Dataset information: A small description is provided [here](2_data_dictionnary.ipynb). \n",
    "This dataset contains 12 variables and provides simple informations about us state population. \n",
    "I keep this variables for this project ( _df_demograph_ ):\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**City**|Name of the city|Silver Spring|Object\n",
    "**State**|US state of the city|Maryland|Object\n",
    "**Median Age**|The median of the age of the population|33.8|Float64\n",
    "**Male Population**|Number of the male population|40601.0|Float64\n",
    "**Female Population**|Number of the female population|41862.0|Float64\n",
    "**Total Population**|Number of the total population|82463 \t|Float64\n",
    "**Foreign-born**|Number of residents of the city that were not born in the city|30908.0|Float64\n",
    "**State Code**|Code of the state of the city|MD|Object|\n",
    "**Race**|Race class|Hispanic or Latino|Object\n",
    "**Count**|Number of individual of each race|25924|Int64\n",
    "\n",
    "#### World Development Indicators\n",
    "The primary World Bank collection of development indicators, compiled from officially-recognized international sources. It presents the most current and accurate global development data available, and includes national, regional and global estimates.   \n",
    "Dataset information: This dataset contains 64 variables with economics context , most of which are variables per year(1960 to 2018).\n",
    "A small description is provided [here](2_data_dictionnary.ipynb).\n",
    "I keep this variables for this project ( _df_indicator_dev_ ):\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**Country Name**|Name of the country|Arab World|Object|\n",
    "**Country Code**|3 letters code of country|ARB|Object\n",
    "**Indicator Name**|indicators of economic development|2005 PPP conversion factor, GDP (LCU per inter...|Object\n",
    "**Indicator Code**|letters indicator code|PA.NUS.PPP.05|Object\n",
    "**1960 ...2018**|one column per year since 1960|2018|Float64\n",
    "\n",
    "#### Education statistics Data\n",
    "* Edit: Not used  \n",
    "The primary World Bank collection of development indicators, compiled from officially-recognized international sources. It presents the most current and accurate global development data available, and includes national, regional and global estimates.    \n",
    "Dataset information: This dataset contains 64 variables witheducation context , most of which are variables per year(1970 to 2100).\n",
    "A small description is provided [here](2_data_dictionnary.ipynb).\n",
    "I keep this variables for this project ( _df_Educ_data_ ):\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**Country Name**|Name of the country|Arab World|Object|\n",
    "**Country Code**|3 letters code of country|ARB|Object\n",
    "**Indicator Name**|indicators of education development|Adjusted net enrolment rate, lower secondary, ...|Object\n",
    "**Indicator Code**|letters indicator code|UIS.NERA.2|Object\n",
    "**1970 ...2100**|one column per year since 1970|2018|Float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore and Assess the Data\n",
    "## Explore the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Source\n",
    "\n",
    "[Datactionnary](2_data_dictionnary.ipynb) provides informations about dataset and tables used. [This notebook](1_Exploration_python.ipynb) performs a first exploration with Python and explain the datasets, which variables I kept. \n",
    "Edit: decide not used Education-statistics, i found indicators from Indicators developpment\n",
    "\n",
    "Dataset |File |Data Source|Dataframe Name\n",
    "-|-|-|-|\n",
    "I94 Immigration | immigration_data_sample.csv| [US National Tourism and Trade Office](https://travel.trade.gov/research/programs/i94/description.asp)| df_immigration\n",
    "I94 Description Labels  Description|I94_SAS_Labels_Descriptions.SAS |US National Tourism and Trade Office|\n",
    "Global Land Temperature|GlobalLandTemperaturesByCity.csv| [Berkeley Earth](http://berkeleyearth.org/)|df_temperature\n",
    "Global Airports|airports-extended.csv| [OpenFlights.org and user contributions](https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals)|df_global_airports\n",
    "Airports codes |airport-codes_csv.csv| provide by Udacity|df_airport_code\n",
    "Iso country | wikipedia-iso-country-codes.csv|[Wikipedia](https://gist.github.com/radcliff/f09c0f88344a7fcef373)|df_iso_country\n",
    "US Cities Demographic| us-cities-demographics.csv|provide by Udacity|df_demograph\n",
    "Indicators developpment| WDIData.csv| [World Bank](https://www.kaggle.com/xavier14/wdidata)|df_indicator_dev\n",
    "Education-statistics| EdStatsData.csv|provide by Kaggle [World Bank](https://www.kaggle.com/kostya23/worldbankedstatsunarchived)|df_Educ_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94 Immigration Data\n",
    "* Source: https://travel.trade.gov/research/reports/historical/2016.html\n",
    "    * data 'data/18-83510-I94-Data-2016', provide one file per month\n",
    "        * These records are described according to 28 variables and 3M  rows per file\n",
    "        *  It's provide information about Arrival/Departure to foreign visitors        \n",
    "    * I94_SAS_Labels_Description.SAS for variable descriptions\n",
    "    \n",
    "##### Global Land Temperature Data\n",
    "* Source: http://berkeleyearth.org/\n",
    "    * data 'GlobalLandTemperaturesByCity.csv' provide climate information\n",
    "        * Each line correspond to a record of temperature per day from city around the world.\n",
    "        * The GlobalLandTemperaturesByCity.csv has 7 variables and 8599213 rows.\n",
    "        \n",
    "##### Global Airports Data\n",
    "* Source: https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals\n",
    "    * data 'airports-extended.csv'. Some of the data come from public sources and some of it comes from OpenFlights.org user contributions.\n",
    "        * It's provide informatioms about of airports, train stations, and ferry terminals around the world.\n",
    "        * There are 4 variables in 'airports-extended.csv'and 10668 rows\n",
    "        \n",
    "##### Airports Data Description Data\n",
    "* Source: https://datahub.io/core/airport-codes#data\n",
    "    * airport-codes_csv.csv. The airport code refers to the IATA airport code, 3 letters code unique for all airports in the world\n",
    "        * The airport-codes_csv.csv provides informations about aiports.\n",
    "        * There are 55075 rows and 12 columns in airport-codes_csv.csv.\n",
    "        \n",
    "##### Iso country Data\n",
    "* Source: https://gist.github.com/radcliff/f09c0f88344a7fcef373\n",
    "    * data 'wikipedia-iso-country-codes.csv'. This is a database about the different code useful to identify country.\n",
    "        * This table gives us informations about Country codes used to identify each country\n",
    "        * There are 4 variables and 247 rows.\n",
    "        \n",
    "##### US cities Demographics Data\n",
    "* Source: https://data.census.gov/cedsci/. \n",
    "    * data 'us-cities-demographics.csv'. This dataset contains information about the demographics of all US cities and come from the US Census Bureau.\n",
    "        * Provides simple informations about US State population\n",
    "        * Contains 12 variables and 2892 rows\n",
    "        \n",
    "##### World Development Indicators Data\n",
    "* Source: https://www.kaggle.com/xavier14/wdidata\n",
    "    * data 'WDIData.csv'. The primary World Bank collection of development indicators, compiled from officially-recognized international sources. \n",
    "        * It presents the most current and accurate global development data available, and includes national, regional and global estimates.\n",
    "        * Contains 64 variables, most of which are variables per year(1960 to 2018), with economics context and 422137 rows.\n",
    "               \n",
    "##### i94addr Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * US States code defined in I94_SAS_Labels_Description.SAS\n",
    "        * data 'i94addr.csv' provides State Id and State name  \n",
    "        \n",
    "##### i94city_i94res Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94cit_i94res.csv' defined Code Country by 3 digits\n",
    "        * data 'i94cit_i94res.csv' provides Country Id and Country name\n",
    "        \n",
    "##### i94mode Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94mode.csv' defined arrival US\n",
    "        * data 'i94mode.csv' provides code Mode and name Code.\n",
    "        \n",
    "##### i94port Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94port.csv'\n",
    "        * data 'i94port.csv' provides Port Id, Port city and State Id.\n",
    "        \n",
    "##### i94visa Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94visa.csv'\n",
    "        * data 'i94visa.csv' povides code Visa ans Visa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP SPARK AND ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parquet = '../../output/'\n",
    "path = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls ../../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No variables match your requested type.\n"
     ]
    }
   ],
   "source": [
    "%whos DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "#TODO update with the latest version\n",
    "def create_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "                    .appName(\"Us_student_immigation\") \\\n",
    "                    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "    return(spark)\n",
    "spark = create_spark_session()\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1_LOAD FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Immigration Data\n",
    "#### Exploration\n",
    "\n",
    "* Path = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "* There are 3096313 rows and 29 columns in *i94_apr16_sub.sas7bdat*.\n",
    "* Name of the dataFrame: df_immigration\n",
    "* As we see in [data exploration file](./0_dataset_information.ipynb), some variables are either not present or not very present (visapost, occup, entdepu, insnum)\n",
    "* Variables droped: depdate, count, occup, entdepa, entdepd, entdepu, matflag, biryear, insnum, dtadfile, visapost, fltno, admnum, insnum, dtaddto. \t\n",
    "* Variables used:\n",
    "\n",
    "Column Name | Description |\n",
    "-|-|\n",
    "**cicid**|     ID uniq per record in the dataset \n",
    "**i94yr**|     4 digit year  \n",
    "**i94mon**|    Numeric month \n",
    "**i94cit**|     3 digit code of source city for immigration (Born country) \n",
    "**i94res**|    3 digit code of source country for immigration\n",
    "**i94port**|   Port addmitted through \n",
    "**i94mode**|   Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) \n",
    "**i94addr**|   State of arrival \n",
    "**i94bir**|    Age in years \n",
    "**i94visa**|   Visa Code - 1 = Business / 2 = Pleasure / 3 = Student\n",
    "**gender**|    Gender\n",
    "**airline**|   Airline used to arrive in U.S.\n",
    "**admnum**|    Admission number, should be unique and not nullable \n",
    "**visatype**|  Class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read I94 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_immigration(path, file):\n",
    "    df = spark.read \\\n",
    "        .format('com.github.saurfang.sas.spark') \\\n",
    "        .option('header', 'true') \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file = '18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# refaire avec S3 et tous les fichiers (get_path_sas_folder parquet file)\n",
    "immigration, rows_immig = load_immigration(path, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Land Temperature Data\n",
    "#### Exploration\n",
    "* Path = '../../data/GlobalLandTemperaturesByCity.csv\n",
    "* There are 8599212 rows and 7 columns in *GlobalLandTemperaturesByCity.csv*.\n",
    "* Name of the dataFrame: df_temperature\n",
    "\n",
    "* As we see in [data exploration file](./0_dataset_information.ipynb), the first date is in 1743, and we find a row per day per town. So we will make aggregation for this data set and drop 'AverageTemperature' , 'Latitude' and 'Longitude' columns\n",
    "* Variables used:\n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**dt**|Date format YYYY-MM-DD| \n",
    "**AverageTemperature**|Average Temperature for the city to th date dt|\n",
    "**City**| City name| \n",
    "**Country**| Country name |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read GlobalLandTemperaturesByCity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temperature(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file = 'GlobalLandTemperaturesByCity.csv'\n",
    "temperature, rows_temp = load_temperature(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports Code Data\n",
    "#### Exploration\n",
    "* Path = '../../data/airport-codes_csv.csv'\n",
    "* There are 55075 rows and 12 column in *airport-codes_csv.csv*\n",
    "* Name of the DataFrame : df_airport_code\n",
    "* Some variables left more 50% of data (continent, iata_code and local_code) so I kept:\n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**ident**| Unique identifier Airport code|\n",
    "**type**| Type of airport | \n",
    "**name**| Name of the airport | \n",
    "**continent**| Continent | | \n",
    "**iso_country**| ISO code of airport country |\n",
    "**iso_region**| ISO code of the region airport | \n",
    "**municipality**| City name where the airport is located | \n",
    "**iata_code**| IATA code of the airport|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Airports Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_airport_code(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file = 'airport-codes_csv.csv'    \n",
    "airport_code, rows_code = load_airport_code(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Airports Data\n",
    "#### Exploration\n",
    "* Path = '../../data/airports-extended.csv'\n",
    "* There are 10668 rows and 13 columns in *airports-extended.csv*\n",
    "* Name of the dataframe : df_global_airports\n",
    "* No missing value, and I kept:\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**airport_name**|Name of airport|Nadzab Airport|Object\n",
    "**airport_city**|Main city served by airport|Nadzab|Object\n",
    "**airport_country**|Country or territory where airport is located|Papua New Guinea|Object\n",
    "**airport_iata**|3-letter IATA code|LAE|Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read airports-extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_airports_schema = T.StructType([\n",
    "    T.StructField('airport_ID', T.IntegerType(), False),\n",
    "    T.StructField('name', T.StringType(), False),\n",
    "    T.StructField('city', T.StringType(), False),\n",
    "    T.StructField('country', T.StringType(), False),\n",
    "    T.StructField('iata', T.StringType(), False),\n",
    "    T.StructField('icao', T.StringType(), False),\n",
    "    T.StructField('latitude', T.StringType(), False),\n",
    "    T.StructField('longitude', T.StringType(), False),\n",
    "    T.StructField('altitude', T.IntegerType(), False),\n",
    "    T.StructField('timezone', T.StringType(), False),\n",
    "    T.StructField('dst', T.StringType(), False),\n",
    "    T.StructField('tz_timezone', T.StringType(), False),\n",
    "    T.StructField('type', T.StringType(), False),\n",
    "    T.StructField('data_source', T.StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_global_airports(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'True') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .schema(global_airports_schema) \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file = 'airports-extended.csv'\n",
    "global_airports, rows_global = load_global_airports(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso Country Data\n",
    "#### Exploration\n",
    "* Path = '../../data/wikipedia-iso-country-codes.csv\n",
    "* There are 246 rows and 5 columns in *wikipedia-iso-country-codes.csv*\n",
    "* Name of the dataframe: df_iso_country\n",
    "* I remove 'ISO 3166-2' column, only one missing value. I choose to replace manually. \n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**Country_name**|Country Name in English|\n",
    "**Alpha2_code**|code 2 letter code for the country|\n",
    "**Alpha3_code**|code 3 letter code for the country|\n",
    "**Numeric_code**|ISO 3166-2 code|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read wikipedia-iso-country-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_country_schema = T.StructType([\n",
    "    T.StructField('English short name lower case', T.StringType(), False),\n",
    "    T.StructField('Alpha-2 code', T.StringType(), False),\n",
    "    T.StructField('Alpha-3 code', T.StringType(), False),\n",
    "    T.StructField('Numeric code', T.StringType(), False),\n",
    "    T.StructField('ISO_3166-2', T.StringType(), True),    \n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iso_country(path, file):\n",
    "    df = spark.read \\\n",
    "            .format(\"csv\") \\\n",
    "            .option('header', 'true') \\\n",
    "            .option('inferSchema', 'true') \\\n",
    "            .schema(iso_country_schema) \\\n",
    "            .load(path+file)\n",
    "    df = df.withColumnRenamed(\"English short name lower case\", \"Country\")\\\n",
    "           .withColumnRenamed(\"Alpha-2 code\", \"Alpha_2\")\\\n",
    "           .withColumnRenamed(\"Alpha-3 code\", \"Alpha_3\")\\\n",
    "           .withColumnRenamed(\"Numeric code\", \"Num_code\")\n",
    "    \n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file = 'wikipedia-iso-country-codes.csv'\n",
    "iso_country, rows_iso = load_iso_country(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US cities Demographics\n",
    "#### Exploration\n",
    "* Path = '../../data/us-cities-demographics.csv\n",
    "* There are 2891 rows and 12 columns in us-cities-demographics.csv\n",
    "* Dataframe name : df_demograph\n",
    "* Missing less than 1% in some variables so I drop 'Number of Veterans', 'Average Household Size' and kept: \n",
    "\n",
    "Column Name | Description | \n",
    "-|-|\n",
    "**City**|Name of the city|\n",
    "**State**|US state of the city|\n",
    "**Median Age**|The median of the age of the population|\n",
    "**Male Population**|Number of the male population|\n",
    "**Female Population**|Number of the female population|\n",
    "**Total Population**|Number of the total population|\n",
    "**Foreign-born**|Number of residents of the city that were not born in the city|\n",
    "**State Code**|Code of the state of the city|\n",
    "**Race**|Race class|\n",
    "**Count**|Number of individual of each race|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read us-cities-demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "demograph_schema = T.StructType([\n",
    "    T.StructField('City', T.StringType(), False),\n",
    "    T.StructField('State', T.StringType(), False),\n",
    "    T.StructField('Median_Age', T.FloatType(), False),\n",
    "    T.StructField('Male_Population', T.IntegerType(), False),\n",
    "    T.StructField('Female_Population', T.IntegerType(), False),\n",
    "    T.StructField('Total_Population', T.IntegerType(), False),\n",
    "    T.StructField('Number_of_Veterans', T.IntegerType(), False),\n",
    "    T.StructField('Foreign-born', T.IntegerType(), False),\n",
    "    T.StructField('Average_Household_Size', T.FloatType(), False),\n",
    "    T.StructField('State_Code', T.StringType(), False),\n",
    "    T.StructField('Race', T.StringType(), False),\n",
    "    T.StructField('Count', T.IntegerType(), False)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demograph(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('delimiter', ';') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .schema(demograph_schema) \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file = 'us-cities-demographics.csv'\n",
    "demograph, rows_demo = load_demograph(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WDIData.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exploration \n",
    "* Path = '../../data/WDIData.csv\n",
    "* There are 422136 rows and 64 columns in *WDIData.csv*\n",
    "* Dataframe name : df_indicator_dev\n",
    "* This dataset contains 64 variables with economics context , most of which are variables per year(1960 to 2018). Data is missing a lot, between 40% and 91%. I just need the year 2015 to explain the Economic context in the country and make aggregation per country. I kept:\n",
    "\n",
    "Column Name | Description | \n",
    "-|-|\n",
    "**Country Name**|Name of the country|\n",
    "**Country Code**|3 letters code of country|\n",
    "**Indicator Name**|indicators of economic development|conversion factor, GDP (LCU per inter...|\n",
    "**Indicator Code**|letters indicator code|\n",
    "**2016**|one column per year since 1960|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read WDIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator_dev(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file) \\\n",
    "        .select(\"Country Name\",\"Country Code\", \"Indicator Name\", \"Indicator Code\", \"2015\" ) \\\n",
    "        .toDF(\"Country_Name\",\"Country_Code\", \"Indicator_Name\", \"Indicator_Code\", \"2015\")\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file = 'WDIData.csv'\n",
    "indicator_dev, rows_dev = load_indicator_dev(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Description Labels  Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here some data extract from '../../data/I94_SAS_Labels_Description.SAS'.\n",
    "To explain code in I94-immigration, I create 5 files and read here. The file was cleaned and parsed with the scrip parse_file.py. see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:38: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:50: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:52: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:25: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:38: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:50: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:52: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-18-d55550718df4>:25: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if key is \"i94port\":\n",
      "<ipython-input-18-d55550718df4>:38: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if key is \"i94cit_i94res\":\n",
      "<ipython-input-18-d55550718df4>:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if key is \"i94mode\":\n",
      "<ipython-input-18-d55550718df4>:50: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if key is \"i94addr\":\n",
      "<ipython-input-18-d55550718df4>:52: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if key is \"i94visa\":\n"
     ]
    }
   ],
   "source": [
    "def parse_file(input_data, file, key):\n",
    "    \"\"\"\n",
    "    fonction to parse file and create parquet file\n",
    "    \"\"\"\n",
    "    output_parquet = '../../data/'\n",
    "    path_file = input_data + file\n",
    "    \n",
    "    #file_parse = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "    with open(path_file, 'r') as f:\n",
    "        file = f.read()\n",
    "    sas_dict={}\n",
    "    key_name = ''\n",
    "\n",
    "    for line in file.split(\"\\n\"):\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        if '/* I94' in line :         \n",
    "            line = line.strip('/* ')\n",
    "            key_name = line.split('-')[0].replace(\"&\", \"_\").replace(\" \", \"\").strip(\" \").lower() \n",
    "            sas_dict[key_name] = []\n",
    "        elif '=' in line and key_name != '' :\n",
    "            #line_trans = re.sub(\"([A-Z]*?),(\\s*?[A-Z]{2}\\s)\",\"\\\\1=\\\\2\", line)\n",
    "            #print(line_trans)\n",
    "            sas_dict[key_name].append([item.strip(' ').strip(\" ';\") for item in line.split('=')])\n",
    "        \n",
    "    if key is \"i94port\":\n",
    "        #pattern = r'[^()]*\\s*\\([^()]*\\)'\n",
    "        columns = [\"Port_id\", \"Port_city\", \"State_id\"]\n",
    "        swap = sas_dict[key]          \n",
    "        sas_dict[key] = []\n",
    "        for x in swap:           \n",
    "            if \",\" in x[1]:\n",
    "                mylist=[]\n",
    "                a = x[1].rsplit(\",\", 1)\n",
    "                b = a[0]\n",
    "                c = a[1].strip()\n",
    "                mylist.extend([x[0], b, c])\n",
    "                sas_dict[key].append(item for item in mylist)\n",
    "    if key is \"i94cit_i94res\":\n",
    "        columns = [\"Country_id\", \"Country\"]\n",
    "        swap = sas_dict[key]\n",
    "        for x in swap:\n",
    "            #x[0] = int(x[0])\n",
    "            if \"mexico\" in x[1]:\n",
    "                x[1] = \"mexico\"        \n",
    "    if key is \"i94mode\":\n",
    "        columns = [\"Mode_id\", \"Mode\"]\n",
    "        #swap = sas_dict[key]\n",
    "        #for x in swap:\n",
    "        #    x[0] = int(x[0])\n",
    "    if key is \"i94addr\":\n",
    "        columns = [\"State_id\", \"State\"]\n",
    "    if key is \"i94visa\":\n",
    "        columns = [\"Code_visa\", \"Visa\"]\n",
    "        #swap = sas_dict[key]\n",
    "        #for x in swap:\n",
    "        #    x[0] = int(x[0])\n",
    "            \n",
    "    df = \"\"                  \n",
    "    if key in sas_dict.keys():\n",
    "        if len(sas_dict[key]) > 0:\n",
    "            df = pd.DataFrame(sas_dict[key], columns = columns)\n",
    "            df.sort_values(df.columns[0], inplace=True)\n",
    "        #with io.open(f\"../../data/{key}.csv\", \"w\") as f:\n",
    "        #    df.to_csv(f, index=False)\n",
    "        df.to_parquet(f'{output_parquet}{key}.parquet')\n",
    "    return(len(sas_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/I94_SAS_Labels_Descriptions.SAS\n",
      "...Begin to create I94 Labels files\n",
      "There are 583 rows in i94port.parquet\n",
      "There are 3 rows in i94visa.parquet\n",
      "There are 55 rows in i94addr.parquet\n",
      "There are 289 rows in i94cit_i94res.parquet\n",
      "There are 4 rows in i94mode.parquet\n"
     ]
    }
   ],
   "source": [
    "### Create Parquet Files \n",
    "# Parse I94_SAS_Labels_Description.SAS and save in parquet format in '../../data/'\n",
    "#file = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "#!python parse_file.py $path $file\n",
    "#### Read Parquet files create from 'I94_SAS_Labels_Descriptions.SAS'\n",
    "\n",
    "path = '../../data/'\n",
    "input_data =  '../../data/'\n",
    "file = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "print(path+file)\n",
    "print(\"...Begin to create I94 Labels files\")\n",
    "file = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "\n",
    "## make i94port.parquet\n",
    "key = \"i94port\"\n",
    "nb = parse_file(input_data, file, key)\n",
    "print(f'There are {nb} rows in {key}.parquet')\n",
    "## make i94visa.csv\n",
    "key = \"i94visa\"\n",
    "nb = parse_file(input_data, file, key)\n",
    "print(f'There are {nb} rows in {key}.parquet')\n",
    "## make i94addr.csv\n",
    "key = \"i94addr\"\n",
    "nb = parse_file(input_data, file, key)\n",
    "print(f'There are {nb} rows in {key}.parquet')\n",
    "# make i94cit_i94res.csv\n",
    "key = \"i94cit_i94res\"\n",
    "nb = parse_file(input_data, file, key)\n",
    "print(f'There are {nb} rows in {key}.parquet')\n",
    "# make i94mode.csv\n",
    "key = \"i94mode\"\n",
    "nb = parse_file(input_data, file, key)\n",
    "print(f'There are {nb} rows in {key}.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Dataframe i94_mode *****\n",
      "There are 4 rows.\n",
      " \n",
      "***** Dataframe i94_ctry *****\n",
      "There are 289 rows.\n",
      " \n",
      "***** Dataframe i94_addr *****\n",
      "There are 55 rows.\n",
      " \n",
      "***** Dataframe i94_visa *****\n",
      "There are 3 rows.\n",
      " \n",
      "***** Dataframe i94_port *****\n",
      "There are 583 rows.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "i94_mode = pd.read_parquet(path+'i94mode.parquet')\n",
    "print(f'***** Dataframe i94_mode *****')\n",
    "print(\"There are {} rows.\".format(len(i94_mode)))\n",
    "print(' ')\n",
    "\n",
    "i94_ctry = pd.read_parquet(path+'i94cit_i94res.parquet')\n",
    "print(f'***** Dataframe i94_ctry *****')\n",
    "print(\"There are {} rows.\".format(len(i94_ctry)))\n",
    "print(' ')\n",
    "\n",
    "i94_addr = pd.read_parquet(path+'i94addr.parquet')\n",
    "print(f'***** Dataframe i94_addr *****')\n",
    "print(\"There are {} rows.\".format(len(i94_addr)))\n",
    "print(' ')\n",
    "\n",
    "i94_visa = pd.read_parquet(path+'i94visa.parquet')\n",
    "print(f'***** Dataframe i94_visa *****')\n",
    "print(\"There are {} rows.\".format(len(i94_visa)))\n",
    "print(' ')\n",
    "\n",
    "i94_port = pd.read_parquet(path+'i94port.parquet')\n",
    "print(f'***** Dataframe i94_port *****')\n",
    "print(\"There are {} rows.\".format(len(i94_port)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i94_addr', 'i94_ctry', 'i94_mode', 'i94_port', 'i94_visa']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%who_ls DataFrame"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(f'df_airport_code: There are {airport_code.count()} rows from file, {rows_code} before staging')\n",
    "display(f'df_demograph : There are {demograph.count()} rows from file, {rows_demo} before staging')\n",
    "display(f'df_global_airports : There are {global_airports.count()} rows parquet file, {rows_global} before staging')\n",
    "display(f'df_immigration : There are {immigration.count()} rows parquet file, {rows_immig} before staging')\n",
    "display(f'df_indicator_dev : There are {indicator_dev.count()} rows parquet file, {rows_dev} before staging')\n",
    "display(f'df_iso_country : There are {iso_country.count()} rows parquet file, {rows_iso} before staging')\n",
    "display(f'df_temperature : There are {temperature.count()} rows parquet file, {rows_temp} before staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I94 Immigration Data\n",
    "* i94addr, missing 152592 values (code State US, 2 letters)\n",
    "    * fill by Port_id from the dataframe 'i94port' \n",
    "    * join on 'df_immigration.i94port == port_state_dic.Port_id', with no missing values\n",
    "    * nul value replace by State_id\n",
    "* int_col = ['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94mode', 'i94bir', 'i94visa']\n",
    "    * fill null by default value from dictionnary and cast the int_col in Integer\n",
    "* str_cols = ['i94addr', 'i94port', 'gender', 'airline', 'visatype']\n",
    "    * fill null by default value from dictionnary\n",
    "* date_col = ['arrdate'(double sas format),'dtadfile'(string YYYYMMDD)]\n",
    "    * 'arrdate' in SAS date format, a value represents the number of days between January 1, 1960, and a other date.\n",
    "    * cast the date and fill the null value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Count of null values of dataframe in pyspark \n",
    "immigration.select([count(when(col(c).isNull(), c)).alias(c) for c in immigration.columns]).toPandas()\n",
    "#Count of Missing values of dataframe in pyspark \n",
    "immigration.select([count(when(isnan(c), c)).alias(c) for c in immigration.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get number of null value in df_immigration.i94port\n",
    "immigration.filter(immigration.i94port.rlike('[A-Z]{3}')) \\\n",
    "              .filter(immigration.i94addr.isNull()) \\\n",
    "              .select(immigration.i94port, immigration.i94addr).count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#drop columns with a lot of null values and not useful\n",
    "drop_col = ['depdate', 'count', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', \\\n",
    "            'insnum','visapost', 'fltno', 'admnum', 'insnum', 'dtaddto', 'arrdate', 'dtadfile']\n",
    "newdf = immigration.drop(*drop_col)\n",
    "newdf.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create dictionnary from i94_port\n",
    "port_state_dic = dict([(i,a) for i, a in zip(i94_port.Port_id,i94_port.State_id)])\n",
    "# lamda function to get State_id\n",
    "user_func =  udf(lambda x: port_state_dic.get(x))\n",
    "\n",
    "newdf = newdf.withColumn('i94addr', F.when((F.col('i94addr').isNull()), \\\n",
    "                                                 user_func(immigration.i94port)) \\\n",
    "                                           .otherwise(F.col('i94addr')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newdf.filter(newdf.i94port.rlike('[A-Z]{3}')) \\\n",
    "     .filter(newdf.i94addr.isNull()) \\\n",
    "     .select(newdf.i94port, newdf.i94addr).count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# replace the null value and cast the columns in integer\n",
    "# int_col = ['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94mode', 'i94bir', 'i94visa']\n",
    "null_int = {'cicid': -1, 'i94yr': -1, 'i94mon': -1,'i94cit': 239, 'i94res': 239, 'i94mode': 9, 'i94bir': -1, 'i94visa': -1}\n",
    "for k in null_int:\n",
    "        newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_int[k])\n",
    "                 .otherwise(F.col(k).cast(\"int\")))\n",
    "        \n",
    "# replace the null value for the string\n",
    "# str_cols = ['i94addr', 'i94port', 'gender', 'airline', 'visatype']\n",
    "null_str = {'i94addr': '99', 'i94port': '999', 'gender': 'U', 'airline': 'unknown', 'visatype': '99' }\n",
    "for k in null_str:\n",
    "        newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_str[k])\n",
    "                                 .otherwise(F.col(k)))\n",
    "        \n",
    "# display distinct value\n",
    "newdf.agg(*(countDistinct(col(c)).alias(c) for c in newdf.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newdf.filter(newdf.i94port.rlike('[A-Z]{3}')) \\\n",
    "     .filter(newdf.i94addr.isNull()) \\\n",
    "     .select(newdf.i94port, newdf.i94addr).count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Land Temperature Data\n",
    "\n",
    "* As we see in [data exploration file](./0_dataset_information.ipynb), the first date is in 1743, and we find a row per day per town. \n",
    "    * Make aggregation \n",
    "* drop \"dt\", \"AverageTemperatureUncertainty\" , \"Latitude\" and \"Longitude\" columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temperature.show(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temperature.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temperature.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Count of null values of dataframe in pyspark \n",
    "immigration.select([count(when(col(c).isNull(), c)).alias(c) for c in immigration.columns]).toPandas()\n",
    "#Count of Missing values of dataframe in pyspark \n",
    "immigration.select([count(when(isnan(c), c)).alias(c) for c in immigration.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports Code Data\n",
    "* MIssing value in Iata_code, Municipality in the whole table\n",
    "    * I drop '[\"elevation_ft\",\"continent\", \"gps_code\", \"coordinates\"]'\n",
    "    * I keep : ident, airport_type, airport_name, country_iso, city_name, iata_code, state_id\n",
    "    The missing value in iata_code left with the drop. \n",
    "* I extract the State_id from the split of the local_code and rename columns."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "airport_code.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(newdf.select([count(when(isnan(c), c)).alias(c) for c in newdf.columns]).toPandas())\n",
    "display(newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# display distinct value\n",
    "newdf.agg(*(countDistinct(col(c)).alias(c) for c in newdf.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Airports Data\n",
    "* There are some missing values.\n",
    "* Data clean: I drop [\"icao\", \"latitude\", \"longitude\", \"altitude\", , \"timezone\", \"dst\", \"tz_timezone\", \"data_source\"] and keep only the airport in 'type'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newdf = global_airports\n",
    "global_airports.count()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "global_airports.show(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(newdf.select([count(when(isnan(c), c)).alias(c) for c in newdf.columns]).toPandas())\n",
    "display(newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# display distinct value\n",
    "newdf.agg(*(countDistinct(col(c)).alias(c) for c in newdf.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso Country Data\n",
    "* No missing values\n",
    "* I drop 'ISO_3166-2' and rename columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "iso_country.show(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newdf = iso_country\n",
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(newdf.select([count(when(isnan(c), c)).alias(c) for c in newdf.columns]).toPandas())\n",
    "display(newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US cities Demographics\n",
    "* missing values\n",
    "* dataclean\n",
    "* df_demograph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "demograph.count()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newdf = demograph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(newdf.select([count(when(isnan(c), c)).alias(c) for c in newdf.columns]).toPandas())\n",
    "display(newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newdf.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Development Indicators Data\n",
    "* df_indicator_dev"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "indicator_dev.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "indicator_dev.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keep_cols = ['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2015']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "indicator_dev.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# count distinc in column\n",
    "#df_indicator_dev.select(F.countDistinct('Country_Name')).show()\n",
    "[i.Country_Name for i in indicator_dev.select('Country_Name').distinct().collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Extract_Transform_Load\n",
    "\n",
    "### Conceptual Data Model\n",
    "\n",
    "On the basis of a star schema, this allows to quickly find the elements linked to each other.It consists of a large fact table and a circle of other tables that contain the descriptive elements of the fact, called \"dimensions\".\n",
    "Table fact contaiins observable data (the facts) that we have on a subject and that we want to study, according axes of analysis (the dimensions).  \n",
    "The immigration dataset is the center of this project and allow us to explore foreign visitors. It will the fact table. Dimension tables give us information about a piece of this visitors, country, airport, indicator economics, and us demography. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The etl will load the diffrent files from different format with spark. Then process the cleasning. The second stage build tdimension table and fact table. Then the files are load in parquet file then make a check."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### 1_load_df() do : read raw file, select column, change name, print schema, few rows, and count rows from this notebook\n",
    "\n",
    "useful for test, reset dataframe and gets nb rows to compare\n",
    "\n",
    "file = '18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# refaire avec S3 et tous les fichiers (get_path_sas_folder parquet file)\n",
    "immigration, rows_immig = load_immigration(path, file)\n",
    "\n",
    "file = 'GlobalLandTemperaturesByCity.csv'\n",
    "temperature, rows_temp = load_temperature(path, file)\n",
    "\n",
    "file = 'airport-codes_csv.csv'    \n",
    "airport_code, rows_code = load_airport_code(path, file)\n",
    "\n",
    "file = 'airports-extended.csv'\n",
    "global_airports, rows_global = load_global_airports(path, file)\n",
    "\n",
    "file = 'wikipedia-iso-country-codes.csv'\n",
    "iso_country, rows_iso = load_iso_country(path, file)\n",
    "\n",
    "file = 'us-cities-demographics.csv'\n",
    "demograph, rows_demo = load_demograph(path, file)\n",
    "\n",
    "file = 'WDIData.csv'\n",
    "indicator_dev, rows_dev = load_indicator_dev(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2_read_file() do : read raw file, select column, change name, print schema, few rows, and count rows from a python script\n",
    "useful for test and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_file1 import read_sas, read_csv, read_csv_global_airports, read_csv_iso_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_sas in module read_file1:\n",
      "\n",
      "read_sas(spark, path, file, cols)\n",
      "    read file from '18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
      "    return numbers of rows and dataframe 'df_immigration'\n",
      "\n",
      "Help on function read_csv in module read_file1:\n",
      "\n",
      "read_csv(spark, path, file, cols, delimiter)\n",
      "    read csv file and return a dataframe\n",
      "\n",
      "Help on function read_csv_global_airports in module read_file1:\n",
      "\n",
      "read_csv_global_airports(spark, path, file, cols, delimiter, schema, header)\n",
      "    read csv file with a custom schema\n",
      "    return a dataframe\n",
      "\n",
      "Help on function read_csv_iso_country in module read_file1:\n",
      "\n",
      "read_csv_iso_country(spark, path, file)\n",
      "    read csv file \n",
      "    return a dataframe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(read_sas)\n",
    "help(read_csv)\n",
    "help(read_csv_global_airports)\n",
    "help(read_csv_iso_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check nb rows with the dataframe immigration\n",
    "print(f'numbers of rows from dataframe immigration before run the read_sas():{rows_immig}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "...Path file is :  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat is processing...\n",
      " \n",
      "*****         Loading 3096313 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-----+------+------+------+------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|cicid|i94yr |i94mon|i94cit|i94res|i94port|i94mode|i94addr|i94bir|i94visa|gender|airline|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|6.0  |2016.0|4.0   |692.0 |692.0 |XXX    |null   |null   |37.0  |2.0    |null  |null   |B2      |\n",
      "|7.0  |2016.0|4.0   |254.0 |276.0 |ATL    |1.0    |AL     |25.0  |3.0    |M     |null   |F1      |\n",
      "|15.0 |2016.0|4.0   |101.0 |101.0 |WAS    |1.0    |MI     |55.0  |2.0    |M     |OS     |B2      |\n",
      "+-----+------+------+------+------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['cicid','i94yr','i94mon','i94cit','i94res','i94port','i94mode', 'i94addr','i94bir','i94visa', 'gender','airline','visatype']\n",
    "file = '18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "delimiter = \"\"\n",
    "# refaire avec S3 et tous les fichiers (get_path_sas_folder parquet file)\n",
    "df_immigration = read_sas(spark, path, file, cols)\n",
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check nb rows with the dataframe temperature\n",
    "print(f'numbers of rows from dataframe temperature before run the read_csv():{rows_temp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "...Path file is :  ../../data/GlobalLandTemperaturesByCity.csv is processing...\n",
      "*****         Loading 8599212 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+------------------+-----+-------+\n",
      "|AverageTemperature|City |Country|\n",
      "+------------------+-----+-------+\n",
      "|6.068             |Ã…rhus|Denmark|\n",
      "|null              |Ã…rhus|Denmark|\n",
      "|null              |Ã…rhus|Denmark|\n",
      "+------------------+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['AverageTemperature', 'City', 'Country']\n",
    "file = 'GlobalLandTemperaturesByCity.csv'\n",
    "delimiter = ','\n",
    "df_temperature = read_csv(spark,path, file, cols, delimiter)\n",
    "df_temperature.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport-Codes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check nb rows with the dataframe Airport_code\n",
    "print(f'numbers of rows from dataframe Airport_code before run the read_csv():{rows_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "...Path file is :  ../../data/airport-codes_csv.csv is processing...\n",
      "*****         Loading 55075 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-----+-------------+--------------------+-----------+----------+------------+---------+----------+\n",
      "|ident|type         |name                |iso_country|iso_region|municipality|iata_code|local_code|\n",
      "+-----+-------------+--------------------+-----------+----------+------------+---------+----------+\n",
      "|00A  |heliport     |Total Rf Heliport   |US         |US-PA     |Bensalem    |null     |00A       |\n",
      "|00AA |small_airport|Aero B Ranch Airport|US         |US-KS     |Leoti       |null     |00AA      |\n",
      "|00AK |small_airport|Lowell Field        |US         |US-AK     |Anchor Point|null     |00AK      |\n",
      "+-----+-------------+--------------------+-----------+----------+------------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'airport-codes_csv.csv'\n",
    "cols = ['ident', 'type','name', 'iso_country', 'iso_region', 'municipality', 'iata_code', 'local_code']\n",
    "delimiter= ','\n",
    "df_airport_code= read_csv(spark,path, file, cols, delimiter)\n",
    "df_airport_code.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global-Airport"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check nb rows with the dataframe Global_airport\n",
    "print(f'numbers of rows from dataframe Global_airport before run the read_csv():{rows_global}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "...Path file is :  ../../data/airports-extended.csv is processing...\n",
      "*****         Loading 10668 rows\n",
      "*****              Display the Schema\n",
      "root\n",
      " |-- airport_ID: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- iata: string (nullable = true)\n",
      "\n",
      "*****              Display few rows\n",
      "+----------+-------+----------------------------+-----------+----------------+----+\n",
      "|airport_ID|type   |name                        |city       |country         |iata|\n",
      "+----------+-------+----------------------------+-----------+----------------+----+\n",
      "|1         |airport|Goroka Airport              |Goroka     |Papua New Guinea|GKA |\n",
      "|2         |airport|Madang Airport              |Madang     |Papua New Guinea|MAG |\n",
      "|3         |airport|Mount Hagen Kagamuga Airport|Mount Hagen|Papua New Guinea|HGU |\n",
      "+----------+-------+----------------------------+-----------+----------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10668"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from read_file1 import read_sas, read_csv, read_csv_global_airports, read_csv_iso_country\n",
    "file = 'airports-extended.csv'\n",
    "cols = ['airport_ID', 'type', 'name', 'city', 'country', 'iata']\n",
    "schema = StructType([\n",
    "        StructField('airport_ID', IntegerType(), False),\n",
    "        StructField('name', StringType(), False),\n",
    "        StructField('city', StringType(), False),\n",
    "        StructField('country', StringType(), False),\n",
    "        StructField('iata', StringType(), False),\n",
    "        StructField('icao', StringType(), False),\n",
    "        StructField('latitude', StringType(), False),\n",
    "        StructField('longitude', StringType(), False),\n",
    "        StructField('altitude', IntegerType(), False),\n",
    "        StructField('timezone', StringType(), False),\n",
    "        StructField('dst', StringType(), False),\n",
    "        StructField('tz_timezone', StringType(), False),\n",
    "        StructField('type', StringType(), False),\n",
    "        StructField('data_source', StringType(), False)\n",
    "    ])\n",
    "delimiter = ','\n",
    "df_global_airports = read_csv_global_airports(spark, path, file, cols, delimiter, schema, header=False )\n",
    "df_global_airports.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iso_country\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check nb rows with the dataframe Iso_country\n",
    "\n",
    "print(f'numbers of rows from dataframe Iso_country before run the read_csv():{rows_iso}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_file1 import read_sas, read_csv, read_csv_global_airports, read_csv_iso_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_iso_country(spark, path, file):\n",
    "    \"\"\"\n",
    "    read csv file \n",
    "    return a dataframe\n",
    "    \"\"\"\n",
    "    print(\" \")\n",
    "    print(f\"...Path file is :  {path}{file} is processing...\")\n",
    "    #cols = ['English short name lower case', 'Alpha-2 code','Alpha-3 code', 'Numeric code', 'ISO_3166-2']\n",
    "    \n",
    "    # *********************************************** remove .schema(schema\\ .select(cols)\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file) \n",
    "        \n",
    "    df.show(3, truncate = False) \n",
    "    df = df.withColumnRenamed(\"English short name lower case\", \"Country\")\\\n",
    "           .withColumnRenamed(\"Alpha-2 code\", \"Alpha_2\")\\\n",
    "           .withColumnRenamed(\"Alpha-3 code\", \"Alpha_3\")\\\n",
    "           .withColumnRenamed(\"Numeric code\", \"Num_code\")\n",
    "    \n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****              Display the Schema')\n",
    "    df.printSchema()          \n",
    "    print(f'*****              Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('English short name lower case', StringType(), False),\n",
    "    StructField('Alpha-2 code', StringType(), False),\n",
    "    StructField('Alpha-3 code', StringType(), False),\n",
    "    StructField('Numeric code', StringType(), False),\n",
    "    StructField('ISO_3166-2', StringType(), True),   \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "...Path file is :  ../../data/wikipedia-iso-country-codes.csv is processing...\n",
      "+-----------------------------+------------+------------+------------+-------------+\n",
      "|English short name lower case|Alpha-2 code|Alpha-3 code|Numeric code|ISO 3166-2   |\n",
      "+-----------------------------+------------+------------+------------+-------------+\n",
      "|Zimbabwe                     |ZW          |ZWE         |716         |ISO 3166-2:ZW|\n",
      "|Zambia                       |ZM          |ZMB         |894         |ISO 3166-2:ZM|\n",
      "|Yemen                        |YE          |YEM         |887         |ISO 3166-2:YE|\n",
      "+-----------------------------+------------+------------+------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "*****         Loading 246 rows\n",
      "*****              Display the Schema\n",
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Alpha_2: string (nullable = true)\n",
      " |-- Alpha_3: string (nullable = true)\n",
      " |-- Num_code: integer (nullable = true)\n",
      " |-- ISO 3166-2: string (nullable = true)\n",
      "\n",
      "*****              Display few rows\n",
      "+--------+-------+-------+--------+-------------+\n",
      "|Country |Alpha_2|Alpha_3|Num_code|ISO 3166-2   |\n",
      "+--------+-------+-------+--------+-------------+\n",
      "|Zimbabwe|ZW     |ZWE    |716     |ISO 3166-2:ZW|\n",
      "|Zambia  |ZM     |ZMB    |894     |ISO 3166-2:ZM|\n",
      "|Yemen   |YE     |YEM    |887     |ISO 3166-2:YE|\n",
      "+--------+-------+-------+--------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'wikipedia-iso-country-codes.csv'\n",
    "df_iso_country = read_csv_iso_country(spark, path, file)\n",
    "df_iso_country.count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### us_cities_demographics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check nb rows with the dataframe Iso_country\n",
    "print(f'numbers of rows from dataframe us_cities_demographics before run the read_csv():{rows_demo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "...Path file is :  ../../data/us-cities-demographics.csv is processing...\n",
      "*****         Loading 2891 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|City         |State        |Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race              |Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|Maryland     |33.8      |40601          |41862            |82463           |1562              |30908       |2.6                   |MD        |Hispanic or Latino|25924|\n",
      "|Quincy       |Massachusetts|41.0      |44129          |49500            |93629           |4147              |32935       |2.39                  |MA        |White             |58723|\n",
      "|Hoover       |Alabama      |38.5      |38040          |46799            |84839           |4819              |8229        |2.58                  |AL        |Asian             |4759 |\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'us-cities-demographics.csv'\n",
    "cols = ['City', 'State', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size', 'State Code', 'Race', 'Count']\n",
    "delimiter = ';'\n",
    "df_demograph = read_csv(spark, path, file, cols, delimiter)\n",
    "df_demograph.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WDIData"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check nb rows with the dataframe Iso_country\n",
    "print(f'numbers of rows from dataframe Iso_country before run the read_csv():{rows_dev}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "...Path file is :  ../../data/WDIData.csv is processing...\n",
      "*****         Loading 422136 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- Country Name: string (nullable = true)\n",
      " |-- Country Code: string (nullable = true)\n",
      " |-- Indicator Name: string (nullable = true)\n",
      " |-- Indicator Code: string (nullable = true)\n",
      " |-- 2015: double (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+------------+------------+-------------------------------------------------------------------------+-----------------+----------------+\n",
      "|Country Name|Country Code|Indicator Name                                                           |Indicator Code   |2015            |\n",
      "+------------+------------+-------------------------------------------------------------------------+-----------------+----------------+\n",
      "|Arab World  |ARB         |2005 PPP conversion factor, GDP (LCU per international $)                |PA.NUS.PPP.05    |null            |\n",
      "|Arab World  |ARB         |2005 PPP conversion factor, private consumption (LCU per international $)|PA.NUS.PRVT.PP.05|null            |\n",
      "|Arab World  |ARB         |Access to clean fuels and technologies for cooking (% of population)     |EG.CFT.ACCS.ZS   |84.1715990242825|\n",
      "+------------+------------+-------------------------------------------------------------------------+-----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "422136"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'WDIData.csv'\n",
    "delimiter = ','\n",
    "cols = ['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2015']\n",
    "df_indicator_dev = read_csv(spark, path, file, cols, delimiter)\n",
    "df_indicator_dev.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable             Type         Data/Info\n",
      "-------------------------------------------\n",
      "df_airport_code      DataFrame    DataFrame[ident: string, <...>ring, local_code: string]\n",
      "df_demograph         DataFrame    DataFrame[City: string, S<...>Race: string, Count: int]\n",
      "df_global_airports   DataFrame    DataFrame[airport_ID: int<...>ry: string, iata: string]\n",
      "df_immigration       DataFrame    DataFrame[cicid: double, <...>string, visatype: string]\n",
      "df_indicator_dev     DataFrame    DataFrame[Country Name: s<...>de: string, 2015: double]\n",
      "df_iso_country       DataFrame    DataFrame[Country: string<...> int, ISO 3166-2: string]\n",
      "df_temperature       DataFrame    DataFrame[AverageTemperat<...> string, Country: string]\n",
      "i94_addr             DataFrame       State_id              <...>    WY            WYOMING\n",
      "i94_ctry             DataFrame        Country_id           <...>n\\n[289 rows x 2 columns]\n",
      "i94_mode             DataFrame      Mode_id          Mode\\n<...>\\n3       9  Not reported\n",
      "i94_port             DataFrame        Port_id              <...>n\\n[583 rows x 3 columns]\n",
      "i94_visa             DataFrame      Code_visa      Visa\\n0 <...>re\\n2         3   Student\n"
     ]
    }
   ],
   "source": [
    "%whos DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Dataframe i94_mode *****\n",
      "There are 4 rows.\n",
      " \n",
      "***** Dataframe i94_ctry *****\n",
      "There are 289 rows.\n",
      " \n",
      "***** Dataframe i94_addr *****\n",
      "There are 55 rows.\n",
      " \n",
      "***** Dataframe i94_visa *****\n",
      "There are 3 rows.\n",
      " \n",
      "***** Dataframe i94_port *****\n",
      "There are 583 rows.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def read_labels_to_df(output_parquet):\n",
    "    i94_mode = pd.read_parquet(path+'i94mode.parquet')\n",
    "    print(f'***** Dataframe i94_mode *****')\n",
    "    print(\"There are {} rows.\".format(len(i94_mode)))\n",
    "    print(' ')\n",
    "\n",
    "    i94_ctry = pd.read_parquet(path+'i94cit_i94res.parquet')\n",
    "    print(f'***** Dataframe i94_ctry *****')\n",
    "    print(\"There are {} rows.\".format(len(i94_ctry)))\n",
    "    print(' ')\n",
    "\n",
    "    i94_addr = pd.read_parquet(path+'i94addr.parquet')\n",
    "    print(f'***** Dataframe i94_addr *****')\n",
    "    print(\"There are {} rows.\".format(len(i94_addr)))\n",
    "    print(' ')\n",
    "\n",
    "    i94_visa = pd.read_parquet(path+'i94visa.parquet')\n",
    "    print(f'***** Dataframe i94_visa *****')\n",
    "    print(\"There are {} rows.\".format(len(i94_visa)))\n",
    "    print(' ')\n",
    "\n",
    "    i94_port = pd.read_parquet(path+'i94port.parquet')\n",
    "    print(f'***** Dataframe i94_port *****')\n",
    "    print(\"There are {} rows.\".format(len(i94_port)))\n",
    "    print(' ')\n",
    "    return(i94_mode,i94_ctry,i94_addr,i94_visa,i94_port)\n",
    "\n",
    "i94_mode,i94_ctry,i94_addr,i94_visa,i94_port = read_labels_to_df(output_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Make df_immigration_clean processing \n",
      "root\n",
      " |-- id_i94: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- country_born_num: integer (nullable = true)\n",
      " |-- country_res_num: integer (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- arri_mode: integer (nullable = true)\n",
      " |-- state_id_arrival: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- arr_reason: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "+-------+------+-----+----------------+---------------+---------+---------+----------------+---+----------+------+-------+--------+\n",
      "| id_i94|  year|month|country_born_num|country_res_num|iata_code|arri_mode|state_id_arrival|age|arr_reason|gender|airline|visatype|\n",
      "+-------+------+-----+----------------+---------------+---------+---------+----------------+---+----------+------+-------+--------+\n",
      "|26800.0|2016.0|  4.0|             133|            264|      NEW|        1|              NY| 28|         3|     M|     LH|      F1|\n",
      "|60305.0|2016.0|  4.0|             209|            209|      LOS|        1|              CA| 29|         3|     M|     OZ|      F1|\n",
      "+-------+------+-----+----------------+---------------+---------+---------+----------------+---+----------+------+-------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_immigration(df_immigration, i94_port):\n",
    "    \"\"\"\n",
    "    clean and format the dataframe df_immigration\n",
    "    \"\"\"\n",
    "    # create dictionnary from i94_port\n",
    "    port_state_dic = dict([(i,a) for i, a in zip(i94_port.Port_id, i94_port.State_id)])\n",
    "    # setup drop column\n",
    "    drop_col = ['depdate', 'count', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', \\\n",
    "                'insnum','visapost', 'fltno', 'admnum', 'insnum', 'dtaddto', 'arrdate', 'dtadfile']\n",
    "    user_func =  udf(lambda x: port_state_dic.get(x))\n",
    "    \n",
    "    # drop columns\n",
    "    newdf = df_immigration.drop(*drop_col) \\\n",
    "                          .withColumn('i94addr', F.when((F.col('i94addr').isNull()), \\\n",
    "                                                user_func(df_immigration.i94port)) \\\n",
    "                                               .otherwise(F.col('i94addr')))\n",
    "    # replace the null value and cast the columns in integer\n",
    "    # int_col = ['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94mode', 'i94bir', 'i94visa']\n",
    "    null_int = {'cicid': -1, 'i94yr': -1, 'i94mon': -1,'i94cit': 239, 'i94res': 239, 'i94mode': 9, 'i94bir': -1, 'i94visa': -1}\n",
    "    for k in null_int:\n",
    "            newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_int[k])\n",
    "                         .otherwise(F.col(k).cast(\"int\")))\n",
    "\n",
    "    # replace the null value for the string\n",
    "    # str_cols = ['i94addr', 'i94port', 'gender', 'airline', 'visatype']\n",
    "    null_str = {'i94addr': '99', 'i94port': '999', 'gender': 'U', 'airline': 'unknown', 'visatype': '99' }\n",
    "    for k in null_str:\n",
    "            newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_str[k])\n",
    "                                     .otherwise(F.col(k)))\n",
    "\n",
    "    df_immigration_clean = (df_immigration.withColumnRenamed(\"cicid\", \"id_i94\") \\\n",
    "                 .withColumnRenamed(\"i94yr\", \"year\") \\\n",
    "                 .withColumnRenamed(\"i94mon\", \"month\") \\\n",
    "                 .withColumnRenamed(\"i94cit\", \"country_born_num\") \\\n",
    "                 .withColumnRenamed(\"i94res\", \"country_res_num\") \\\n",
    "                 .withColumnRenamed(\"i94port\", \"iata_code\") \\\n",
    "                 .withColumnRenamed(\"i94mode\", \"arri_mode\") \\\n",
    "                 .withColumnRenamed(\"i94addr\", \"state_id_arrival\") \\\n",
    "                 .withColumnRenamed(\"i94bir\", \"age\") \\\n",
    "                 .withColumnRenamed(\"i94visa\", \"arr_reason\") \\\n",
    "                 .withColumnRenamed(\"gender\", \"gender\") \\\n",
    "                 .withColumnRenamed(\"airline\",\"airline\") \\\n",
    "                 .withColumnRenamed(\"visatype:\", \"visatype\"))\n",
    "    df_immigration_clean = df_immigration_clean \\\n",
    "            .withColumn('arr_reason', df_immigration_clean.arr_reason.cast('int')) \\\n",
    "            .withColumn('arri_mode', df_immigration_clean.arri_mode.cast('int'))\\\n",
    "            .withColumn('country_res_num', df_immigration_clean.country_res_num.cast('int')) \\\n",
    "            .withColumn('country_born_num', df_immigration_clean.country_born_num.cast('int')) \\\n",
    "            .withColumn('age', df_immigration_clean.age.cast('int')) \\\n",
    "            .filter('arr_reason == 3') \\\n",
    "            .dropDuplicates()\n",
    "            \n",
    "    print('***** Make df_immigration_clean processing ')\n",
    "    df_immigration_clean.printSchema()\n",
    "    df_immigration_clean.show(2)\n",
    "    return(df_immigration_clean)\n",
    "\n",
    "df_clean_immigration = clean_immigration(df_immigration, i94_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43366"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_immigration.filter(df_clean_immigration.arr_reason.contains('3')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c5fc5f1409b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_clean_immigration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_clean_immigration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display(df_clean_immigration.select([count(when(col(c).isNull(), c)).alias(c) for c in df_clean_immigration.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Land Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_temperature(df_temperature):\n",
    "    \"\"\"\n",
    "    Clean and format dataframe df_temperature\n",
    "    \"\"\"\n",
    "    # drop column \"AverageTemperatureUncertainty\"\n",
    "    drop_cols = [\"dt\", \"AverageTemperatureUncertainty\", \"Latitude\", \"Longitude\", \"city\"]\n",
    "    newdf = df_temperature.drop(*drop_cols)\n",
    "    # make aggregation by temperature\n",
    "    newdf = newdf.groupBy('Country') \\\n",
    "        .agg(avg(\"AverageTemperature\")) \\\n",
    "        .orderBy('Country') \\\n",
    "        .dropDuplicates()\n",
    "    newdf = (newdf.withColumnRenamed(\"Country\", \"country\") \\\n",
    "               .withColumnRenamed(\"avg(AverageTemperature)\", \"avg_temperature\"))\n",
    "    newdf = newdf.withColumn(\"avg_temperature\", newdf.avg_temperature.cast('float'))\n",
    "    df_clean_temperature = newdf.orderBy('country')\n",
    "    print('***** Make df_clean_temperature processing ')\n",
    "    df_clean_temperature.printSchema()\n",
    "    df_clean_temperature.show(2)\n",
    "    return(df_clean_temperature)\n",
    "\n",
    "df_clean_temperature = clean_temperature(df_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports Code Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_airport_code(df_airport_code):\n",
    "    # drop columns\n",
    "    # filter closed , heliport and seaplace base airport, small_airport\n",
    "    # keep us airport\n",
    "    drop_cols = [\"elevation_ft\",\"continent\", \"gps_code\", \"coordinates\"]\n",
    "    drop_airport = ['closed', 'heliport', 'seaplane_base', 'small_airport', 'balloonport']\n",
    "    keep_us = ['US']\n",
    "    newdf =df_airport_code.drop(*drop_cols) \\\n",
    "                        .filter(~df_airport_code.type.isin(drop_airport)) \\\n",
    "                        .filter(df_airport_code.iso_country.isin(keep_us))\n",
    "    #airport_code.groupBy('iso_country', 'iso_region').agg(count(\"*\")).show()\n",
    "    #l = ['US']\n",
    "    newdf = newdf.withColumn(\"myisocountry\", split(F.col(\"iso_region\"), \"-\").getItem(0)) \\\n",
    "                .withColumn(\"myisoregion\", split(F.col(\"iso_region\"), \"-\").getItem(1))\n",
    "    newdf = newdf.withColumn(\"myisocountry\",coalesce(newdf.myisocountry,newdf.iso_country))\n",
    "    drop_cols = ['myisocountry', 'iso_region', 'local_code']\n",
    "    newdf = newdf.drop(*drop_cols)\n",
    "    airport_code = newdf.filter(~newdf.iata_code.isNull()).dropDuplicates()\n",
    "    df_clean_airport_code = (airport_code.withColumnRenamed(\"ident\", \"ident\") \\\n",
    "                           .withColumnRenamed(\"type\", \"airport_type\") \\\n",
    "                           .withColumnRenamed(\"name\", \"airport_name\") \\\n",
    "                           .withColumnRenamed(\"iso_country\", \"country_iso2\") \\\n",
    "                           .withColumnRenamed(\"municipality\", \"city_name\" ) \\\n",
    "                           .withColumnRenamed(\"iata_code\", \"iata_code\") \\\n",
    "                           .withColumnRenamed(\"myisoregion\", \"state_id\"))\n",
    "    \n",
    "    print('***** Make df_clean_airport_code processing ')\n",
    "    df_clean_airport_code.printSchema()\n",
    "    df_clean_airport_code.show(2)\n",
    "    return(df_clean_airport_code)\n",
    "\n",
    "df_clean_airport_code = clean_airport_code(df_airport_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Airports Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_global_airports(df_global_airports):\n",
    "    drop_cols = [\"icao\",\"type\", \"latitude\", \"longitude\", \"altitude\", \"timezone\", \"dst\", \"tz_timezone\", \"data_source\"]\n",
    "    newdf = df_global_airports.filter(df_global_airports.type.isin('airport', 'unknown')) \\\n",
    "                        .drop(*drop_cols)\n",
    "\n",
    "    df_clean_global_airports = newdf.select(F.col(\"airport_ID\").alias(\"airport_id\").cast(\"int\"), \\\n",
    "                                              F.col(\"name\").alias(\"airport_name\"), \\\n",
    "                                              F.col(\"city\").alias(\"city_name\"), \\\n",
    "                                              F.col(\"country\").alias(\"country_name\"), \\\n",
    "                                              F.col(\"iata\").alias(\"iata_code\")) \\\n",
    "                                    .dropDuplicates()    \n",
    "    print('***** Make df_clean_global_airports processing ')\n",
    "    df_clean_global_airports.printSchema()\n",
    "    df_clean_global_airports.show(2)\n",
    "    return(df_clean_global_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_global_airports = clean_global_airports(df_global_airports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso Country Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_iso_country(df_iso_country):\n",
    "    df_clean_iso_country =  df_iso_country.drop(\"ISO_3166-2\") \\\n",
    "                                .select(F.col(\"Country\").alias(\"country_name\"), \\\n",
    "                                    F.col(\"Alpha_2\").alias(\"country_iso2\"), \\\n",
    "                                    F.col(\"Alpha_3\").alias(\"country_iso3\"),\n",
    "                                    F.col(\"Num_code\").alias(\"country_num\") \\\n",
    "                                .cast(\"int\")) \\\n",
    "                                .dropDuplicates()\n",
    "    print('***** Make df_clean_iso_country processing ')\n",
    "    df_clean_iso_country.printSchema()\n",
    "    df_clean_iso_country.show(2)\n",
    "    return(df_clean_iso_country)\n",
    "\n",
    "df_clean_iso_country = clean_iso_country(df_iso_country) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US cities Demographics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel df_clean_demograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_demograph(df_demograph): \n",
    "    drop_cols = [\"Number_of_Veterans\"]\n",
    "    newdf = df_demograph.drop(*drop_cols) \\\n",
    "                 .select(F.col(\"City\").alias(\"city_name\"), \\\n",
    "                         F.col(\"State\").alias(\"state_name\"), \\\n",
    "                         F.col(\"Median Age\").alias(\"median_age\"), \\\n",
    "                         F.col(\"Male Population\").alias(\"male_population\"), \\\n",
    "                         F.col(\"Female Population\").alias(\"female_population\"), \\\n",
    "                         F.col(\"Total Population\").alias(\"totale_population\"), \\\n",
    "                         F.col(\"Foreign-born\").alias(\"foreign_born\"), \\\n",
    "                         F.col(\"State Code\").alias(\"state_id\"), \\\n",
    "                         F.col(\"Race\").alias(\"ethnic\"), \\\n",
    "                         F.col(\"Count\"))\n",
    "    df_clean_demograph = newdf.groupBy(\"state_name\", \"state_id\", \"city_name\", \"median_age\", \"male_population\", \"female_population\", \"ethnic\") \\\n",
    "                              .agg(F.avg(\"Count\").cast('int').alias(\"ethnic_count\")) \\\n",
    "                              .orderBy(\"state_name\", \"city_name\", \"ethnic\") \\\n",
    "                              .dropDuplicates()\n",
    "    print('***** Make df_clean_demograph processing ')\n",
    "    df_clean_demograph.printSchema()\n",
    "    df_clean_demograph.show(2)\n",
    "    return(df_clean_demograph)\n",
    "\n",
    "df_clean_demograph = clean_demograph(df_demograph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Development Indicators Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_indicator_dev(df_indicator_dev):\n",
    "    # get key words for indicators fields\n",
    "    demography = ['population','birth','death','fertility','mortality','expectancy']\n",
    "    food = ['food','grain','nutrition','calories']\n",
    "    trade = ['trade','import','export','good','shipping','shipment']\n",
    "    health = ['health','desease','hospital','mortality','doctor']\n",
    "    economy = ['income','gdp','gni','deficit','budget','market','stock','bond','infrastructure']\n",
    "    energy = ['fuel','energy','power','emission','electric','electricity']\n",
    "    education = ['education','literacy']\n",
    "    employment =['employed','employment','umemployed','unemployment']\n",
    "    rural = ['rural','village']\n",
    "    urban = ['urban','city']\n",
    "    # select data in '2015'\n",
    "    newdf = df_indicator_dev.where(F.col(\"2015\").isNotNull())\n",
    "    newdf = newdf.withColumnRenamed('2015', 'indic_2015')\n",
    "    newdf = newdf.withColumn('indic_2015', newdf.indic_2015.cast(DecimalType(22, 2)))\n",
    "    # create columns 'Indicator_group' to setup indicator fields\n",
    "    newdf = newdf.withColumn(\n",
    "        \"indicator_group\", \n",
    "        F.when( F.lower(F.col('Indicator Name')).rlike('|'.join(demography)), F.lit('demography').cast('string')) \\\n",
    "        .when( F.lower(F.col('Indicator Name')).rlike('|'.join(food)), F.lit('food').cast('string')) \\\n",
    "        .when( F.lower(F.col('Indicator Name')).rlike('|'.join(trade)), F.lit('trade').cast('string')) \\\n",
    "        .when( F.lower(F.col('Indicator Name')).rlike('|'.join(health)), F.lit('health').cast('string')) \\\n",
    "        .when( F.lower(F.col('Indicator Name')).rlike('|'.join(economy)), F.lit('economy').cast('string')) \\\n",
    "        .when( F.lower(F.col('Indicator Name')).rlike('|'.join(energy)), F.lit('energy').cast('string')) \\\n",
    "        .when( F.lower(F.col('Indicator Name')).rlike('|'.join(education)), F.lit('education').cast('string')) \\\n",
    "        .when( F.lower(F.col('Indicator Name')).rlike('|'.join(employment)), F.lit('employment').cast('string')) \\\n",
    "        .when( F.lower(F.col('Indicator Name')).rlike('|'.join(rural)), F.lit('rural').cast('string')) \\\n",
    "        .when( F.lower(F.col('Indicator Name')).rlike('|'.join(urban)), F.lit('urban').cast('string')))  \n",
    "    # make aggregation \n",
    "    newdf = newdf.groupBy('Country Name', 'Country Code', 'indicator_group') \\\n",
    "             .agg(avg('indic_2015')).alias('avg_2015') \\\n",
    "             .orderBy('Country Name', 'indicator_group') \\\n",
    "             .where(F.col('indicator_group').isNotNull())\n",
    "    df_clean_indicator_dev = newdf \\\n",
    "                        .select(F.col('Country Name').alias('country_name'), \\\n",
    "                                F.col('Country code').alias('country_code'), \\\n",
    "                        'indicator_group', \\\n",
    "                        F.round(F.col('avg(indic_2015)'), 2).alias('avg_2015'))\n",
    "    print('***** Make df_clean_indicator_dev processing ')\n",
    "    df_clean_indicator_dev.printSchema()\n",
    "    df_clean_indicator_dev.show()\n",
    "    return( df_clean_indicator_dev)\n",
    "\n",
    "df_clean_indicator_dev = clean_indicator_dev(df_indicator_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(table, parquet_path):\n",
    "    \"\"\"\n",
    "    write parquet files \n",
    "    \"\"\"\n",
    "    try:\n",
    "        table.write.parquet(parquet_path, mode = 'overwrite')\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%whos DataFrame"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_clean_immigration.printSchema()\n",
    "df_clean_temperature.printSchema() #\n",
    "print(df_clean_temperature.count())\n",
    "#print(df_clean_global_airports.count()) #\n",
    "#df_clean_global_airports.printSchema()#\n",
    "#print(df_clean_airport_code.count())\n",
    "#df_clean_airport_code.printSchema()#\n",
    "#df_clean_iso_country.printSchema()#\n",
    "#print(df_clean_iso_country.count())\n",
    "print(df_clean_demograph.count())\n",
    "#df_clean_demograph.printSchema()\n",
    "#print(df_clean_indicator_dev.count())\n",
    "df_clean_indicator_dev.printSchema()\n",
    "i94_addr.info()\n",
    "i94_ctry.info()\n",
    "i94_mode.info()\n",
    "i94_port.info()\n",
    "i94_visa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_us_airport_table(df_clean_airport_code, df_clean_global_airports):\n",
    "    # create airport table\n",
    "    tac = df_clean_airport_code.alias('tac')\n",
    "    tga = df_clean_global_airports.alias('tga')\n",
    "    df_join = tac.join(tga, ((tac.iata_code == tga.iata_code) | (tac.airport_name == tga.airport_name) | (tac.city_name == tga.city_name)), how='left')\n",
    "    dim_airport_us = df_join \\\n",
    "                    .filter('tga != tga.iata_code \"\"' and 'state_id !=\"\"') \\\n",
    "                    .selectExpr(\"airport_id\",\n",
    "                               \"ident\",\n",
    "                               \"tga.iata_code\",\n",
    "                               \"tac.airport_name\",\n",
    "                               \"tac.city_name\",\n",
    "                               \"state_id\") \\\n",
    "                    .sort('ident') \\\n",
    "                    .dropDuplicates(['iata_code'])\n",
    "    dim_airport_us.printSchema()\n",
    "    print(dim_airport_us.count())\n",
    "    dim_airport_us.show(2)\n",
    "    dim_airport_us.collect()\n",
    "    parquet_path = output_parquet + 'us_airport.parquet'\n",
    "    write_parquet(dim_airport_us, parquet_path)\n",
    "    return(dim_airport_us)\n",
    "\n",
    "dim_airport_us = create_us_airport_table(df_clean_airport_code, df_clean_global_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_airport_us.show(2)\n",
    "usairportParquet=spark.read.parquet(output_parquet + 'us_airport.parquet')\n",
    "print(usairportParquet.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_country_table(df_clean_iso_country, df_clean_temperature, output_parquet):\n",
    "    # create country table\n",
    "    # output_parquet = '../../output/'\n",
    "    tic = df_clean_iso_country.alias('tic')\n",
    "    tt = df_clean_temperature.alias('tt')\n",
    "    df_join = tic.join(tt, (tic.country_name == tt.country), how='left')\n",
    "    dim_country = df_join \\\n",
    "                        .filter('country_num != \"\"' and 'country_iso3 != \"\"') \\\n",
    "                        .drop_duplicates(subset = ['country_name']) \\\n",
    "                        .orderBy('country_name') \\\n",
    "                        .drop('country')\n",
    "    dim_country.show(5)\n",
    "    dim_country.collect()\n",
    "    parquet_path = output_parquet + 'country.parquet'\n",
    "    write_parquet(dim_country, parquet_path)\n",
    "    return(dim_country)\n",
    "\n",
    "dim_country = create_country_table(df_clean_iso_country, df_clean_temperature, output_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryParquet=spark.read.parquet(output_parquet + 'country.parquet')\n",
    "dim_country.show(2)\n",
    "print(countryParquet.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indicator_table(df_clean_indicator_dev, output_parquet):\n",
    "    dim_indicator = df_clean_indicator_dev \\\n",
    "                .filter('country_code !=\"\"') \\\n",
    "                .dropDuplicates() \\\n",
    "                .orderBy('country_name') \\\n",
    "                .select('country_code', 'indicator_group', 'avg_2015' )\n",
    "    dim_indicator.printSchema()\n",
    "    dim_indicator.show()\n",
    "    dim_indicator.collect()\n",
    "    parquet_path = output_parquet + 'indicator.parquet'\n",
    "    write_parquet(dim_indicator, parquet_path)\n",
    "    return(dim_indicator)\n",
    "\n",
    "dim_indicator = create_indicator_table(df_clean_indicator_dev, output_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicatorParquet=spark.read.parquet(output_parquet + 'indicator.parquet')\n",
    "dim_indicator.show(2)\n",
    "print(indicatorParquet.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demography_table(df_clean_demograph,output_parquet):\n",
    "    # create demography table per ethnic per state\n",
    "    dim_demography = df_clean_demograph \\\n",
    "                    .groupBy( 'state_id', 'ethnic') \\\n",
    "                    .agg((avg('ethnic_count').cast(DecimalType(22, 2))).alias('avg_ethnic')) \\\n",
    "                    .dropDuplicates() \\\n",
    "                    .orderBy(\"state_id\")                 \n",
    "\n",
    "    dim_demography.printSchema()\n",
    "    #print(dim_airport_us.count())\n",
    "    dim_demography.show(5)\n",
    "    dim_demography.collect()\n",
    "    parquet_path = output_parquet + 'demograph.parquet'\n",
    "    write_parquet(dim_demography, parquet_path)\n",
    "    return(dim_demography)\n",
    "\n",
    "dim_demography = create_demography_table(df_clean_demograph, output_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographParquet=spark.read.parquet(output_parquet + 'demograph.parquet')\n",
    "dim_demography.show(2)\n",
    "print(demographParquet.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ../../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fact_student_table( df_clean_immigration, dim_airport_us, dim_country, dim_demography, dim_indicator):  \n",
    "    # create facts tables\n",
    "    ti = df_clean_immigration.alias('ti')\n",
    "    ta = dim_airport_us.alias('ta')\n",
    "    tc = dim_country.alias('tc')\n",
    "    td = dim_demography.alias('td')\n",
    "    to = dim_indicator.alias('to')\n",
    "\n",
    "    inner_join = ti.join(ta, \n",
    "                         (ti.state_id_arrival == ta.state_id) & (ti.iata_code == ta.iata_code), \n",
    "                         how='inner') \\\n",
    "                   .join(tc, \n",
    "                         (ti.country_born_num == tc.country_num), \n",
    "                         how='inner') \\\n",
    "                   .join(td,\n",
    "                         (ti.state_id_arrival == td.state_id),\n",
    "                          how = 'inner') \\\n",
    "                    .join(to,\n",
    "                         (to.country_code == tc.country_iso3),\n",
    "                         how = 'inner')\n",
    "\n",
    "\n",
    "    fact_student = inner_join \\\n",
    "                        .selectExpr('ti.id_i94', \n",
    "                                    'ti.year',\n",
    "                                    'ti.month',\n",
    "                                    'ti.country_born_num',\n",
    "                                    'ti.country_res_num', \n",
    "                                    'ti.age',\n",
    "                                    'ti.gender',\n",
    "                                    'td.ethnic',\n",
    "                                    'ti.iata_code',\n",
    "                                    'tc.avg_temperature', \n",
    "                                    'to.avg_2015',\n",
    "                                    'ta.city_name', \n",
    "                                    'ti.visatype') \\\n",
    "                        .withColumn('ti.id_i94', monotonically_increasing_id())\n",
    "    fact_student.printSchema()\n",
    "    fact_student.collect()\n",
    "    fact_student.show(5)\n",
    "    parquet_path = output_parquet + 'fact_student.parquet'\n",
    "    write_parquet(fact_student, parquet_path)\n",
    "    fact_student.count()\n",
    "    return(fact_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_student = create_fact_student_table(df_clean_immigration, dim_airport_us, dim_country, dim_demography, dim_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fact_student.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(fact_student.select([count(when(col(c).isNull(), c)).alias(c) for c in fact_student.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

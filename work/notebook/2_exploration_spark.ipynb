{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Engineering Capstone Project\n",
    "\n",
    "# US Student Immigration\n",
    "> The purpose of this project is to study the foreign students. The goal is to offer Data teams Analysts a selection of data concerning immigration to the United States.\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* [Step 1: Scope the Project and Gather Data](#Step-1:-Scope-the-Project-and-Gather-Data)\n",
    "\n",
    "* [Step 2: Explore and Assess the Data](#Step-2:-Explore-Assess-the-Data) \n",
    "\n",
    "* [Step 3: Define the Data Model](#Step-3:-Define-the-Data-Model)\n",
    "* [Step 4: Run ETL to Model the Data](#Step-4:-Run-ETL-to-Model-the-Data)\n",
    "* [Step 5: Complete Project Write Up](#Step-5:-Complete-Project-Write-Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType, StringType, DecimalType\n",
    "import pyspark.sql.functions as func\n",
    "import datetime as dt\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "                    .enableHiveSupport().getOrCreate()\n",
    "               \n",
    "\n",
    "#pd.set_option(\"display.max.columns\", None)\n",
    "#pd.set_option(\"display.precision\", 2)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "\n",
    "Data warehouse allow us to collect, transform and manage data from varied sources. Then, Data Team Business connect to it and analyse data. \n",
    "Apache Spark has been used to gather data\n",
    "Amazon S3 buckets store the data in parquet files for the Data teams.\n",
    "The main dataset includes data on immigration to the United State.\n",
    "The questions about foreign students and their choice to come to US may be useful to propose services.   \n",
    "How many students arrived in US in April?    \n",
    "Which Airline bring the most student in April?    \n",
    "What are the top city to arrive in the USA?   \n",
    "Where are from?   \n",
    "what are the student profils (age, country born, country indicators)? \n",
    "\n",
    "#### Data Source\n",
    "\n",
    "[Datactionnary](2_data_dictionnary.ipynb) provides informations about dataset and tables used. [This notebook](1_Exploration_python.ipynb) performs a first exploration with Python and explain the datasets, which variables I kept. \n",
    "Edit: decide not used Education-statistics, i found indicators from Indicators developpment\n",
    "\n",
    "Dataset |File |Data Source|Dataframe Name\n",
    "-|-|-|-|\n",
    "I94 Immigration | immigration_data_sample.csv| [US National Tourism and Trade Office](https://travel.trade.gov/research/programs/i94/description.asp)| df_immigration\n",
    "I94 Description Labels  Description|I94_SAS_Labels_Descriptions.SAS |US National Tourism and Trade Office|\n",
    "Global Land Temperature|GlobalLandTemperaturesByCity.csv| [Berkeley Earth](http://berkeleyearth.org/)|df_temperature\n",
    "Global Airports|airports-extended.csv| [OpenFlights.org and user contributions](https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals)|df_global_airports\n",
    "Airports codes |airport-codes_csv.csv| provide by Udacity|df_airport_code\n",
    "Iso country | wikipedia-iso-country-codes.csv|[Wikipedia](https://gist.github.com/radcliff/f09c0f88344a7fcef373)|df_iso_country\n",
    "US Cities Demographic| us-cities-demographics.csv|provide by Udacity|df_demograph\n",
    "Indicators developpment| WDIData.csv| [World Bank](https://www.kaggle.com/xavier14/wdidata)|df_indicator_dev\n",
    "Education-statistics| EdStatsData.csv|provide by Kaggle [World Bank](https://www.kaggle.com/kostya23/worldbankedstatsunarchived)|df_Educ_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94 Immigration Data\n",
    "* Source: https://travel.trade.gov/research/reports/historical/2016.html\n",
    "    * data 'data/18-83510-I94-Data-2016', provide one file per month\n",
    "        * These records are described according to 28 variables and 3M  rows per file\n",
    "        *  It's provide information about Arrival/Departure to foreign visitors        \n",
    "    * I94_SAS_Labels_Description.SAS for variable descriptions\n",
    "    \n",
    "##### Global Land Temperature Data\n",
    "* Source: http://berkeleyearth.org/\n",
    "    * data 'GlobalLandTemperaturesByCity.csv' provide climate information\n",
    "        * Each line correspond to a record of temperature per day from city around the world.\n",
    "        * The GlobalLandTemperaturesByCity.csv has 7 variables and 8599213 rows.\n",
    "        \n",
    "##### Global Airports Data\n",
    "* Source: https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals\n",
    "    * data 'airports-extended.csv'. Some of the data come from public sources and some of it comes from OpenFlights.org user contributions.\n",
    "        * It's provide informatioms about of airports, train stations, and ferry terminals around the world.\n",
    "        * There are 4 variables in 'airports-extended.csv'and 10668 rows\n",
    "        \n",
    "##### Airports Data Description Data\n",
    "* Source: https://datahub.io/core/airport-codes#data\n",
    "    * airport-codes_csv.csv. The airport code refers to the IATA airport code, 3 letters code unique for all airports in the world\n",
    "        * The airport-codes_csv.csv provides informations about aiports.\n",
    "        * There are 55075 rows and 12 columns in airport-codes_csv.csv.\n",
    "        \n",
    "##### Iso country Data\n",
    "* Source: https://gist.github.com/radcliff/f09c0f88344a7fcef373\n",
    "    * data 'wikipedia-iso-country-codes.csv'. This is a database about the different code useful to identify country.\n",
    "        * This table gives us informations about Country codes used to identify each country\n",
    "        * There are 4 variables and 247 rows.\n",
    "        \n",
    "##### US cities Demographics Data\n",
    "* Source: https://data.census.gov/cedsci/. \n",
    "    * data 'us-cities-demographics.csv'. This dataset contains information about the demographics of all US cities and come from the US Census Bureau.\n",
    "        * Provides simple informations about US State population\n",
    "        * Contains 12 variables and 2892 rows\n",
    "        \n",
    "##### World Development Indicators Data\n",
    "* Source: https://www.kaggle.com/xavier14/wdidata\n",
    "    * data 'WDIData.csv'. The primary World Bank collection of development indicators, compiled from officially-recognized international sources. \n",
    "        * It presents the most current and accurate global development data available, and includes national, regional and global estimates.\n",
    "        * Contains 64 variables, most of which are variables per year(1960 to 2018), with economics context and 422137 rows.\n",
    "               \n",
    "##### i94addr Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * US States code defined in I94_SAS_Labels_Description.SAS\n",
    "        * data 'i94addr.csv' provides State Id and State name  \n",
    "        \n",
    "##### i94city_i94res Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94cit_i94res.csv' defined Code Country by 3 digits\n",
    "        * data 'i94cit_i94res.csv' provides Country Id and Country name\n",
    "        \n",
    "##### i94mode Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94mode.csv' defined arrival US\n",
    "        * data 'i94mode.csv' provides code Mode and name Code.\n",
    "        \n",
    "##### i94port Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94port.csv'\n",
    "        * data 'i94port.csv' provides Port Id, Port city and State Id.\n",
    "        \n",
    "##### i94visa Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94visa.csv'\n",
    "        * data 'i94visa.csv' povides code Visa ans Visa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP SPARK AND ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parquet = '../../output/'\n",
    "path = '../../data/'\n",
    "date_time = datetime.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "                    .appName(\"Us_student_immigation\") \\\n",
    "                    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "    return spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### I94 Immigration Data\n",
    "##### Global Land Temperature Data\n",
    "##### Global Airports Data\n",
    "##### Airports Data Description Data\n",
    "##### Iso country Data\n",
    "##### US cities Demographics Data\n",
    "##### World Development Indicators Data\n",
    "##### i94addr Data\n",
    "##### i94city_i94res Data\n",
    "##### i94mode Data\n",
    "##### i94port Data\n",
    "##### i94visa Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enhaut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DOWN](#enbas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Immigration Data\n",
    "#### Exploration\n",
    "\n",
    "* Path = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "* There are 3096313 rows and 29 columns in *i94_apr16_sub.sas7bdat*.\n",
    "* Name of the dataFrame: df_immigration\n",
    "* As we see in [data exploration file](./0_dataset_information.ipynb), some variables are either not present or not very present (visapost, occup, entdepu, insnum)\n",
    "* Variables droped: depdate, count, occup, entdepa, entdepd, entdepu, matflag, biryear, insnum, dtadfile, visapost, fltno, admnum, insnum, dtaddto. \t\n",
    "* Variables used:\n",
    "\n",
    "Column Name | Description |\n",
    "-|-|\n",
    "**cicid**|     ID uniq per record in the dataset \n",
    "**i94yr**|     4 digit year  \n",
    "**i94mon**|    Numeric month \n",
    "**i94cit**|     3 digit code of source city for immigration (Born country) \n",
    "**i94res**|    3 digit code of source country for immigration\n",
    "**i94port**|   Port addmitted through \n",
    "**arrdate**|   Arrival date in the USA\n",
    "**i94mode**|   Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) \n",
    "**i94addr**|   State of arrival \n",
    "**i94bir**|    Age in years \n",
    "**i94visa**|   Visa Code - 1 = Business / 2 = Pleasure / 3 = Student\n",
    "**gender**|    Gender\n",
    "**airline**|   Airline used to arrive in U.S.\n",
    "**admnum**|    Admission number, should be unique and not nullable \n",
    "**visatype**|  Class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read I94 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_immigration(path, file):\n",
    "    df = spark.read \\\n",
    "        .format('com.github.saurfang.sas.spark') \\\n",
    "        .option('header', 'true') \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 3096313 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid|i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum       |fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|6.0  |2016.0|4.0   |692.0 |692.0 |XXX    |20573.0|null   |null   |null   |37.0  |2.0    |1.0  |null    |null    |null |T      |null   |U      |null   |1979.0 |10282016|null  |null  |null   |1.897628485E9|null |B2      |\n",
      "|7.0  |2016.0|4.0   |254.0 |276.0 |ATL    |20551.0|1.0    |AL     |null   |25.0  |3.0    |1.0  |20130811|SEO     |null |G      |null   |Y      |null   |1991.0 |D/S     |M     |null  |null   |3.73679633E9 |00296|F1      |\n",
      "|15.0 |2016.0|4.0   |101.0 |101.0 |WAS    |20545.0|1.0    |MI     |20691.0|55.0  |2.0    |1.0  |20160401|null    |null |T      |O      |null   |M      |1961.0 |09302016|M     |null  |OS     |6.66643185E8 |93   |B2      |\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = '18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# refaire avec S3 et tous les fichiers (get_path_sas_folder parquet file)\n",
    "df_immigration, rows_immig = load_immigration(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/i94_apr16_staging.parquet_20200527\"\n",
      " Done for \"i94_apr16_staging.parquet_20200527\" !\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'((i94_[a-z]{3}[1-9]{2}))', file).group(0)\n",
    "df = df_immigration\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !')\n",
    "\n",
    "path_i94_immigration = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Land Temperature Data\n",
    "#### Exploration\n",
    "* Path = '../../data/GlobalLandTemperaturesByCity.csv\n",
    "* There are 8599212 rows and 7 columns in *GlobalLandTemperaturesByCity.csv*.\n",
    "* Name of the dataFrame: df_temperature\n",
    "\n",
    "* As we see in [data exploration file](./0_dataset_information.ipynb), the first date is in 1743, and we find a row per day per town. So we will make aggregation for this data set and drop 'AverageTemperature' , 'Latitude' and 'Longitude' columns\n",
    "* Variables used:\n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**dt**|Date format YYYY-MM-DD| \n",
    "**AverageTemperature**|Average Temperature for the city to th date dt|\n",
    "**City**| City name| \n",
    "**Country**| Country name |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read GlobalLandTemperaturesByCity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temperature(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 8599212 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|dt                 |AverageTemperature|AverageTemperatureUncertainty|City |Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|6.068             |1.7369999999999999           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1743-12-01 00:00:00|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-01-01 00:00:00|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature, rows_temp = load_temperature(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/GlobalLandTemperaturesByCity_staging.parquet_20200527\"\n",
      " Done for \"GlobalLandTemperaturesByCity_staging.parquet_20200527\" !\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_temperature\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !')\n",
    "\n",
    "path_temperature = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports Code Data\n",
    "#### Exploration\n",
    "* Path = '../../data/airport-codes_csv.csv'\n",
    "* There are 55075 rows and 12 column in *airport-codes_csv.csv*\n",
    "* Name of the DataFrame : df_airport_code\n",
    "* Some variables left more 50% of data (continent, iata_code and local_code) so I kept:\n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**ident**| Unique identifier Airport code|\n",
    "**type**| Type of airport | \n",
    "**name**| Name of the airport | \n",
    "**continent**| Continent | | \n",
    "**iso_country**| ISO code of airport country |\n",
    "**iso_region**| ISO code of the region airport | \n",
    "**municipality**| City name where the airport is located | \n",
    "**iata_code**| IATA code of the airport|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Airports Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_airport_code(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 55075 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+----------------------------------+\n",
      "|ident|type         |name                |elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates                       |\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+----------------------------------+\n",
      "|00A  |heliport     |Total Rf Heliport   |11          |NA       |US         |US-PA     |Bensalem    |00A     |null     |00A       |-74.93360137939453, 40.07080078125|\n",
      "|00AA |small_airport|Aero B Ranch Airport|3435        |NA       |US         |US-KS     |Leoti       |00AA    |null     |00AA      |-101.473911, 38.704022            |\n",
      "|00AK |small_airport|Lowell Field        |450         |NA       |US         |US-AK     |Anchor Point|00AK    |null     |00AK      |-151.695999146, 59.94919968       |\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'airport-codes_csv.csv'    \n",
    "df_airport_code, rows_code = load_airport_code(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/airport-codes_csv_staging.parquet_20200527\"\n",
      " Done for \"airport-codes_csv_staging.parquet_20200527\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_airport_code\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_aiport_code = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Airports Data\n",
    "#### Exploration\n",
    "* Path = '../../data/airports-extended.csv'\n",
    "* There are 10668 rows and 13 columns in *airports-extended.csv*\n",
    "* Name of the dataframe : df_global_airports\n",
    "* No missing value, and I kept:\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**airport_name**|Name of airport|Nadzab Airport|Object\n",
    "**airport_city**|Main city served by airport|Nadzab|Object\n",
    "**airport_country**|Country or territory where airport is located|Papua New Guinea|Object\n",
    "**airport_iata**|3-letter IATA code|LAE|Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read airports-extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_airports_schema = T.StructType([\n",
    "    T.StructField('airport_ID', T.IntegerType(), False),\n",
    "    T.StructField('name', T.StringType(), False),\n",
    "    T.StructField('city', T.StringType(), False),\n",
    "    T.StructField('country', T.StringType(), False),\n",
    "    T.StructField('iata', T.StringType(), False),\n",
    "    T.StructField('icao', T.StringType(), False),\n",
    "    T.StructField('latitude', T.StringType(), False),\n",
    "    T.StructField('longitude', T.StringType(), False),\n",
    "    T.StructField('altitude', T.IntegerType(), False),\n",
    "    T.StructField('timezone', T.StringType(), False),\n",
    "    T.StructField('dst', T.StringType(), False),\n",
    "    T.StructField('tz_timezone', T.StringType(), False),\n",
    "    T.StructField('type', T.StringType(), False),\n",
    "    T.StructField('data_source', T.StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_global_airports(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'True') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .schema(global_airports_schema) \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 10667 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- airport_ID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- iata: string (nullable = true)\n",
      " |-- icao: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- altitude: integer (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- tz_timezone: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- data_source: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+----------+----------------------------+-----------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
      "|airport_ID|name                        |city       |country         |iata|icao|latitude          |longitude         |altitude|timezone|dst|tz_timezone         |type   |data_source|\n",
      "+----------+----------------------------+-----------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
      "|2         |Madang Airport              |Madang     |Papua New Guinea|MAG |AYMD|-5.20707988739    |145.789001465     |20      |10      |U  |Pacific/Port_Moresby|airport|OurAirports|\n",
      "|3         |Mount Hagen Kagamuga Airport|Mount Hagen|Papua New Guinea|HGU |AYMH|-5.826789855957031|144.29600524902344|5388    |10      |U  |Pacific/Port_Moresby|airport|OurAirports|\n",
      "|4         |Nadzab Airport              |Nadzab     |Papua New Guinea|LAE |AYNZ|-6.569803         |146.725977        |239     |10      |U  |Pacific/Port_Moresby|airport|OurAirports|\n",
      "+----------+----------------------------+-----------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'airports-extended.csv'\n",
    "df_global_airports, rows_global = load_global_airports(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/airports-extended_staging.parquet_20200527\"\n",
      " Done for \"airports-extended_staging.parquet_20200527\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_global_airports\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_global_airports = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso Country Data\n",
    "#### Exploration\n",
    "* Path = '../../data/wikipedia-iso-country-codes.csv\n",
    "* There are 246 rows and 5 columns in *wikipedia-iso-country-codes.csv*\n",
    "* Name of the dataframe: df_iso_country\n",
    "* I remove 'ISO 3166-2' column, only one missing value. I choose to replace manually. \n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**Country_name**|Country Name in English|\n",
    "**Alpha2_code**|code 2 letter code for the country|\n",
    "**Alpha3_code**|code 3 letter code for the country|\n",
    "**Numeric_code**|ISO 3166-2 code|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read wikipedia-iso-country-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_country_schema = T.StructType([\n",
    "    T.StructField('Country', T.StringType(), False),\n",
    "    T.StructField('Alpha_2', T.StringType(), False),\n",
    "    T.StructField('Alpha_3', T.StringType(), False),\n",
    "    T.StructField('Num_code', T.StringType(), False),\n",
    "    T.StructField('ISO_3166-2', T.StringType(), True),    \n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iso_country(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .schema(iso_country_schema) \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 246 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Alpha_2: string (nullable = true)\n",
      " |-- Alpha_3: string (nullable = true)\n",
      " |-- Num_code: string (nullable = true)\n",
      " |-- ISO_3166-2: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+--------+-------+-------+--------+-------------+\n",
      "|Country |Alpha_2|Alpha_3|Num_code|ISO_3166-2   |\n",
      "+--------+-------+-------+--------+-------------+\n",
      "|Zimbabwe|ZW     |ZWE    |716     |ISO 3166-2:ZW|\n",
      "|Zambia  |ZM     |ZMB    |894     |ISO 3166-2:ZM|\n",
      "|Yemen   |YE     |YEM    |887     |ISO 3166-2:YE|\n",
      "+--------+-------+-------+--------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'wikipedia-iso-country-codes.csv'\n",
    "df_iso_country, rows_iso = load_iso_country(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/wikipedia-iso-country-codes_staging.parquet_20200527\"\n",
      " Done for \"wikipedia-iso-country-codes_staging.parquet_20200527\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_iso_country\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_iso_country = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US cities Demographics\n",
    "#### Exploration\n",
    "* Path = '../../data/us-cities-demographics.csv\n",
    "* There are 2891 rows and 12 columns in us-cities-demographics.csv\n",
    "* Dataframe name : df_demograph\n",
    "* Missing less than 1% in some variables so I drop 'Number of Veterans', 'Average Household Size' and kept: \n",
    "\n",
    "Column Name | Description | \n",
    "-|-|\n",
    "**City**|Name of the city|\n",
    "**State**|US state of the city|\n",
    "**Median Age**|The median of the age of the population|\n",
    "**Male Population**|Number of the male population|\n",
    "**Female Population**|Number of the female population|\n",
    "**Total Population**|Number of the total population|\n",
    "**Foreign-born**|Number of residents of the city that were not born in the city|\n",
    "**State Code**|Code of the state of the city|\n",
    "**Race**|Race class|\n",
    "**Count**|Number of individual of each race|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read us-cities-demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "demograph_schema = T.StructType([\n",
    "    T.StructField('City', T.StringType(), False),\n",
    "    T.StructField('State', T.StringType(), False),\n",
    "    T.StructField('Median_Age', T.FloatType(), False),\n",
    "    T.StructField('Male_Population', T.IntegerType(), False),\n",
    "    T.StructField('Female_Population', T.IntegerType(), False),\n",
    "    T.StructField('Total_Population', T.IntegerType(), False),\n",
    "    T.StructField('Number_of_Veterans', T.IntegerType(), False),\n",
    "    T.StructField('Foreign-born', T.IntegerType(), False),\n",
    "    T.StructField('Average_Household_Size', T.FloatType(), False),\n",
    "    T.StructField('State_Code', T.StringType(), False),\n",
    "    T.StructField('Race', T.StringType(), False),\n",
    "    T.StructField('Count', T.IntegerType(), False)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demograph(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('delimiter', ';') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .schema(demograph_schema) \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 2891 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median_Age: float (nullable = true)\n",
      " |-- Male_Population: integer (nullable = true)\n",
      " |-- Female_Population: integer (nullable = true)\n",
      " |-- Total_Population: integer (nullable = true)\n",
      " |-- Number_of_Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average_Household_Size: float (nullable = true)\n",
      " |-- State_Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|City         |State        |Median_Age|Male_Population|Female_Population|Total_Population|Number_of_Veterans|Foreign-born|Average_Household_Size|State_Code|Race              |Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|Maryland     |33.8      |40601          |41862            |82463           |1562              |30908       |2.6                   |MD        |Hispanic or Latino|25924|\n",
      "|Quincy       |Massachusetts|41.0      |44129          |49500            |93629           |4147              |32935       |2.39                  |MA        |White             |58723|\n",
      "|Hoover       |Alabama      |38.5      |38040          |46799            |84839           |4819              |8229        |2.58                  |AL        |Asian             |4759 |\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'us-cities-demographics.csv'\n",
    "df_demograph, rows_demo = load_demograph(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/us-cities-demographics_staging.parquet_20200527\"\n",
      " Done for \"us-cities-demographics_staging.parquet_20200527\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_demograph\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_demograph = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Development Indicators Data\n",
    "#### Exploration \n",
    "* Path = '../../data/WDIData.csv\n",
    "* There are 422136 rows and 64 columns in *WDIData.csv*\n",
    "* Dataframe name : df_indicator_dev\n",
    "* This dataset contains 64 variables with economics context , most of which are variables per year(1960 to 2018). Data is missing a lot, between 40% and 91%. I just need the year 2015 to explain the Economic context in the country and make aggregation per country. I kept:\n",
    "\n",
    "Column Name | Description | \n",
    "-|-|\n",
    "**Country Name**|Name of the country|\n",
    "**Country Code**|3 letters code of country|\n",
    "**Indicator Name**|indicators of economic development|conversion factor, GDP (LCU per inter...|\n",
    "**Indicator Code**|letters indicator code|\n",
    "**2016**|one column per year since 1960|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read WDIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator_dev(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file) \\\n",
    "        .select(\"Country Name\",\"Country Code\", \"Indicator Name\", \"Indicator Code\", \"2015\" ) \\\n",
    "        .toDF(\"Country_Name\",\"Country_Code\", \"Indicator_Name\", \"Indicator_Code\", \"2015\")\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 422136 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- Country_Name: string (nullable = true)\n",
      " |-- Country_Code: string (nullable = true)\n",
      " |-- Indicator_Name: string (nullable = true)\n",
      " |-- Indicator_Code: string (nullable = true)\n",
      " |-- 2015: double (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+------------+------------+-------------------------------------------------------------------------+-----------------+----------------+\n",
      "|Country_Name|Country_Code|Indicator_Name                                                           |Indicator_Code   |2015            |\n",
      "+------------+------------+-------------------------------------------------------------------------+-----------------+----------------+\n",
      "|Arab World  |ARB         |2005 PPP conversion factor, GDP (LCU per international $)                |PA.NUS.PPP.05    |null            |\n",
      "|Arab World  |ARB         |2005 PPP conversion factor, private consumption (LCU per international $)|PA.NUS.PRVT.PP.05|null            |\n",
      "|Arab World  |ARB         |Access to clean fuels and technologies for cooking (% of population)     |EG.CFT.ACCS.ZS   |84.1715990242825|\n",
      "+------------+------------+-------------------------------------------------------------------------+-----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'WDIData.csv'\n",
    "df_indicator_dev, rows_dev = load_indicator_dev(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/WDIData_staging.parquet_20200527\"\n",
      " Done for \"WDIData_staging.parquet_20200527\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_indicator_dev\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_indicator_dev = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
      " \n",
      "There are 583 rows in i94port.parquet\n",
      "There are 3 rows in i94visa.parquet\n",
      "There are 55 rows in i94addr.parquet\n",
      "There are 289 rows in i94cit_i94res.parquet\n",
      "There are 4 rows in i94mode.parquet\n",
      " \n",
      "***** Make i94 labels files is done!\n"
     ]
    }
   ],
   "source": [
    "### Create Parquet Files \n",
    "# Parse I94_SAS_Labels_Description.SAS and save in parquet format in '../../data/'\n",
    "file = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "!python parse_file.py $path $file\n",
    "#### Read Parquet files create from 'I94_SAS_Labels_Descriptions.SAS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Description Labels  Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here some data extract from '../../data/I94_SAS_Labels_Description.SAS'.    \n",
    "To explain code in I94-immigration, I create 5 files and read here. The file was cleaned and parsed with the scrip *parse_file.py*. see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/'\n",
    "file = 'I94_SAS_Labels_Descriptions.SAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/I94_SAS_Labels_Descriptions.SAS\n"
     ]
    }
   ],
   "source": [
    "print(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat parse_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
      " \n",
      "There are 583 rows in i94port.parquet\n",
      "There are 3 rows in i94visa.parquet\n",
      "There are 55 rows in i94addr.parquet\n",
      "There are 289 rows in i94cit_i94res.parquet\n",
      "There are 4 rows in i94mode.parquet\n",
      " \n",
      "***** Make i94 labels files is done!\n"
     ]
    }
   ],
   "source": [
    "%run -i parse_file.py $path $file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airport-codes_csv_staging.parquet_20200526\r\n",
      "airport-codes_csv_staging.parquet_20200527\r\n",
      "airports-extended_staging.parquet_20200526\r\n",
      "airports-extended_staging.parquet_20200527\r\n",
      "education-statistics\r\n",
      "GlobalLandTemperaturesByCity_staging.parquet_20200526\r\n",
      "GlobalLandTemperaturesByCity_staging.parquet_20200527\r\n",
      "i94addr.parquet\r\n",
      "i94_apr16_staging.parquet_20200526\r\n",
      "i94_apr16_staging.parquet_20200527\r\n",
      "i94cit_i94res.parquet\r\n",
      "i94mode.parquet\r\n",
      "i94port.parquet\r\n",
      "i94visa.parquet\r\n",
      "us-cities-demographics_staging.parquet_20200526\r\n",
      "us-cities-demographics_staging.parquet_20200527\r\n",
      "WDIData_staging.parquet_20200526\r\n",
      "WDIData_staging.parquet_20200527\r\n",
      "wikipedia-iso-country-codes_staging.parquet_20200526\r\n",
      "wikipedia-iso-country-codes_staging.parquet_20200527\r\n"
     ]
    }
   ],
   "source": [
    "!ls $output_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_code = spark.read.parquet(output_parquet+'airport-codes_csv_staging.parquet_20200526')\n",
    "demograph = spark.read.parquet(output_parquet+'us-cities-demographics_staging.parquet_20200526')\n",
    "immigration = spark.read.parquet(output_parquet+'i94_apr16_staging.parquet_20200526')\n",
    "temperature = spark.read.parquet(output_parquet+'GlobalLandTemperaturesByCity_staging.parquet_20200526')\n",
    "global_airports = spark.read.parquet(output_parquet+'airports-extended_staging.parquet_20200526')\n",
    "iso_country = spark.read.parquet(output_parquet+'wikipedia-iso-country-codes_staging.parquet_20200526')                                    \n",
    "df_indicator_dev = spark.read.parquet(output_parquet+'WDIData_staging.parquet_20200526')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet files\n",
    "path = '../data/'\n",
    "df_immigration = spark.read.parquet(path_i94_immigration)\n",
    "df_temperature = spark.read.parquet(path_temperature)\n",
    "df_airport_code = spark.read.parquet(path_aiport_code)\n",
    "df_global_airports = spark.read.parquet(path_global_airports)\n",
    "df_iso_country = spark.read.parquet(path_iso_country)\n",
    "df_demograph = spark.read.parquet(path_demograph)\n",
    "df_indicator_dev = spark.read.parquet(path_indicator_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rows_educ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5bc145ed73fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#display(f'df_airport_code: There are {df_airport_code.count()} rows from parquet file, {rows_code} before staging')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#display(f'df_demograph : There are {df_demograph.count()} rows from parquet file, {rows_demo} before staging')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'df_educ_data : There are {df_educ_data.count()} rows from parquet file, {rows_educ} before staging'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#display(f'df_global_airports : There are {df_global_airports.count()} rows from parquet file, {rows_global} before staging')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#display(f'df_immigration : There are {df_immigration.count()} rows from parquet file, {rows_immig} before staging')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rows_educ' is not defined"
     ]
    }
   ],
   "source": [
    "display(f'df_airport_code: There are {df_airport_code.count()} rows from parquet file, {rows_code} before staging')\n",
    "display(f'df_demograph : There are {df_demograph.count()} rows from parquet file, {rows_demo} before staging')\n",
    "display(f'df_global_airports : There are {df_global_airports.count()} rows from parquet file, {rows_global} before staging')\n",
    "display(f'df_immigration : There are {df_immigration.count()} rows from parquet file, {rows_immig} before staging')\n",
    "display(f'df_indicator_dev : There are {df_indicator_dev.count()} rows from parquet file, {rows_dev} before staging')\n",
    "display(f'df_iso_country : There are {df_iso_country.count()} rows from parquet file, {rows_iso} before staging')\n",
    "display(f'df_temperature : There are {df_temperature.count()} rows from parquet file, {rows_temp} before staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Dataframe i94_mode *****\n",
      "There are 4 rows.\n",
      " \n",
      "***** Dataframe i94_ctry *****\n",
      "There are 289 rows.\n",
      " \n",
      "***** Dataframe i94_addr *****\n",
      "There are 55 rows.\n",
      " \n",
      "***** Dataframe i94_visa *****\n",
      "There are 3 rows.\n",
      " \n",
      "***** Dataframe i94_port *****\n",
      "There are 583 rows.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "i94_mode = pd.read_parquet(output_parquet+'i94mode.parquet')\n",
    "print(f'***** Dataframe i94_mode *****')\n",
    "print(\"There are {} rows.\".format(len(i94_mode)))\n",
    "print(' ')\n",
    "\n",
    "i94_ctry = pd.read_parquet(output_parquet+'i94cit_i94res.parquet')\n",
    "print(f'***** Dataframe i94_ctry *****')\n",
    "print(\"There are {} rows.\".format(len(i94_ctry)))\n",
    "print(' ')\n",
    "\n",
    "i94_addr = pd.read_parquet(output_parquet+'i94addr.parquet')\n",
    "print(f'***** Dataframe i94_addr *****')\n",
    "print(\"There are {} rows.\".format(len(i94_addr)))\n",
    "print(' ')\n",
    "\n",
    "i94_visa = pd.read_parquet(output_parquet+'i94visa.parquet')\n",
    "print(f'***** Dataframe i94_visa *****')\n",
    "print(\"There are {} rows.\".format(len(i94_visa)))\n",
    "print(' ')\n",
    "\n",
    "i94_port = pd.read_parquet(output_parquet+'i94port.parquet')\n",
    "print(f'***** Dataframe i94_port *****')\n",
    "print(\"There are {} rows.\".format(len(i94_port)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../output/\n",
      "../data/\n"
     ]
    }
   ],
   "source": [
    "print(output_parquet)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameError: name 'df' is not defined\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airport_code',\n",
       " 'demograph',\n",
       " 'df_educ_data',\n",
       " 'df_indicator_dev',\n",
       " 'global_airports',\n",
       " 'immigration',\n",
       " 'iso_country',\n",
       " 'temperature']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%xdel df\n",
    "%who_ls DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Immigration Data\n",
    "* i94addr, missing 152592 values (code State US, 2 letters)\n",
    "    * fill by Port_id from the dataframe 'i94port' \n",
    "    * join on 'df_immigration.i94port == port_state_dic.Port_id', with no missing values\n",
    "    * nul value replace by State_id\n",
    "* int_col = ['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94mode', 'i94bir', 'i94visa']\n",
    "    * fill null by default value from dictionnary and cast the int_col in Integer\n",
    "* str_cols = ['i94addr', 'i94port', 'gender', 'airline', 'visatype']\n",
    "    * fill null by default value from dictionnary\n",
    "* date_col = ['arrdate'(double sas format),'dtadfile'(string YYYYMMDD)]\n",
    "    * 'arrdate' in SAS date format, a value represents the number of days between January 1, 1960, and a other date.\n",
    "    * cast the date and fill the null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_i94_immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%xdel immigration \n",
    "#immigration = spark.read.parquet(path_i94_immigration)\n",
    "immigration = spark.read.parquet(output_parquet+'i94_apr16_staging.parquet_20200526')\n",
    "immigration.createOrReplaceTempView(\"temp_immig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid  i94yr  i94mon  i94cit  i94res  i94port  arrdate  i94mode  i94addr  \\\n",
       "0      0      0       0       0       0        0        0        0        0   \n",
       "\n",
       "   depdate  ...  entdepu  matflag  biryear  dtaddto  gender  insnum  airline  \\\n",
       "0        0  ...        0        0        0        0       0       0        0   \n",
       "\n",
       "   admnum  fltno  visatype  \n",
       "0       0      0         0  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of null values of dataframe in pyspark \n",
    "immigration.select([count(when(col(c).isNull(), c)).alias(c) for c in immigration.columns]).toPandas()\n",
    "#Count of Missing values of dataframe in pyspark \n",
    "immigration.select([count(when(isnan(c), c)).alias(c) for c in immigration.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152079"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of null value in df_immigration.i94port\n",
    "immigration.filter(immigration.i94port.rlike('[A-Z]{3}')) \\\n",
    "              .filter(immigration.i94addr.isNull()) \\\n",
    "              .select(immigration.i94port, immigration.i94addr).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns with a lot of null values and not useful\n",
    "drop_col = ['depdate', 'count', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', \\\n",
    "            'insnum','visapost', 'fltno', 'admnum', 'insnum', 'dtaddto']\n",
    "# create dictionnary from i94_port\n",
    "port_state_dic = dict([(i,a) for i, a in zip(i94_port.Port_id,i94_port.State_id)])\n",
    "# lamda function to get State_id\n",
    "user_func =  udf(lambda x: port_state_dic.get(x))\n",
    "\n",
    "newdf = immigration.drop(*drop_col) \\\n",
    "                   .withColumn('i94addr', F.when((F.col('i94addr').isNull()), \\\n",
    "                                                 user_func(immigration.i94port)) \\\n",
    "                                           .otherwise(F.col('i94addr')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3700"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.filter(newdf.i94port.rlike('[A-Z]{3}')) \\\n",
    "     .filter(newdf.i94addr.isNull()) \\\n",
    "     .select(newdf.i94port, newdf.i94addr).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the null value and cast the columns in integer\n",
    "# int_col = ['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94mode', 'i94bir', 'i94visa']\n",
    "null_int = {'cicid': -1, 'i94yr': -1, 'i94mon': -1,'i94cit': 239, 'i94res': 239, 'i94mode': 9, 'i94bir': -1, 'i94visa': -1}\n",
    "for k in null_int:\n",
    "        newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_int[k])\n",
    "                 .otherwise(F.col(k).cast(\"int\")))\n",
    "        \n",
    "# replace the null value for the string\n",
    "# str_cols = ['i94addr', 'i94port', 'gender', 'airline', 'visatype']\n",
    "null_str = {'i94addr': '99', 'i94port': '999', 'gender': 'U', 'airline': 'unknown', 'visatype': '99' }\n",
    "for k in null_str:\n",
    "        newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_str[k])\n",
    "                                 .otherwise(F.col(k)))\n",
    "        \n",
    "# date_col = ['arrdate'(double sas format),\n",
    "#             'dtadfile'(string YYYYMMDD), \n",
    "null_date = {'arrdate': 'NA', 'dtadfile': 'NA'}\n",
    "setup_date = udf(lambda x: (datetime(1960, 1, 1). date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "newdf = newdf.withColumn(\"arrdate\", setup_date(newdf.arrdate)) \\\n",
    "             .withColumn(\"dtadfile\",to_date(unix_timestamp(col(\"dtadfile\"),\"yyyyMMdd\").cast(\"timestamp\")))\n",
    "for k in null_date:\n",
    "        newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_date[k])\n",
    "                                 .otherwise(F.col(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>gender</th>\n",
       "      <th>airline</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3096313</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "      <td>229</td>\n",
       "      <td>299</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>466</td>\n",
       "      <td>113</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>535</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cicid  i94yr  i94mon  i94cit  i94res  i94port  arrdate  i94mode  i94addr  \\\n",
       "0  3096313      1       1     243     229      299       30        4      466   \n",
       "\n",
       "   i94bir  i94visa  dtadfile  gender  airline  visatype  \n",
       "0     113        3       118       4      535        17  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display distinct value\n",
    "newdf.agg(*(countDistinct(col(c)).alias(c) for c in newdf.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration = newdf.withColumn('dtadfile', to_date(newdf.dtadfile, 'yyyyMMdd').cast('date')) \\\n",
    "                               .withColumn('arrdate', to_date(newdf.arrdate, 'yyyyMMdd').cast('date')) \\\n",
    "                               .dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration = (df_immigration.withColumnRenamed(\"cicid\", \"id_i94\") \\\n",
    "             .withColumnRenamed(\"i94yr\", \"year\") \\\n",
    "             .withColumnRenamed(\"i94mon\", \"month\") \\\n",
    "             .withColumnRenamed(\"i94cit\", \"country_born_num\") \\\n",
    "             .withColumnRenamed(\"i94res\", \"country_res_num\") \\\n",
    "             .withColumnRenamed(\"i94port\", \"iata_code\") \\\n",
    "             .withColumnRenamed(\"arrdate\", \"arr_date\") \\\n",
    "             .withColumnRenamed(\"i94mode\", \"arri_mode\") \\\n",
    "             .withColumnRenamed(\"i94addr\", \"state_id_arrival\") \\\n",
    "             .withColumnRenamed(\"i94bir\", \"age\") \\\n",
    "             .withColumnRenamed(\"i94visa\", \"arr_reason\") \\\n",
    "             .withColumnRenamed(\"dtadfile\", \"dt_add_i94\") \\\n",
    "             .withColumnRenamed(\"dtaddto\", \"depar_max\") \\\n",
    "             .withColumnRenamed(\"gender\", \"gender\") \\\n",
    "             .withColumnRenamed(\"airline\",\"airline\") \\\n",
    "             .withColumnRenamed(\"visatype:\", \"visatype\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_i94: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- country_born_num: integer (nullable = true)\n",
      " |-- country_res_num: integer (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- arr_date: date (nullable = true)\n",
      " |-- arri_mode: integer (nullable = true)\n",
      " |-- state_id_arrival: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- arr_reason: integer (nullable = true)\n",
      " |-- dt_add_i94: date (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "+------+----+-----+----------------+---------------+---------+--------+---------+----------------+---+----------+----------+------+-------+--------+\n",
      "|id_i94|year|month|country_born_num|country_res_num|iata_code|arr_date|arri_mode|state_id_arrival|age|arr_reason|dt_add_i94|gender|airline|visatype|\n",
      "+------+----+-----+----------------+---------------+---------+--------+---------+----------------+---+----------+----------+------+-------+--------+\n",
      "|  4722|2016|    4|             111|            111|      CHM|    null|        3|              NJ| 42|         2|      null|     M|unknown|      WT|\n",
      "| 50048|2016|    4|             148|            112|      PEM|    null|        3|              TX| 45|         1|      null|     M|unknown|      WB|\n",
      "| 51334|2016|    4|             148|            112|      CHM|    null|        3|              NY| 19|         2|      null|     F|unknown|      B2|\n",
      "| 65525|2016|    4|             213|            213|      NYC|    null|        3|              VA| 32|         2|      null|     F|unknown|      B2|\n",
      "| 65526|2016|    4|             213|            213|      LOS|    null|        3|              VA| 32|         1|      null|     F|unknown|      B1|\n",
      "+------+----+-----+----------------+---------------+---------+--------+---------+----------------+---+----------+----------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.printSchema()\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Land Temperature Data\n",
    "\n",
    "* As we see in [data exploration file](./0_dataset_information.ipynb), the first date is in 1743, and we find a row per day per town. \n",
    "    * Make aggregation \n",
    "* drop \"dt\", \"AverageTemperatureUncertainty\" , \"Latitude\" and \"Longitude\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../output/GlobalLandTemperaturesByCity_staging.parquet_20200526\n"
     ]
    }
   ],
   "source": [
    "print(output_parquet+'GlobalLandTemperaturesByCity_staging.parquet_20200526')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = spark.read.parquet(output_parquet+'GlobalLandTemperaturesByCity_staging.parquet_20200526')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------------------+--------+-------+--------+---------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|    City|Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+--------+-------+--------+---------+\n",
      "|1907-07-01 00:00:00|            14.739|                        0.624|Edmonton| Canada|  53.84N|  113.18W|\n",
      "|1907-08-01 00:00:00|            12.001|                        0.603|Edmonton| Canada|  53.84N|  113.18W|\n",
      "+-------------------+------------------+-----------------------------+--------+-------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column \"AverageTemperatureUncertainty\"\n",
    "drop_cols = [\"dt\", \"AverageTemperatureUncertainty\", \"Latitude\", \"Longitude\"]\n",
    "newdf = temperature.drop(*drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf.groupBy('Country', 'City') \\\n",
    "    .agg(avg(\"AverageTemperature\")) \\\n",
    "    .orderBy('Country') \\\n",
    "    .dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = (newdf.withColumnRenamed(\"Country\", \"Country\") \\\n",
    "           .withColumnRenamed(\"City\", \"City\") \\\n",
    "           .withColumnRenamed(\"avg(AverageTemperature)\", \"AverageTemperature\"))\n",
    "newdf = newdf.withColumn(\"AverageTemperature\", newdf.AverageTemperature.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- AverageTemperature: float (nullable = true)\n",
      "\n",
      "+-----------+------+------------------+\n",
      "|    Country|  City|AverageTemperature|\n",
      "+-----------+------+------------------+\n",
      "|Afghanistan|Gardez|          17.27424|\n",
      "|Afghanistan| Kabul|         14.342919|\n",
      "|Afghanistan| Gazni|         10.311996|\n",
      "|Afghanistan|Qunduz|         10.790278|\n",
      "|Afghanistan| Herat|         14.213004|\n",
      "+-----------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature = newdf.orderBy('Country')\n",
    "df_temperature.printSchema()\n",
    "df_temperature.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports Code Data\n",
    "* MIssing value in Iata_code, Municipality in the whole table\n",
    "    * I drop '[\"elevation_ft\",\"continent\", \"gps_code\", \"coordinates\"]'\n",
    "    * I keep : ident, airport_type, airport_name, country_iso, city_name, iata_code, state_id\n",
    "    The missing value in iata_code left with the drop. \n",
    "* I extract the State_id from the split of the local_code and rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_code = spark.read.parquet(output_parquet+'airport-codes_csv_staging.parquet_20200526')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_code.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "# filter closed , heliport and seaplace base airport, small_airport\n",
    "# keep us airport\n",
    "drop_cols = [\"elevation_ft\",\"continent\", \"gps_code\", \"coordinates\"]\n",
    "drop_airport = ['closed', 'heliport', 'seaplane_base', 'small_airport']\n",
    "keep_us = ['US']\n",
    "newdf = airport_code.drop(*drop_cols) \\\n",
    "                    .filter(~airport_code.type.isin(drop_airport)) \\\n",
    "                    .filter(airport_code.iso_country.isin(keep_us))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ident  type  name  iso_country  iso_region  municipality  iata_code  \\\n",
       "0      0     0     0            0           0             0          0   \n",
       "\n",
       "   local_code  \n",
       "0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ident  type  name  iso_country  iso_region  municipality  iata_code  \\\n",
       "0      0     0     0            0           0             5         60   \n",
       "\n",
       "   local_code  \n",
       "0           9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(newdf.select([count(when(isnan(c), c)).alias(c) for c in newdf.columns]).toPandas())\n",
    "display(newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airport_code.groupBy('iso_country', 'iso_region').agg(count(\"*\")).show()\n",
    "#l = ['US']\n",
    "newdf = newdf.withColumn(\"myisocountry\", split(col(\"iso_region\"), \"-\").getItem(0)) \\\n",
    "            .withColumn(\"myisoregion\", split(col(\"iso_region\"), \"-\").getItem(1))\n",
    "newdf = newdf.withColumn(\"myisocountry\",coalesce(newdf.myisocountry,newdf.iso_country))\n",
    "drop_cols = ['myisocountry', 'iso_region', 'local_code']\n",
    "newdf = newdf.drop(*drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "820"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_code = newdf.filter(~newdf.iata_code.isNull()).dropDuplicates()\n",
    "df_airport_code = (df_airport_code.withColumnRenamed(\"ident\", \"ident\") \\\n",
    "                       .withColumnRenamed(\"type\", \"airport_type\") \\\n",
    "                       .withColumnRenamed(\"name\", \"airport_name\") \\\n",
    "                       .withColumnRenamed(\"iso_country\", \"country_iso2\") \\\n",
    "                       .withColumnRenamed(\"municipality\", \"city_name\" ) \\\n",
    "                       .withColumnRenamed(\"iata_code\", \"iata_code\") \\\n",
    "                       .withColumnRenamed(\"myisoregion\", \"state_id\"))\n",
    "df_airport_code.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport_code = df_airport_code.filter(~df_airport_code.airport_type.isin('small_airport'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+------------+----------+---------+--------+\n",
      "|ident|  airport_type|        airport_name|country_iso2| city_name|iata_code|state_id|\n",
      "+-----+--------------+--------------------+------------+----------+---------+--------+\n",
      "| KCWA|medium_airport|Central Wisconsin...|          US|   Mosinee|      CWA|      WI|\n",
      "| KAFW| large_airport|Fort Worth Allian...|          US|Fort Worth|      AFW|      TX|\n",
      "+-----+--------------+--------------------+------------+----------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_code.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>airport_type</th>\n",
       "      <th>airport_name</th>\n",
       "      <th>country_iso2</th>\n",
       "      <th>city_name</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ident  airport_type  airport_name  country_iso2  city_name  iata_code  \\\n",
       "0      0             0             0             0          0          0   \n",
       "\n",
       "   state_id  \n",
       "0         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>airport_type</th>\n",
       "      <th>airport_name</th>\n",
       "      <th>country_iso2</th>\n",
       "      <th>city_name</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ident  airport_type  airport_name  country_iso2  city_name  iata_code  \\\n",
       "0      0             0             0             0          0          0   \n",
       "\n",
       "   state_id  \n",
       "0         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_airport_code.select([count(when(isnan(c), c)).alias(c) for c in df_airport_code.columns]).toPandas())\n",
    "display(df_airport_code.select([count(when(col(c).isNull(), c)).alias(c) for c in df_airport_code.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- airport_type: string (nullable = true)\n",
      " |-- airport_name: string (nullable = true)\n",
      " |-- country_iso2: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- state_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# missing Value in iata code.\n",
    "# TODO: make join with 2 others tables \n",
    "df_airport_code.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Airports Data\n",
    "* There are some missing values.\n",
    "* Data clean: I drop [\"icao\", \"latitude\", \"longitude\", \"altitude\", , \"timezone\", \"dst\", \"tz_timezone\", \"data_source\"] and keep only the airport in 'type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10667"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_airports = spark.read.parquet(output_parquet+'airports-extended_staging.parquet_20200526')\n",
    "newdf = global_airports\n",
    "global_airports.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
      "|airport_ID|                name|       city|         country|iata|icao|          latitude|         longitude|altitude|timezone|dst|         tz_timezone|   type|data_source|\n",
      "+----------+--------------------+-----------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
      "|         2|      Madang Airport|     Madang|Papua New Guinea| MAG|AYMD|    -5.20707988739|     145.789001465|      20|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
      "|         3|Mount Hagen Kagam...|Mount Hagen|Papua New Guinea| HGU|AYMH|-5.826789855957031|144.29600524902344|    5388|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
      "+----------+--------------------+-----------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_airports.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"icao\",\"type\", \"latitude\", \"longitude\", \"altitude\", \"timezone\", \"dst\", \"tz_timezone\", \"data_source\"]\n",
    "newdf = global_airports.filter(global_airports.type.isin('airport', 'unknown')) \\\n",
    "                    .drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_ID</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>iata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airport_ID  name  city  country  iata\n",
       "0           0     0     0        0     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_ID</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>iata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airport_ID  name  city  country  iata\n",
       "0           0     0    44        0     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(newdf.select([count(when(isnan(c), c)).alias(c) for c in newdf.columns]).toPandas())\n",
    "display(newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_airports = newdf.select(col(\"airport_ID\").alias(\"airport_id\").cast(\"int\"), \\\n",
    "                                  col(\"name\").alias(\"airport_name\"), \\\n",
    "                                  col(\"city\").alias(\"city_name\"), \\\n",
    "                                  col(\"country\").alias(\"country_name\"), \\\n",
    "                                  col(\"iata\").alias(\"iata_code\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airport_id: integer (nullable = true)\n",
      " |-- airport_name: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_global_airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+----------------+---------+\n",
      "|airport_id|        airport_name|   city_name|    country_name|iata_code|\n",
      "+----------+--------------------+------------+----------------+---------+\n",
      "|         2|      Madang Airport|      Madang|Papua New Guinea|      MAG|\n",
      "|         3|Mount Hagen Kagam...| Mount Hagen|Papua New Guinea|      HGU|\n",
      "|         4|      Nadzab Airport|      Nadzab|Papua New Guinea|      LAE|\n",
      "|         5|Port Moresby Jack...|Port Moresby|Papua New Guinea|      POM|\n",
      "|         6|Wewak Internation...|       Wewak|Papua New Guinea|      WWK|\n",
      "+----------+--------------------+------------+----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_global_airports.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso Country Data\n",
    "* No missing values\n",
    "* I drop 'ISO_3166-2' and rename columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_country = spark.read.parquet(output_parquet+'wikipedia-iso-country-codes_staging.parquet_20200526')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = iso_country\n",
    "iso_country.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Alpha_2</th>\n",
       "      <th>Alpha_3</th>\n",
       "      <th>Num_code</th>\n",
       "      <th>ISO_3166-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country  Alpha_2  Alpha_3  Num_code  ISO_3166-2\n",
       "0        0        0        0         0           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Alpha_2</th>\n",
       "      <th>Alpha_3</th>\n",
       "      <th>Num_code</th>\n",
       "      <th>ISO_3166-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country  Alpha_2  Alpha_3  Num_code  ISO_3166-2\n",
       "0        0        0        0         0           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(newdf.select([count(when(isnan(c), c)).alias(c) for c in newdf.columns]).toPandas())\n",
    "display(newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+--------+-------------+\n",
      "| Country|Alpha_2|Alpha_3|Num_code|   ISO_3166-2|\n",
      "+--------+-------+-------+--------+-------------+\n",
      "|Zimbabwe|     ZW|    ZWE|     716|ISO 3166-2:ZW|\n",
      "|  Zambia|     ZM|    ZMB|     894|ISO 3166-2:ZM|\n",
      "+--------+-------+-------+--------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iso_country.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Country`' given input columns: [country_name.Country_Name, country_name.indicator_group, country_name.avg(2015)];;\\n'Project ['Country AS country_name#2540, 'Alpha_2 AS country_iso2#2541, 'Alpha_3 AS country_iso3#2542, unresolvedalias(cast('Num_code as int), None)]\\n+- SubqueryAlias `country_name`\\n   +- Sort [Country_Name#2377 ASC NULLS FIRST], true\\n      +- Aggregate [Country_Name#2377, indicator_group#2490], [Country_Name#2377, indicator_group#2490, avg(2015#2381) AS avg(2015)#2519]\\n         +- Project [Country_Name#2377, Country_Code#2378, Indicator_Name#2379, Indicator_Code#2380, 2015#2381, CASE WHEN lower(Indicator_Name#2379) RLIKE population|birth|death|fertility|mortality|expectancy THEN cast(demography as string) WHEN lower(Indicator_Name#2379) RLIKE food|grain|nutrition|calories THEN cast(food as string) WHEN lower(Indicator_Name#2379) RLIKE trade|import|export|good|shipping|shipment THEN cast(trade as string) WHEN lower(Indicator_Name#2379) RLIKE health|desease|hospital|mortality|doctor THEN cast(health as string) WHEN lower(Indicator_Name#2379) RLIKE income|gdp|gni|deficit|budget|market|stock|bond|infrastructure THEN cast(economy as string) WHEN lower(Indicator_Name#2379) RLIKE fuel|energy|power|emission|electric|electricity THEN cast(energy as string) WHEN lower(Indicator_Name#2379) RLIKE education|literacy THEN cast(education as string) WHEN lower(Indicator_Name#2379) RLIKE employed|employment|umemployed|unemployment THEN cast(employment as string) WHEN lower(Indicator_Name#2379) RLIKE rural|village THEN cast(rural as string) WHEN lower(Indicator_Name#2379) RLIKE urban|city THEN cast(urban as string) END AS indicator_group#2490]\\n            +- Relation[Country_Name#2377,Country_Code#2378,Indicator_Name#2379,Indicator_Code#2380,2015#2381] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2243.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Country`' given input columns: [country_name.Country_Name, country_name.indicator_group, country_name.avg(2015)];;\n'Project ['Country AS country_name#2540, 'Alpha_2 AS country_iso2#2541, 'Alpha_3 AS country_iso3#2542, unresolvedalias(cast('Num_code as int), None)]\n+- SubqueryAlias `country_name`\n   +- Sort [Country_Name#2377 ASC NULLS FIRST], true\n      +- Aggregate [Country_Name#2377, indicator_group#2490], [Country_Name#2377, indicator_group#2490, avg(2015#2381) AS avg(2015)#2519]\n         +- Project [Country_Name#2377, Country_Code#2378, Indicator_Name#2379, Indicator_Code#2380, 2015#2381, CASE WHEN lower(Indicator_Name#2379) RLIKE population|birth|death|fertility|mortality|expectancy THEN cast(demography as string) WHEN lower(Indicator_Name#2379) RLIKE food|grain|nutrition|calories THEN cast(food as string) WHEN lower(Indicator_Name#2379) RLIKE trade|import|export|good|shipping|shipment THEN cast(trade as string) WHEN lower(Indicator_Name#2379) RLIKE health|desease|hospital|mortality|doctor THEN cast(health as string) WHEN lower(Indicator_Name#2379) RLIKE income|gdp|gni|deficit|budget|market|stock|bond|infrastructure THEN cast(economy as string) WHEN lower(Indicator_Name#2379) RLIKE fuel|energy|power|emission|electric|electricity THEN cast(energy as string) WHEN lower(Indicator_Name#2379) RLIKE education|literacy THEN cast(education as string) WHEN lower(Indicator_Name#2379) RLIKE employed|employment|umemployed|unemployment THEN cast(employment as string) WHEN lower(Indicator_Name#2379) RLIKE rural|village THEN cast(rural as string) WHEN lower(Indicator_Name#2379) RLIKE urban|city THEN cast(urban as string) END AS indicator_group#2490]\n            +- Relation[Country_Name#2377,Country_Code#2378,Indicator_Name#2379,Indicator_Code#2380,2015#2381] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-7cb1011a4c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Alpha_3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"country_iso3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Num_code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"country_num\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     .cast(\"int\"))\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Country`' given input columns: [country_name.Country_Name, country_name.indicator_group, country_name.avg(2015)];;\\n'Project ['Country AS country_name#2540, 'Alpha_2 AS country_iso2#2541, 'Alpha_3 AS country_iso3#2542, unresolvedalias(cast('Num_code as int), None)]\\n+- SubqueryAlias `country_name`\\n   +- Sort [Country_Name#2377 ASC NULLS FIRST], true\\n      +- Aggregate [Country_Name#2377, indicator_group#2490], [Country_Name#2377, indicator_group#2490, avg(2015#2381) AS avg(2015)#2519]\\n         +- Project [Country_Name#2377, Country_Code#2378, Indicator_Name#2379, Indicator_Code#2380, 2015#2381, CASE WHEN lower(Indicator_Name#2379) RLIKE population|birth|death|fertility|mortality|expectancy THEN cast(demography as string) WHEN lower(Indicator_Name#2379) RLIKE food|grain|nutrition|calories THEN cast(food as string) WHEN lower(Indicator_Name#2379) RLIKE trade|import|export|good|shipping|shipment THEN cast(trade as string) WHEN lower(Indicator_Name#2379) RLIKE health|desease|hospital|mortality|doctor THEN cast(health as string) WHEN lower(Indicator_Name#2379) RLIKE income|gdp|gni|deficit|budget|market|stock|bond|infrastructure THEN cast(economy as string) WHEN lower(Indicator_Name#2379) RLIKE fuel|energy|power|emission|electric|electricity THEN cast(energy as string) WHEN lower(Indicator_Name#2379) RLIKE education|literacy THEN cast(education as string) WHEN lower(Indicator_Name#2379) RLIKE employed|employment|umemployed|unemployment THEN cast(employment as string) WHEN lower(Indicator_Name#2379) RLIKE rural|village THEN cast(rural as string) WHEN lower(Indicator_Name#2379) RLIKE urban|city THEN cast(urban as string) END AS indicator_group#2490]\\n            +- Relation[Country_Name#2377,Country_Code#2378,Indicator_Name#2379,Indicator_Code#2380,2015#2381] parquet\\n\""
     ]
    }
   ],
   "source": [
    "df_iso_country =  newdf.drop(\"ISO_3166-2\") \\\n",
    "                    .select(col(\"Country\").alias(\"country_name\"), \\\n",
    "                            col(\"Alpha_2\").alias(\"country_iso2\"), \\\n",
    "                            col(\"Alpha_3\").alias(\"country_iso3\"),\n",
    "                            col(\"Num_code\").alias(\"country_num\") \\\n",
    "                    .cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------------+-----------+\n",
      "|     country_name|country_iso2|country_iso2|country_num|\n",
      "+-----------------+------------+------------+-----------+\n",
      "|         Zimbabwe|          ZW|         ZWE|        716|\n",
      "|           Zambia|          ZM|         ZMB|        894|\n",
      "|            Yemen|          YE|         YEM|        887|\n",
      "|   Western Sahara|          EH|         ESH|        732|\n",
      "|Wallis and Futuna|          WF|         WLF|        876|\n",
      "+-----------------+------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iso_country.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US cities Demographics\n",
    "* missing values\n",
    "* dataclean\n",
    "* df_demograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "demograph = spark.read.parquet(output_parquet+'us-cities-demographics_staging.parquet_20200526')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demograph.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"Number_of_Veterans\"]\n",
    "newdf = demograph.drop(*drop_cols) \\\n",
    "                 .select(col(\"City\").alias(\"city_name\"), \\\n",
    "                         col(\"State\").alias(\"state_name\"), \\\n",
    "                         col(\"Median_age\").alias(\"median_age\"), \\\n",
    "                         col(\"Male_population\").alias(\"male_population\"), \\\n",
    "                         col(\"Female_population\").alias(\"female_population\"), \\\n",
    "                         col(\"Total_population\").alias(\"totale_population\"), \\\n",
    "                         col(\"Foreign-born\").alias(\"foreign_born\"), \\\n",
    "                         col(\"State_Code\").alias(\"state_id\"), \\\n",
    "                         col(\"Race\").alias(\"ethnic\"), \\\n",
    "                         col(\"Count\").alias(\"ethic_count\").cast(\"int\"))\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf.groupBy(\"state_name\", \"state_id\", \"city_name\", \"median_age\", \"male_population\", \"female_population\", \"ethnic\") \\\n",
    "        .agg(sum(\"ethic_count\")) \\\n",
    "        .orderBy(\"state_name\", \"city_name\", \"ethnic\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+----------+---------------+-----------------+--------------------+----------------+\n",
      "|state_name|state_id| city_name|median_age|male_population|female_population|              ethnic|sum(ethic_count)|\n",
      "+----------+--------+----------+----------+---------------+-----------------+--------------------+----------------+\n",
      "|   Alabama|      AL|Birmingham|      35.6|         102122|           112789|American Indian a...|            1319|\n",
      "|   Alabama|      AL|Birmingham|      35.6|         102122|           112789|               Asian|            1500|\n",
      "|   Alabama|      AL|Birmingham|      35.6|         102122|           112789|Black or African-...|          157985|\n",
      "|   Alabama|      AL|Birmingham|      35.6|         102122|           112789|  Hispanic or Latino|            8940|\n",
      "|   Alabama|      AL|Birmingham|      35.6|         102122|           112789|               White|           51728|\n",
      "+----------+--------+----------+----------+---------------+-----------------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name</th>\n",
       "      <th>state_id</th>\n",
       "      <th>city_name</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>sum(ethic_count)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state_name  state_id  city_name  median_age  male_population  \\\n",
       "0           0         0          0           0                0   \n",
       "\n",
       "   female_population  ethnic  sum(ethic_count)  \n",
       "0                  0       0                 0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name</th>\n",
       "      <th>state_id</th>\n",
       "      <th>city_name</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>sum(ethic_count)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state_name  state_id  city_name  median_age  male_population  \\\n",
       "0           0         0          0           0                3   \n",
       "\n",
       "   female_population  ethnic  sum(ethic_count)  \n",
       "0                  3       0                 0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(newdf.select([count(when(isnan(c), c)).alias(c) for c in newdf.columns]).toPandas())\n",
    "display(newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- state_id: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- ethnic: string (nullable = true)\n",
      " |-- sum(ethic_count): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|         City|        State|Median_Age|Male_Population|Female_Population|Total_Population|Number_of_Veterans|Foreign-born|Average_Household_Size|State_Code|              Race|Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|Hispanic or Latino|25924|\n",
      "|       Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|             White|58723|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demograph.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Development Indicators Data\n",
    "* df_indicator_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_dev = spark.read.parquet(output_parquet+'WDIData_staging.parquet_20200526')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+--------------------+-------------------+\n",
      "|Country_Name|Country_Code|      Indicator_Name|      Indicator_Code|               2015|\n",
      "+------------+------------+--------------------+--------------------+-------------------+\n",
      "|    Honduras|         HND|GDP per capita (c...|      NY.GDP.PCAP.KD|   2052.97245412901|\n",
      "|    Honduras|         HND|GDP per capita (c...|      NY.GDP.PCAP.KN|   20860.1066881336|\n",
      "|    Honduras|         HND|GDP per capita (c...|      NY.GDP.PCAP.CN|   50522.2697103759|\n",
      "|    Honduras|         HND|GDP per capita (c...|      NY.GDP.PCAP.CD|   2286.20003874608|\n",
      "|    Honduras|         HND|GDP per capita gr...|   NY.GDP.PCAP.KD.ZG|   2.04736639148943|\n",
      "|    Honduras|         HND|GDP per capita, P...|   NY.GDP.PCAP.PP.KD|   4247.38174923163|\n",
      "|    Honduras|         HND|GDP per capita, P...|   NY.GDP.PCAP.PP.CD|   4536.13535649536|\n",
      "|    Honduras|         HND|GDP per person em...|   SL.GDP.PCAP.EM.KD|      10284.4609375|\n",
      "|    Honduras|         HND|GDP per unit of e...|EG.GDP.PUSE.KO.PP.KD|               null|\n",
      "|    Honduras|         HND|GDP per unit of e...|   EG.GDP.PUSE.KO.PP|               null|\n",
      "|    Honduras|         HND|GDP, PPP (constan...|   NY.GDP.MKTP.PP.KD|3.87060331006809E10|\n",
      "|    Honduras|         HND|GDP, PPP (current...|   NY.GDP.MKTP.PP.CD|4.13374204683723E10|\n",
      "|    Honduras|         HND|GDP: linked serie...|   NY.GDP.MKTP.CN.AD|        4.604052E11|\n",
      "|    Honduras|         HND|General governmen...|      NE.CON.GOVT.ZS|   14.3052467695847|\n",
      "|    Honduras|         HND|General governmen...|   NE.CON.GOVT.KD.ZG|   2.09197627299706|\n",
      "|    Honduras|         HND|General governmen...|      NE.CON.GOVT.KD|  2.9331998287051E9|\n",
      "|    Honduras|         HND|General governmen...|      NE.CON.GOVT.KN|         2.54208E10|\n",
      "|    Honduras|         HND|General governmen...|      NE.CON.GOVT.CN|         6.58621E10|\n",
      "|    Honduras|         HND|General governmen...|      NE.CON.GOVT.CD| 2.98034780375225E9|\n",
      "|    Honduras|         HND|GHG net emissions...|   EN.CLC.GHGR.MT.CE|               null|\n",
      "+------------+------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indicator_dev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422136"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator_dev.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country_Name: string (nullable = true)\n",
      " |-- Country_Code: string (nullable = true)\n",
      " |-- Indicator_Name: string (nullable = true)\n",
      " |-- Indicator_Code: string (nullable = true)\n",
      " |-- 2015: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indicator_dev.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['South Asia',\n",
       " 'Chad',\n",
       " 'Paraguay',\n",
       " 'Lower middle income',\n",
       " 'Low & middle income',\n",
       " 'Heavily indebted poor countries (HIPC)',\n",
       " 'World',\n",
       " 'Congo, Dem. Rep.',\n",
       " 'Senegal',\n",
       " 'Cabo Verde',\n",
       " 'East Asia & Pacific (IDA & IBRD countries)',\n",
       " 'Sweden',\n",
       " 'Kiribati',\n",
       " 'Least developed countries: UN classification',\n",
       " 'Guyana',\n",
       " 'Eritrea',\n",
       " 'Pacific island small states',\n",
       " 'Philippines',\n",
       " 'Djibouti',\n",
       " 'Tonga',\n",
       " 'Malaysia',\n",
       " 'Singapore',\n",
       " 'Fiji',\n",
       " 'Turkey',\n",
       " 'Malawi',\n",
       " 'Iraq',\n",
       " 'Sint Maarten (Dutch part)',\n",
       " 'Northern Mariana Islands',\n",
       " 'Germany',\n",
       " 'Comoros',\n",
       " 'Cambodia',\n",
       " 'Afghanistan',\n",
       " 'Jordan',\n",
       " 'Maldives',\n",
       " 'Rwanda',\n",
       " 'Sudan',\n",
       " 'Palau',\n",
       " 'France',\n",
       " 'Turks and Caicos Islands',\n",
       " 'Greece',\n",
       " 'Kosovo',\n",
       " 'Middle income',\n",
       " 'Late-demographic dividend',\n",
       " 'Caribbean small states',\n",
       " 'Sri Lanka',\n",
       " 'Macao SAR, China',\n",
       " 'British Virgin Islands',\n",
       " 'Dominica',\n",
       " 'Equatorial Guinea',\n",
       " 'Algeria',\n",
       " 'Togo',\n",
       " 'Not classified',\n",
       " 'Argentina',\n",
       " 'Iran, Islamic Rep.',\n",
       " 'Belgium',\n",
       " 'Angola',\n",
       " 'San Marino',\n",
       " 'Ecuador',\n",
       " 'Qatar',\n",
       " 'Lesotho',\n",
       " 'Madagascar',\n",
       " 'Albania',\n",
       " 'Finland',\n",
       " 'New Caledonia',\n",
       " 'Myanmar',\n",
       " 'Nicaragua',\n",
       " 'Ghana',\n",
       " 'Brunei Darussalam',\n",
       " 'IDA only',\n",
       " 'Pre-demographic dividend',\n",
       " 'Benin',\n",
       " 'Peru',\n",
       " 'Sierra Leone',\n",
       " 'St. Lucia',\n",
       " 'India',\n",
       " 'China',\n",
       " 'Curacao',\n",
       " 'Arab World',\n",
       " 'Latin America & Caribbean (excluding high income)',\n",
       " 'Sub-Saharan Africa',\n",
       " 'United States',\n",
       " 'Belarus',\n",
       " 'St. Kitts and Nevis',\n",
       " 'Kuwait',\n",
       " 'Malta',\n",
       " 'American Samoa',\n",
       " 'Timor-Leste',\n",
       " 'Lao PDR',\n",
       " 'Sao Tome and Principe',\n",
       " 'Marshall Islands',\n",
       " 'Somalia',\n",
       " 'Tuvalu',\n",
       " 'Chile',\n",
       " 'Puerto Rico',\n",
       " 'Tajikistan',\n",
       " 'Venezuela, RB',\n",
       " 'Cayman Islands',\n",
       " 'Isle of Man',\n",
       " 'Croatia',\n",
       " 'Europe & Central Asia',\n",
       " 'Burundi',\n",
       " 'Nigeria',\n",
       " 'Bolivia',\n",
       " 'Andorra',\n",
       " 'Gabon',\n",
       " 'Italy',\n",
       " 'Suriname',\n",
       " 'OECD members',\n",
       " 'Lithuania',\n",
       " 'Norway',\n",
       " 'Turkmenistan',\n",
       " 'Spain',\n",
       " 'Cuba',\n",
       " 'St. Martin (French part)',\n",
       " 'Mauritania',\n",
       " 'North America',\n",
       " 'Congo, Rep.',\n",
       " 'Niger',\n",
       " 'Central African Republic',\n",
       " 'Denmark',\n",
       " 'Bangladesh',\n",
       " 'Barbados',\n",
       " 'Russian Federation',\n",
       " 'Ireland',\n",
       " 'Liechtenstein',\n",
       " 'Gambia, The',\n",
       " 'Thailand',\n",
       " 'Bhutan',\n",
       " 'Monaco',\n",
       " 'Morocco',\n",
       " 'Panama',\n",
       " 'European Union',\n",
       " 'South Asia (IDA & IBRD)',\n",
       " 'Nauru',\n",
       " 'Post-demographic dividend',\n",
       " 'Ukraine',\n",
       " 'Iceland',\n",
       " 'Israel',\n",
       " 'Europe & Central Asia (IDA & IBRD countries)',\n",
       " 'Channel Islands',\n",
       " 'Korea, Dem. People’s Rep.',\n",
       " 'East Asia & Pacific (excluding high income)',\n",
       " 'Oman',\n",
       " 'French Polynesia',\n",
       " 'Cyprus',\n",
       " 'Gibraltar',\n",
       " 'High income',\n",
       " 'Uruguay',\n",
       " 'Mexico',\n",
       " \"Cote d'Ivoire\",\n",
       " 'Aruba',\n",
       " 'Montenegro',\n",
       " 'Estonia',\n",
       " 'Georgia',\n",
       " 'Upper middle income',\n",
       " 'Zimbabwe',\n",
       " 'Indonesia',\n",
       " 'Central Europe and the Baltics',\n",
       " 'Mongolia',\n",
       " 'Guam',\n",
       " 'Guatemala',\n",
       " 'Middle East & North Africa (IDA & IBRD countries)',\n",
       " 'Korea, Rep.',\n",
       " 'Libya',\n",
       " 'Egypt, Arab Rep.',\n",
       " 'Azerbaijan',\n",
       " 'Grenada',\n",
       " 'Armenia',\n",
       " 'Liberia',\n",
       " 'Tunisia',\n",
       " 'Honduras',\n",
       " 'Trinidad and Tobago',\n",
       " 'Saudi Arabia',\n",
       " 'Middle East & North Africa (excluding high income)',\n",
       " 'Uganda',\n",
       " 'Fragile and conflict affected situations',\n",
       " 'Namibia',\n",
       " 'Virgin Islands (U.S.)',\n",
       " 'Switzerland',\n",
       " 'Zambia',\n",
       " 'Latin America & the Caribbean (IDA & IBRD countries)',\n",
       " 'Low income',\n",
       " 'Ethiopia',\n",
       " 'Jamaica',\n",
       " 'Latvia',\n",
       " 'Eswatini',\n",
       " 'Early-demographic dividend',\n",
       " 'South Sudan',\n",
       " 'United Arab Emirates',\n",
       " 'Guinea',\n",
       " 'Hong Kong SAR, China',\n",
       " 'Micronesia, Fed. Sts.',\n",
       " 'Canada',\n",
       " 'East Asia & Pacific',\n",
       " 'Seychelles',\n",
       " 'North Macedonia',\n",
       " 'Faroe Islands',\n",
       " 'Samoa',\n",
       " 'Uzbekistan',\n",
       " 'Kyrgyz Republic',\n",
       " 'Mozambique',\n",
       " 'Czech Republic',\n",
       " 'Belize',\n",
       " 'Brazil',\n",
       " 'Kenya',\n",
       " 'IDA blend',\n",
       " 'St. Vincent and the Grenadines',\n",
       " 'West Bank and Gaza',\n",
       " 'Lebanon',\n",
       " 'Yemen, Rep.',\n",
       " 'Antigua and Barbuda',\n",
       " 'Slovenia',\n",
       " 'Dominican Republic',\n",
       " 'Middle East & North Africa',\n",
       " 'Japan',\n",
       " 'Botswana',\n",
       " 'Tanzania',\n",
       " 'Europe & Central Asia (excluding high income)',\n",
       " 'Luxembourg',\n",
       " 'New Zealand',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Greenland',\n",
       " 'Sub-Saharan Africa (IDA & IBRD countries)',\n",
       " 'Bahamas, The',\n",
       " 'Haiti',\n",
       " 'Euro area',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'Papua New Guinea',\n",
       " 'Cameroon',\n",
       " 'Australia',\n",
       " 'Guinea-Bissau',\n",
       " 'IBRD only',\n",
       " 'Romania',\n",
       " 'Bulgaria',\n",
       " 'Other small states',\n",
       " 'Small states',\n",
       " 'Solomon Islands',\n",
       " 'Nepal',\n",
       " 'Austria',\n",
       " 'Kazakhstan',\n",
       " 'Costa Rica',\n",
       " 'El Salvador',\n",
       " 'Serbia',\n",
       " 'Burkina Faso',\n",
       " 'South Africa',\n",
       " 'Bermuda',\n",
       " 'Bahrain',\n",
       " 'Colombia',\n",
       " 'Hungary',\n",
       " 'IDA total',\n",
       " 'Latin America & Caribbean',\n",
       " 'IDA & IBRD total',\n",
       " 'Pakistan',\n",
       " 'Vanuatu',\n",
       " 'Mauritius',\n",
       " 'Moldova',\n",
       " 'Syrian Arab Republic',\n",
       " 'United Kingdom',\n",
       " 'Slovak Republic',\n",
       " 'Vietnam',\n",
       " 'Mali',\n",
       " 'Netherlands',\n",
       " 'Sub-Saharan Africa (excluding high income)']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count distinc in column\n",
    "#df_indicator_dev.select(F.countDistinct('Country_Name')).show()\n",
    "[i.Country_Name for i in indicator_dev.select('Country_Name').distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "demography = ['population','birth','death','fertility','mortality','expectancy']\n",
    "food = ['food','grain','nutrition','calories']\n",
    "trade = ['trade','import','export','good','shipping','shipment']\n",
    "health = ['health','desease','hospital','mortality','doctor']\n",
    "economy = ['income','gdp','gni','deficit','budget','market','stock','bond','infrastructure']\n",
    "energy = ['fuel','energy','power','emission','electric','electricity']\n",
    "education = ['education','literacy']\n",
    "employment =['employed','employment','umemployed','unemployment']\n",
    "rural = ['rural','village']\n",
    "urban = ['urban','city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+-----------------+---------------+\n",
      "|Country_Name|Country_Code|      Indicator_Name|   Indicator_Code|     indic_2015|\n",
      "+------------+------------+--------------------+-----------------+---------------+\n",
      "|    Honduras|         HND|GDP per capita (c...|   NY.GDP.PCAP.KD|        2052.97|\n",
      "|    Honduras|         HND|GDP per capita (c...|   NY.GDP.PCAP.KN|       20860.11|\n",
      "|    Honduras|         HND|GDP per capita (c...|   NY.GDP.PCAP.CN|       50522.27|\n",
      "|    Honduras|         HND|GDP per capita (c...|   NY.GDP.PCAP.CD|        2286.20|\n",
      "|    Honduras|         HND|GDP per capita gr...|NY.GDP.PCAP.KD.ZG|           2.05|\n",
      "|    Honduras|         HND|GDP per capita, P...|NY.GDP.PCAP.PP.KD|        4247.38|\n",
      "|    Honduras|         HND|GDP per capita, P...|NY.GDP.PCAP.PP.CD|        4536.14|\n",
      "|    Honduras|         HND|GDP per person em...|SL.GDP.PCAP.EM.KD|       10284.46|\n",
      "|    Honduras|         HND|GDP, PPP (constan...|NY.GDP.MKTP.PP.KD| 38706033100.68|\n",
      "|    Honduras|         HND|GDP, PPP (current...|NY.GDP.MKTP.PP.CD| 41337420468.37|\n",
      "|    Honduras|         HND|GDP: linked serie...|NY.GDP.MKTP.CN.AD|460405200000.00|\n",
      "|    Honduras|         HND|General governmen...|   NE.CON.GOVT.ZS|          14.31|\n",
      "|    Honduras|         HND|General governmen...|NE.CON.GOVT.KD.ZG|           2.09|\n",
      "|    Honduras|         HND|General governmen...|   NE.CON.GOVT.KD|  2933199828.71|\n",
      "|    Honduras|         HND|General governmen...|   NE.CON.GOVT.KN| 25420800000.00|\n",
      "|    Honduras|         HND|General governmen...|   NE.CON.GOVT.CN| 65862100000.00|\n",
      "|    Honduras|         HND|General governmen...|   NE.CON.GOVT.CD|  2980347803.75|\n",
      "|    Honduras|         HND|GINI index (World...|      SI.POV.GINI|          49.60|\n",
      "|    Honduras|         HND|GNI (constant 201...|   NY.GNP.MKTP.KD| 17424309123.42|\n",
      "|    Honduras|         HND|  GNI (constant LCU)|   NY.GNP.MKTP.KN|177707524907.05|\n",
      "+------------+------------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#indicator_dev.show(2)\n",
    "#newdf = indicator_dev.where(F.col('2015').isnotNull())\n",
    "newdf = indicator_dev.where(F.col(\"2015\").isNotNull())\n",
    "newdf = newdf.withColumnRenamed('2015', 'indic_2015')\n",
    "newdf = newdf.withColumn('indic_2015', newdf.indic_2015.cast(DecimalType(18, 2)))\n",
    "newdf.show()\n",
    "#df.withColumn('total_sale_volume', df.total_sale_volume.cast(DecimalType(18, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column with a key word link to the indicator_name\n",
    "newdf = newdf.withColumn(\n",
    "    \"indicator_group\", \n",
    "    F.when( F.lower(col('Indicator_Name')).rlike('|'.join(demography)), F.lit('demography').cast('string')) \\\n",
    "    .when( F.lower(col('Indicator_Name')).rlike('|'.join(food)), F.lit('food').cast('string')) \\\n",
    "    .when( F.lower(col('Indicator_Name')).rlike('|'.join(trade)), F.lit('trade').cast('string')) \\\n",
    "    .when( F.lower(col('Indicator_Name')).rlike('|'.join(health)), F.lit('health').cast('string')) \\\n",
    "    .when( F.lower(col('Indicator_Name')).rlike('|'.join(economy)), F.lit('economy').cast('string')) \\\n",
    "    .when( F.lower(col('Indicator_Name')).rlike('|'.join(energy)), F.lit('energy').cast('string')) \\\n",
    "    .when( F.lower(col('Indicator_Name')).rlike('|'.join(education)), F.lit('education').cast('string')) \\\n",
    "    .when( F.lower(col('Indicator_Name')).rlike('|'.join(employment)), F.lit('employment').cast('string')) \\\n",
    "    .when( F.lower(col('Indicator_Name')).rlike('|'.join(rural)), F.lit('rural').cast('string')) \\\n",
    "    .when( F.lower(col('Indicator_Name')).rlike('|'.join(urban)), F.lit('urban').cast('string')))         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf.filter(newdf.indicator_group.isNull()).select('Indicator_Name').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf.groupBy('Country_Name', 'Country_code', 'indicator_group') \\\n",
    "             .agg(avg('indic_2015')).alias('avg_2015') \\\n",
    "             .orderBy('Country_Name', 'indicator_group') \\\n",
    "             .where(col('indicator_group').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- countr_name: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- indicator_group: string (nullable = true)\n",
      " |-- avg_2015: decimal(22,2) (nullable = true)\n",
      "\n",
      "+-----------+------------+---------------+---------------+\n",
      "|countr_name|country_code|indicator_group|       avg_2015|\n",
      "+-----------+------------+---------------+---------------+\n",
      "|Afghanistan|         AFG|     demography|     1163499.16|\n",
      "|Afghanistan|         AFG|        economy| 34890745342.78|\n",
      "|Afghanistan|         AFG|      education|    18307359.06|\n",
      "|Afghanistan|         AFG|     employment|          33.46|\n",
      "|Afghanistan|         AFG|         energy|   135179888.48|\n",
      "|Afghanistan|         AFG|           food|          51.22|\n",
      "|Afghanistan|         AFG|         health|          42.98|\n",
      "|Afghanistan|         AFG|          trade|  5911911338.74|\n",
      "|Afghanistan|         AFG|          urban|          51.11|\n",
      "|    Albania|         ALB|     demography|       87476.38|\n",
      "|    Albania|         ALB|        economy| 57126217307.84|\n",
      "|    Albania|         ALB|      education|     9495153.70|\n",
      "|    Albania|         ALB|     employment|          33.45|\n",
      "|    Albania|         ALB|         energy|   127275716.69|\n",
      "|    Albania|         ALB|           food|          45.03|\n",
      "|    Albania|         ALB|         health|         148.08|\n",
      "|    Albania|         ALB|          trade| 16574014640.65|\n",
      "|    Albania|         ALB|          urban|          72.22|\n",
      "|    Algeria|         DZA|     demography|     1207041.10|\n",
      "|    Algeria|         DZA|        economy|506870799503.61|\n",
      "+-----------+------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indicator_dev = newdf \\\n",
    "    .select(col('Country_name').alias('countr_name'), \\\n",
    "            col('Country_code').alias('country_code'), \\\n",
    "            'indicator_group', \\\n",
    "            F.round(col('avg(indic_2015)'), 2).alias('avg_2015'))\n",
    "df_indicator_dev.printSchema()\n",
    "df_indicator_dev.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [UP](#enhaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_key(my): \n",
    "    for key, value in port_state_dic.items():\n",
    "        if my == key: \n",
    "            return(key, value)\n",
    "        if my == value:\n",
    "                return(key, value)\n",
    "print(get_key('XT'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enbas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i94_mode\n",
    "i94_mode = pd.read_parquet(path+'i94mode.parquet')\n",
    "#i94_mode.head(5)\n",
    "# dictionnary\n",
    "mode_dic = i94_mode.set_index(\"Mode_id\")[\"Mode\"].to_dict()\n",
    "\n",
    "#display(i94_mode.head())\n",
    "#i94_mode = pd.read_csv(path+'i94mode.csv')\n",
    "#display(mode_dic)\n",
    "#{'1': 'air', '2': 'sea', '3': 'land', '9': 'not reported'}\n",
    "\n",
    "# i94cit_i94res\n",
    "path = '../../data/'\n",
    "i94_ctry = pd.read_parquet(path+'i94cit_i94res.parquet')\n",
    "#i94_ctry.head(5)\n",
    "# dictionnary\n",
    "ctry_dic = i94_ctry.set_index(\"Country_id\")['Country'].to_dict()\n",
    "\n",
    "#display(ctry_dic)\n",
    "#{582: 'mexico',\n",
    "# 236: 'afghanistan',\n",
    "# 101: 'albania',...}\n",
    "#display(i94_ctry.head())\n",
    "\n",
    "# i94_addr\n",
    "path = '../../data/'\n",
    "i94_addr = pd.read_parquet(path+'i94addr.parquet')\n",
    "i94_addr.head(5)\n",
    "# dictionnary\n",
    "addr_dic = i94_addr.set_index(\"State_id\")['State'].to_dict()\n",
    "#display(addr_dic)\n",
    "#{'al': 'alabama',\n",
    "# 'ak': 'alaska',\n",
    "# 'az': 'arizona',...}\n",
    "#display(i94_addr.head())\n",
    "\n",
    "# i94_visa\n",
    "path = '../../data/'\n",
    "i94_visa = pd.read_parquet(path+'i94visa.parquet')\n",
    "#i94_visa.head(5)\n",
    "# dictionnary\n",
    "visa_dic = i94_visa.set_index('Code_visa')['Visa'].to_dict()\n",
    "\n",
    "#display(visa_dic)\n",
    "# {1: 'business', 2: 'pleasure', 3: 'student'}\n",
    "#display(i94_visa.head())\n",
    "\n",
    "# i94_port\n",
    "path = '../../data/'\n",
    "i94_port = pd.read_parquet(path+'i94port.parquet')\n",
    "#i94_port.head(5)\n",
    "# dictionnary\n",
    "port_dic= dict([(i,[a,b]) for i, a,b in zip(i94_port.Port_id, i94_port.Port_city,i94_port.State_id)])\n",
    "port_state_dic = dict([(i,a) for i, a in zip(i94_port.Port_id,i94_port.State_id)])\n",
    "\n",
    "\n",
    "# each row becomes a dictionary where key is column name and value is the data in the cell\n",
    "# [{'Port_id': 'alc', 'Port_city': 'alcan', 'State_id': 'ak'},\n",
    "# {'Port_id': 'anc', 'Port_city': 'anchorage', 'State_id': 'ak'},...]\n",
    "#port_dic = i94_port.to_dict('records')\n",
    "#display(port_dic)\n",
    "#display(i94_port.head())\n",
    "\n",
    "#display(port_state_dic)\n",
    "#display(port_dic)\n",
    "#{'alc': ['alcan', 'ak'],\n",
    "# 'anc': ['anchorage', 'ak'],\n",
    "# 'bar': ['baker aaf - baker island', 'ak'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_null(c):\n",
    "    return when(~(col(c).isNull() | isnan(col(c)) | (trim(col(c)) == \"\")), col(c))\n",
    "\n",
    "\n",
    "df.select([to_null(c).alias(c) for c in df.columns]).na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not duplicates, the twice values are the same\n",
    "display(f\"There are {df_immigration.distinct().count()} distinc values\")\n",
    "display(f\"There are {df_immigration.count()} values\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of null values of dataframe in pyspark \n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_columns = ['cicid', 'i94yr', 'i94mon', 'i94visa', 'gender', 'airline', 'visatype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.where(col(\"i94addr\").isNull()).filter(newdf.i94port =='MIA').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of null values of dataframe in pyspark \n",
    "newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesA = [('',1, 'lun'),('Monkey',2, 'mar'),('',3, 'mer'),('Darth Vader',4, 'jeu')]\n",
    "TableA = spark.createDataFrame(valuesA,['name','myid', 'day'])\n",
    "# Put null in empty cell\n",
    "TableA = TableA.withColumn('name', when(col('name') == '', None).otherwise(col('name')))\n",
    "\n",
    "\n",
    "valuesB = [('Rutabaga',1),('Pirate',2),('Ninja',3),('Darth Vader',4)]\n",
    "TableB = spark.createDataFrame(valuesB,['name','myid'])\n",
    " \n",
    "TableA.show()\n",
    "TableB.show()\n",
    "TableA.createOrReplaceTempView(\"TableA\")\n",
    "TableB.createOrReplaceTempView(\"TableB\")\n",
    "\n",
    "TableA.alias('A').join(TableB.alias('B'), on='myid', how='left') \\\n",
    "                 .select(\n",
    "                        'myid',\n",
    "                        'day',\n",
    "                        F.when(\n",
    "                            F.isnull(F.col('A.name')),\n",
    "                            F.col('B.name')\n",
    "                        ).otherwise(F.col('A.name')).alias('name')\n",
    "                    ) \\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Basically below line of code check all 5 SAL fields and if it is null, replace it with 0. \n",
    "If not keep the original value.\n",
    "\n",
    "df1 = df.withColumn(\"SAL1\", when(df.SAL1.isNull(), lit(0)).otherwise(df.SAL1))\\\n",
    ".withColumn(\"SAL2\", when(df.SAL2.isNull(), lit(0)).otherwise(df.SAL2))\\\n",
    ".withColumn(\"SAL3\", when(df.SAL3.isNull(), lit(0)).otherwise(df.SAL3))\\\n",
    ".withColumn(\"SAL4\", when(df.SAL4.isNull(), lit(0)).otherwise(df.SAL4))\\\n",
    ".withColumn(\"SAL5\", when(df.SAL5.isNull(), lit(0)).otherwise(df.SAL5))\\\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta.join(tb, ta.myid == tb.myid, 'left').select(tb.myid, coalesce(ta.name, tb.name)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#I94_SAS_Labels_Description.SAS\n",
    "#def SAS_parser(file_parse, item, columns):\n",
    "import re\n",
    "import io\n",
    "\n",
    "def parse_file(path_file, key):\n",
    "    \"\"\"\n",
    "    fonction to parse file and create csv file\n",
    "    return dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    #file_parse = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "    with open(path_file, 'r') as f:\n",
    "        file = f.read()\n",
    "    sas_dict={}\n",
    "    key_name = ''\n",
    "\n",
    "    for line in file.split(\"\\n\"):\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        if '/* I94' in line :         \n",
    "            line = line.strip('/* ')\n",
    "            key_name = line.split('-')[0].replace(\"&\", \"_\").replace(\" \", \"\").strip(\" \").lower() \n",
    "            sas_dict[key_name] = []\n",
    "        elif '=' in line and key_name != '' :\n",
    "            #line_trans = re.sub(\"([A-Z]*?),(\\s*?[A-Z]{2}\\s)\",\"\\\\1=\\\\2\", line)\n",
    "            #print(line_trans)\n",
    "            sas_dict[key_name].append([item.strip(' ').strip(\" ';\") for item in line.split('=')])\n",
    "        \n",
    "\n",
    "    if key is \"i94port\":\n",
    "        #pattern = r'[^()]*\\s*\\([^()]*\\)'\n",
    "        columns = [\"Port_id\", \"Port_city\", \"State_id\"]\n",
    "        swap = sas_dict[key]          \n",
    "        sas_dict[key] = []\n",
    "        for x in swap:           \n",
    "            if \",\" in x[1]:\n",
    "                mylist=[]\n",
    "                a = x[1].rsplit(\",\", 1)\n",
    "                b = a[0]\n",
    "                c = a[1].strip()\n",
    "                mylist.extend([x[0], b, c])\n",
    "                sas_dict[key].append(item for item in mylist)\n",
    "\n",
    "                \n",
    "                \n",
    "    if key is \"i94cit_i94res\":\n",
    "        columns = [\"Country_id\", \"Country\"]\n",
    "        swap = sas_dict[key]\n",
    "        for x in swap:\n",
    "            #x[0] = int(x[0])\n",
    "            if \"mexico\" in x[1]:\n",
    "                x[1] = \"mexico\"        \n",
    "        \n",
    "    if key is \"i94mode\":\n",
    "        columns = [\"Mode_id\", \"Mode\"]\n",
    "        #swap = sas_dict[key]\n",
    "        #for x in swap:\n",
    "        #    x[0] = int(x[0])\n",
    "            \n",
    "    if key is \"i94addr\":\n",
    "        columns = [\"State_id\", \"State\"]\n",
    "        \n",
    "    if key is \"i94visa\":\n",
    "        columns = [\"Code_visa\", \"Visa\"]\n",
    "        #swap = sas_dict[key]\n",
    "        #for x in swap:\n",
    "        #    x[0] = int(x[0])\n",
    "            \n",
    "    df = \"\"           \n",
    "            \n",
    "\n",
    "    if key in sas_dict.keys():\n",
    "        if len(sas_dict[key]) > 0:\n",
    "            df = pd.DataFrame(sas_dict[key], columns = columns)\n",
    "            df.sort_values(df.columns[0], inplace=True)\n",
    "        with io.open(f\"../../data/{key}.csv\", \"w\") as f:\n",
    "            df.to_csv(f, index=False) \n",
    "           \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make i94port.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94port\"\n",
    "parse_file(path_file, key)\n",
    "\n",
    "# make i94visa.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94visa\"\n",
    "parse_file(path_file, key)\n",
    "\n",
    "# make i94addr.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94addr\"\n",
    "parse_file(path_file, key) \n",
    "\n",
    "# make i94cit_i94res.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94cit_i94res\"\n",
    "parse_file(path_file, key)\n",
    "\n",
    "# make i94mode.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94mode\"\n",
    "parse_file(path_file, key)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def explore(df):\n",
    "    \"\"\"\n",
    "    Counts number of nulls and nans in each column\n",
    "    \"\"\"\n",
    "    #df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "    df1 = df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns])\n",
    "    df2 = df.select([(count(when(isnan(c) | col(c).isNull(), c))/count(lit(1))).alias(c) for c in df.columns])\n",
    "    df3 = df1.union(df2)\n",
    "    col_counts = df3.agg(*(countDistinct(col(c)).alias(c) for c in df3.columns)).collect()[0].asDict()\n",
    "    # select the cols with count=1 in an array\n",
    "    cols_to_drop = [col for col in df3.columns if col_counts[col] == 1 ]\n",
    "    # drop the selected column\n",
    "    df3.drop(*cols_to_drop).show(truncate=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = '../../data/'\n",
    "i94_visa_spark = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(path+'i94visa.csv')\n",
    "\n",
    "\n",
    "i94_addr_spark = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(path+\"i94addr.csv\")\n",
    "\n",
    "\n",
    "i94_mode_spark = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(path+\"i94mode.csv\")\n",
    "\n",
    "\n",
    "i94_cntry_spark = spark.read.format(\"csv\") \\\n",
    "                 .option(\"header\", \"true\") \\\n",
    "                 .load(path+\"i94cit_i94res.csv\") \\\n",
    "                    \n",
    "\n",
    "\n",
    "i94_port_spark = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(path+\"i94port.csv\")\n",
    "\n",
    "#todo mettre en int ce qu'il faut. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# put all in lowercase \n",
    "for col in df.columns:\n",
    "    df = df.withColumn(col, F.lower(F.col(col)))\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Engineering Capstone Project\n",
    "\n",
    "# US Student Immigration\n",
    "> The purpose of this project is to study the foreign students. The goal is to offer Data teams Analysts a selection of data concerning immigration to the United States.\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* [Step 1: Scope the Project and Gather Data](#Step-1:-Scope-the-Project-and-Gather-Data)\n",
    "\n",
    "* [Step 2: Explore and Assess the Data](#Step-2:-Explore-Assess-the-Data) \n",
    "\n",
    "* [Step 3: Define the Data Model](#Step-3:-Define-the-Data-Model)\n",
    "* [Step 4: Run ETL to Model the Data](#Step-4:-Run-ETL-to-Model-the-Data)\n",
    "* [Step 5: Complete Project Write Up](#Step-5:-Complete-Project-Write-Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as func\n",
    "import datetime as dt\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "                    .enableHiveSupport().getOrCreate()\n",
    "               \n",
    "\n",
    "#pd.set_option(\"display.max.columns\", None)\n",
    "#pd.set_option(\"display.precision\", 2)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "\n",
    "Data warehouse allow us to collect, transform and manage data from varied sources. Then, Data Team Business connect to it and analyse data. \n",
    "Apache Spark has been used to gather data\n",
    "Amazon S3 buckets store the data in parquet files for the Data teams.\n",
    "The main dataset includes data on immigration to the United State.\n",
    "The questions about foreign students and their choice to come to US may be useful to propose services.   \n",
    "How many students arrived in US in April?    \n",
    "Which Airline bring the most student in April?    \n",
    "What are the top city to arrive in the USA?   \n",
    "Where are from?   \n",
    "what are the student profils (age, country born, country indicators)? \n",
    "\n",
    "#### Data Source\n",
    "\n",
    "[Datactionnary](2_data_dictionnary.ipynb) provides informations about dataset and tables used. [This notebook](1_Exploration_python.ipynb) performs a first exploration with Python and explain the datasets, which variables I kept. \n",
    "\n",
    "Dataset |File |Data Source|Dataframe Name\n",
    "-|-|-|-|\n",
    "I94 Immigration | immigration_data_sample.csv| [US National Tourism and Trade Office](https://travel.trade.gov/research/programs/i94/description.asp)| df_immigration\n",
    "I94 Description Labels  Description|I94_SAS_Labels_Descriptions.SAS |US National Tourism and Trade Office|\n",
    "Global Land Temperature|GlobalLandTemperaturesByCity.csv| [Berkeley Earth](http://berkeleyearth.org/)|df_temperature\n",
    "Global Airports|airports-extended.csv| [OpenFlights.org and user contributions](https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals)|df_global_airports\n",
    "Airports codes |airport-codes_csv.csv| provide by Udacity|df_airport_code\n",
    "Iso country | wikipedia-iso-country-codes.csv|[Wikipedia](https://gist.github.com/radcliff/f09c0f88344a7fcef373)|df_iso_country\n",
    "US Cities Demographic| us-cities-demographics.csv|provide by Udacity|df_demograph\n",
    "Indicators developpment| WDIData.csv| [World Bank](https://www.kaggle.com/xavier14/wdidata)|df_indicator_dev\n",
    "Education-statistics| EdStatsData.csv|provide by Kaggle [World Bank](https://www.kaggle.com/kostya23/worldbankedstatsunarchived)|df_Educ_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94 Immigration Data\n",
    "* Source: https://travel.trade.gov/research/reports/historical/2016.html\n",
    "    * data 'data/18-83510-I94-Data-2016', provide one file per month\n",
    "        * These records are described according to 28 variables and 3M  rows per file\n",
    "        *  It's provide information about Arrival/Departure to foreign visitors        \n",
    "    * I94_SAS_Labels_Description.SAS for variable descriptions\n",
    "    \n",
    "##### Global Land Temperature Data\n",
    "* Source: http://berkeleyearth.org/\n",
    "    * data 'GlobalLandTemperaturesByCity.csv' provide climate information\n",
    "        * Each line correspond to a record of temperature per day from city around the world.\n",
    "        * The GlobalLandTemperaturesByCity.csv has 7 variables and 8599213 rows.\n",
    "        \n",
    "##### Global Airports Data\n",
    "* Source: https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals\n",
    "    * data 'airports-extended.csv'. Some of the data come from public sources and some of it comes from OpenFlights.org user contributions.\n",
    "        * It's provide informatioms about of airports, train stations, and ferry terminals around the world.\n",
    "        * There are 4 variables in 'airports-extended.csv'and 10668 rows\n",
    "        \n",
    "##### Airports Data Description Data\n",
    "* Source: https://datahub.io/core/airport-codes#data\n",
    "    * airport-codes_csv.csv. The airport code refers to the IATA airport code, 3 letters code unique for all airports in the world\n",
    "        * The airport-codes_csv.csv provides informations about aiports.\n",
    "        * There are 55075 rows and 12 columns in airport-codes_csv.csv.\n",
    "        \n",
    "##### Iso country Data\n",
    "* Source: https://gist.github.com/radcliff/f09c0f88344a7fcef373\n",
    "    * data 'wikipedia-iso-country-codes.csv'. This is a database about the different code useful to identify country.\n",
    "        * This table gives us informations about Country codes used to identify each country\n",
    "        * There are 4 variables and 247 rows.\n",
    "        \n",
    "##### US cities Demographics Data\n",
    "* Source: https://data.census.gov/cedsci/. \n",
    "    * data 'us-cities-demographics.csv'. This dataset contains information about the demographics of all US cities and come from the US Census Bureau.\n",
    "        * Provides simple informations about US State population\n",
    "        * Contains 12 variables and 2892 rows\n",
    "        \n",
    "##### World Development Indicators Data\n",
    "* Source: https://www.kaggle.com/xavier14/wdidata\n",
    "    * data 'WDIData.csv'. The primary World Bank collection of development indicators, compiled from officially-recognized international sources. \n",
    "        * It presents the most current and accurate global development data available, and includes national, regional and global estimates.\n",
    "        * Contains 64 variables, most of which are variables per year(1960 to 2018), with economics context and 422137 rows.\n",
    "        \n",
    "##### Education statistics Data\n",
    "* Source: https://www.kaggle.com/kostya23/worldbankedstatsunarchived\n",
    "    * data 'education-statistics/EdStatsData.csv' \n",
    "        * This dataset contains 64 variables witheducation context , most of which are variables per year(1970 to 2100) and 886931 rows. \n",
    "        \n",
    "##### i94addr Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * US States code defined in I94_SAS_Labels_Description.SAS\n",
    "        * data 'i94addr.csv' provides State Id and State name  \n",
    "        \n",
    "##### i94city_i94res Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94cit_i94res.csv' defined Code Country by 3 digits\n",
    "        * data 'i94cit_i94res.csv' provides Country Id and Country name\n",
    "        \n",
    "##### i94mode Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94mode.csv' defined arrival US\n",
    "        * data 'i94mode.csv' provides code Mode and name Code.\n",
    "        \n",
    "##### i94port Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94port.csv'\n",
    "        * data 'i94port.csv' provides Port Id, Port city and State Id.\n",
    "        \n",
    "##### i94visa Data\n",
    "* Source: I94_SAS_Labels_Description.SAS\n",
    "    * data 'i94visa.csv'\n",
    "        * data 'i94visa.csv' povides code Visa ans Visa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP SPARK AND ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parquet = '../../output/'\n",
    "path = '../../data/'\n",
    "date_time = datetime.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "                    .appName(\"Us_student_immigation\") \\\n",
    "                    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "    return spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### I94 Immigration Data\n",
    "##### Global Land Temperature Data\n",
    "##### Global Airports Data\n",
    "##### Airports Data Description Data\n",
    "##### Iso country Data\n",
    "##### US cities Demographics Data\n",
    "##### World Development Indicators Data\n",
    "##### Education statistics Data\n",
    "##### i94addr Data\n",
    "##### i94city_i94res Data\n",
    "##### i94mode Data\n",
    "##### i94port Data\n",
    "##### i94visa Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enhaut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DOWN](#enbas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Immigration Data\n",
    "#### Exploration\n",
    "\n",
    "* Path = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "* There are 3096313 rows and 29 columns in *i94_apr16_sub.sas7bdat*.\n",
    "* Name of the dataFrame: df_immigration\n",
    "* As we see in [data exploration file](./0_dataset_information.ipynb), some variables are either not present or not very present (visapost, occup, entdepu, insnum)\n",
    "* Variables droped: depdate, count, occup, entdepa, entdepd, entdepu, matflag, biryear, insnum, dtadfile, visapost, fltno, admnum, insnum.  \t\n",
    "* Variables used:\n",
    "\n",
    "Column Name | Description |\n",
    "-|-|\n",
    "**cicid**|     ID uniq per record in the dataset \n",
    "**i94yr**|     4 digit year  \n",
    "**i94mon**|    Numeric month \n",
    "**i94cit**|     3 digit code of source city for immigration (Born country) \n",
    "**i94res**|    3 digit code of source country for immigration\n",
    "**i94port**|   Port addmitted through \n",
    "**arrdate**|   Arrival date in the USA\n",
    "**i94mode**|   Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) \n",
    "**i94addr**|   State of arrival \n",
    "**i94bir**|    Age in years \n",
    "**i94visa**|   Visa Code - 1 = Business / 2 = Pleasure / 3 = Student\n",
    "**gender**|    Gender\n",
    "**dtaddto**|   Date to which admitted to U.S. (allowed to stay until)\n",
    "**airline**|   Airline used to arrive in U.S.\n",
    "**admnum**|    Admission number, should be unique and not nullable \n",
    "**visatype**|  Class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read I94 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_immigration(path, file):\n",
    "    df = spark.read \\\n",
    "        .format('com.github.saurfang.sas.spark') \\\n",
    "        .option('header', 'true') \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 3096313 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid|i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum       |fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|6.0  |2016.0|4.0   |692.0 |692.0 |XXX    |20573.0|null   |null   |null   |37.0  |2.0    |1.0  |null    |null    |null |T      |null   |U      |null   |1979.0 |10282016|null  |null  |null   |1.897628485E9|null |B2      |\n",
      "|7.0  |2016.0|4.0   |254.0 |276.0 |ATL    |20551.0|1.0    |AL     |null   |25.0  |3.0    |1.0  |20130811|SEO     |null |G      |null   |Y      |null   |1991.0 |D/S     |M     |null  |null   |3.73679633E9 |00296|F1      |\n",
      "|15.0 |2016.0|4.0   |101.0 |101.0 |WAS    |20545.0|1.0    |MI     |20691.0|55.0  |2.0    |1.0  |20160401|null    |null |T      |O      |null   |M      |1961.0 |09302016|M     |null  |OS     |6.66643185E8 |93   |B2      |\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = '18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# refaire avec S3 et tous les fichiers (get_path_sas_folder parquet file)\n",
    "df_immigration, rows_immig = load_immigration(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/i94_apr16_staging.parquet_20200526\"\n",
      " Done for \"i94_apr16_staging.parquet_20200526\" !\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'((i94_[a-z]{3}[1-9]{2}))', file).group(0)\n",
    "df = df_immigration\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !')\n",
    "\n",
    "path_i94_immigration = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Land Temperature Data\n",
    "#### Exploration\n",
    "* Path = '../../data/GlobalLandTemperaturesByCity.csv\n",
    "* There are 8599212 rows and 7 columns in *GlobalLandTemperaturesByCity.csv*.\n",
    "* Name of the dataFrame: df_temperature\n",
    "\n",
    "* As we see in [data exploration file](./0_dataset_information.ipynb), the first date is in 1743, and we find a row per day per town. So we will make aggregation for this data set and drop 'AverageTemperature' , 'Latitude' and 'Longitude' columns\n",
    "* Variables used:\n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**dt**|Date format YYYY-MM-DD| \n",
    "**AverageTemperature**|Average Temperature for the city to th date dt|\n",
    "**City**| City name| \n",
    "**Country**| Country name |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read GlobalLandTemperaturesByCity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temperature(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 8599212 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|dt                 |AverageTemperature|AverageTemperatureUncertainty|City |Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|6.068             |1.7369999999999999           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1743-12-01 00:00:00|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-01-01 00:00:00|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature, rows_temp = load_temperature(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/GlobalLandTemperaturesByCity_staging.parquet_20200526\"\n",
      " Done for \"GlobalLandTemperaturesByCity_staging.parquet_20200526\" !\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_temperature\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !')\n",
    "\n",
    "path_temperature = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports Code Data\n",
    "#### Exploration\n",
    "* Path = '../../data/airport-codes_csv.csv'\n",
    "* There are 55075 rows and 12 column in *airport-codes_csv.csv*\n",
    "* Name of the DataFrame : df_airport_code\n",
    "* Some variables left more 50% of data (continent, iata_code and local_code) so I kept:\n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**ident**| Unique identifier Airport code|\n",
    "**type**| Type of airport | \n",
    "**name**| Name of the airport | \n",
    "**continent**| Continent | | \n",
    "**iso_country**| ISO code of airport country |\n",
    "**iso_region**| ISO code of the region airport | \n",
    "**municipality**| City name where the airport is located | \n",
    "**iata_code**| IATA code of the airport|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Airports Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_airport_code(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 55075 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+----------------------------------+\n",
      "|ident|type         |name                |elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates                       |\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+----------------------------------+\n",
      "|00A  |heliport     |Total Rf Heliport   |11          |NA       |US         |US-PA     |Bensalem    |00A     |null     |00A       |-74.93360137939453, 40.07080078125|\n",
      "|00AA |small_airport|Aero B Ranch Airport|3435        |NA       |US         |US-KS     |Leoti       |00AA    |null     |00AA      |-101.473911, 38.704022            |\n",
      "|00AK |small_airport|Lowell Field        |450         |NA       |US         |US-AK     |Anchor Point|00AK    |null     |00AK      |-151.695999146, 59.94919968       |\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'airport-codes_csv.csv'    \n",
    "df_airport_code, rows_code = load_airport_code(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/airport-codes_csv_staging.parquet_20200526\"\n",
      " Done for \"airport-codes_csv_staging.parquet_20200526\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_airport_code\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_aiport_code = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Airports Data\n",
    "#### Exploration\n",
    "* Path = '../../data/airports-extended.csv'\n",
    "* There are 10668 rows and 13 columns in *airports-extended.csv*\n",
    "* Name of the dataframe : df_global_airports\n",
    "* No missing value, and I kept:\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "**airport_name**|Name of airport|Nadzab Airport|Object\n",
    "**airport_city**|Main city served by airport|Nadzab|Object\n",
    "**airport_country**|Country or territory where airport is located|Papua New Guinea|Object\n",
    "**airport_iata**|3-letter IATA code|LAE|Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read airports-extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_airports_schema = T.StructType([\n",
    "    T.StructField('airport_ID', T.IntegerType(), False),\n",
    "    T.StructField('name', T.StringType(), False),\n",
    "    T.StructField('city', T.StringType(), False),\n",
    "    T.StructField('country', T.StringType(), False),\n",
    "    T.StructField('iata', T.StringType(), False),\n",
    "    T.StructField('icao', T.StringType(), False),\n",
    "    T.StructField('latitude', T.StringType(), False),\n",
    "    T.StructField('longitude', T.StringType(), False),\n",
    "    T.StructField('altitude', T.IntegerType(), False),\n",
    "    T.StructField('timezone', T.StringType(), False),\n",
    "    T.StructField('dst', T.StringType(), False),\n",
    "    T.StructField('tz_timezone', T.StringType(), False),\n",
    "    T.StructField('type', T.StringType(), False),\n",
    "    T.StructField('data_source', T.StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_global_airports(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'True') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .schema(global_airports_schema) \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 10667 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- airport_ID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- iata: string (nullable = true)\n",
      " |-- icao: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- altitude: integer (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- tz_timezone: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- data_source: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+----------+----------------------------+-----------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
      "|airport_ID|name                        |city       |country         |iata|icao|latitude          |longitude         |altitude|timezone|dst|tz_timezone         |type   |data_source|\n",
      "+----------+----------------------------+-----------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
      "|2         |Madang Airport              |Madang     |Papua New Guinea|MAG |AYMD|-5.20707988739    |145.789001465     |20      |10      |U  |Pacific/Port_Moresby|airport|OurAirports|\n",
      "|3         |Mount Hagen Kagamuga Airport|Mount Hagen|Papua New Guinea|HGU |AYMH|-5.826789855957031|144.29600524902344|5388    |10      |U  |Pacific/Port_Moresby|airport|OurAirports|\n",
      "|4         |Nadzab Airport              |Nadzab     |Papua New Guinea|LAE |AYNZ|-6.569803         |146.725977        |239     |10      |U  |Pacific/Port_Moresby|airport|OurAirports|\n",
      "+----------+----------------------------+-----------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'airports-extended.csv'\n",
    "df_global_airports, rows_global = load_global_airports(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/airports-extended_staging.parquet_20200526\"\n",
      " Done for \"airports-extended_staging.parquet_20200526\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_global_airports\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_global_airports = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso Country Data\n",
    "#### Exploration\n",
    "* Path = '../../data/wikipedia-iso-country-codes.csv\n",
    "* There are 246 rows and 5 columns in *wikipedia-iso-country-codes.csv*\n",
    "* Name of the dataframe: df_iso_country\n",
    "* I remove 'ISO 3166-2' column, only one missing value. I choose to replace manually. \n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**Country_name**|Country Name in English|\n",
    "**Alpha2_code**|code 2 letter code for the country|\n",
    "**Alpha3_code**|code 3 letter code for the country|\n",
    "**Numeric_code**|ISO 3166-2 code|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read wikipedia-iso-country-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_country_schema = T.StructType([\n",
    "    T.StructField('Country', T.StringType(), False),\n",
    "    T.StructField('Alpha_2', T.StringType(), False),\n",
    "    T.StructField('Alpha_3', T.StringType(), False),\n",
    "    T.StructField('Num_code', T.StringType(), False),\n",
    "    T.StructField('ISO_3166-2', T.StringType(), True),    \n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iso_country(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .schema(iso_country_schema) \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 246 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Alpha_2: string (nullable = true)\n",
      " |-- Alpha_3: string (nullable = true)\n",
      " |-- Num_code: string (nullable = true)\n",
      " |-- ISO_3166-2: string (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+--------+-------+-------+--------+-------------+\n",
      "|Country |Alpha_2|Alpha_3|Num_code|ISO_3166-2   |\n",
      "+--------+-------+-------+--------+-------------+\n",
      "|Zimbabwe|ZW     |ZWE    |716     |ISO 3166-2:ZW|\n",
      "|Zambia  |ZM     |ZMB    |894     |ISO 3166-2:ZM|\n",
      "|Yemen   |YE     |YEM    |887     |ISO 3166-2:YE|\n",
      "+--------+-------+-------+--------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'wikipedia-iso-country-codes.csv'\n",
    "df_iso_country, rows_iso = load_iso_country(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/wikipedia-iso-country-codes_staging.parquet_20200526\"\n",
      " Done for \"wikipedia-iso-country-codes_staging.parquet_20200526\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_iso_country\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_iso_country = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US cities Demographics\n",
    "#### Exploration\n",
    "* Path = '../../data/us-cities-demographics.csv\n",
    "* There are 2891 rows and 12 columns in us-cities-demographics.csv\n",
    "* Dataframe name : df_demograph\n",
    "* Missing less than 1% in some variables so I drop 'Number of Veterans', 'Average Household Size' and kept: \n",
    "\n",
    "Column Name | Description | \n",
    "-|-|\n",
    "**City**|Name of the city|\n",
    "**State**|US state of the city|\n",
    "**Median Age**|The median of the age of the population|\n",
    "**Male Population**|Number of the male population|\n",
    "**Female Population**|Number of the female population|\n",
    "**Total Population**|Number of the total population|\n",
    "**Foreign-born**|Number of residents of the city that were not born in the city|\n",
    "**State Code**|Code of the state of the city|\n",
    "**Race**|Race class|\n",
    "**Count**|Number of individual of each race|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read us-cities-demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "demograph_schema = T.StructType([\n",
    "    T.StructField('City', T.StringType(), False),\n",
    "    T.StructField('State', T.StringType(), False),\n",
    "    T.StructField('Median_Age', T.FloatType(), False),\n",
    "    T.StructField('Male_Population', T.IntegerType(), False),\n",
    "    T.StructField('Female_Population', T.IntegerType(), False),\n",
    "    T.StructField('Total_Population', T.IntegerType(), False),\n",
    "    T.StructField('Number_of_Veterans', T.IntegerType(), False),\n",
    "    T.StructField('Foreign-born', T.IntegerType(), False),\n",
    "    T.StructField('Average_Household_Size', T.FloatType(), False),\n",
    "    T.StructField('State_Code', T.StringType(), False),\n",
    "    T.StructField('Race', T.StringType(), False),\n",
    "    T.StructField('Count', T.IntegerType(), False)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demograph(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('delimiter', ';') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .schema(demograph_schema) \\\n",
    "        .load(path+file)\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 2891 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median_Age: float (nullable = true)\n",
      " |-- Male_Population: integer (nullable = true)\n",
      " |-- Female_Population: integer (nullable = true)\n",
      " |-- Total_Population: integer (nullable = true)\n",
      " |-- Number_of_Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average_Household_Size: float (nullable = true)\n",
      " |-- State_Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|City         |State        |Median_Age|Male_Population|Female_Population|Total_Population|Number_of_Veterans|Foreign-born|Average_Household_Size|State_Code|Race              |Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|Maryland     |33.8      |40601          |41862            |82463           |1562              |30908       |2.6                   |MD        |Hispanic or Latino|25924|\n",
      "|Quincy       |Massachusetts|41.0      |44129          |49500            |93629           |4147              |32935       |2.39                  |MA        |White             |58723|\n",
      "|Hoover       |Alabama      |38.5      |38040          |46799            |84839           |4819              |8229        |2.58                  |AL        |Asian             |4759 |\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'us-cities-demographics.csv'\n",
    "df_demograph, rows_demo = load_demograph(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/us-cities-demographics_staging.parquet_20200526\"\n",
      " Done for \"us-cities-demographics_staging.parquet_20200526\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_demograph\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_demograph = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Development Indicators Data\n",
    "#### Exploration \n",
    "* Path = '../../data/WDIData.csv\n",
    "* There are 422136 rows and 64 columns in *WDIData.csv*\n",
    "* Dataframe name : df_indicator_dev\n",
    "* This dataset contains 64 variables with economics context , most of which are variables per year(1960 to 2018). Data is missing a lot, between 40% and 91%. I just need the year 2015 to explain the Economic context in the country and make aggregation per country. I kept:\n",
    "\n",
    "Column Name | Description | \n",
    "-|-|\n",
    "**Country Name**|Name of the country|\n",
    "**Country Code**|3 letters code of country|\n",
    "**Indicator Name**|indicators of economic development|conversion factor, GDP (LCU per inter...|\n",
    "**Indicator Code**|letters indicator code|\n",
    "**2016**|one column per year since 1960|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read WDIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator_dev(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file) \\\n",
    "        .select(\"Country Name\",\"Country Code\", \"Indicator Name\", \"Indicator Code\", \"2015\" ) \\\n",
    "        .toDF(\"Country_Name\",\"Country_Code\", \"Indicator_Name\", \"Indicator_Code\", \"2015\")\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 422136 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- Country_Name: string (nullable = true)\n",
      " |-- Country_Code: string (nullable = true)\n",
      " |-- Indicator_Name: string (nullable = true)\n",
      " |-- Indicator_Code: string (nullable = true)\n",
      " |-- 2015: double (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+------------+------------+-------------------------------------------------------------------------+-----------------+----------------+\n",
      "|Country_Name|Country_Code|Indicator_Name                                                           |Indicator_Code   |2015            |\n",
      "+------------+------------+-------------------------------------------------------------------------+-----------------+----------------+\n",
      "|Arab World  |ARB         |2005 PPP conversion factor, GDP (LCU per international $)                |PA.NUS.PPP.05    |null            |\n",
      "|Arab World  |ARB         |2005 PPP conversion factor, private consumption (LCU per international $)|PA.NUS.PRVT.PP.05|null            |\n",
      "|Arab World  |ARB         |Access to clean fuels and technologies for cooking (% of population)     |EG.CFT.ACCS.ZS   |84.1715990242825|\n",
      "+------------+------------+-------------------------------------------------------------------------+-----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'WDIData.csv'\n",
    "df_indicator_dev, rows_dev = load_indicator_dev(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/WDIData_staging.parquet_20200526\"\n",
      " Done for \"WDIData_staging.parquet_20200526\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_indicator_dev\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_indicator_dev = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education statistics Data\n",
    "#### Exploration\n",
    "* Path = 'education-statistics/EdStatsData.csv'\n",
    "* There are 886930 rows and 64 columns in *EdStatsData.csv*\n",
    "* DtaFrane name : 'df_Educ_data'\n",
    "* There more missing value with *EdStatsData.csv* than *WDIData.csv*. I just need the year 2015 to explain the Education context in the country and make aggregation per country. I kept:\n",
    " \n",
    "Column Name | Description \n",
    "-|-|\n",
    "**Country Name**|Name of the country|\n",
    "**Country Code**|3 letters code of country|\n",
    "**Indicator Name**|indicators of education development|\n",
    "**Indicator Code**|letters indicator code|\n",
    "**1970 ...2100**|one column per year since 1970|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read EdStatsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Educ_data(path, file):\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load(path+file) \\\n",
    "        .select(\"Country Name\",\"Country Code\", \"Indicator Name\", \"Indicator Code\", \"2015\" ) \\\n",
    "        .toDF(\"Country_Name\",\"Country_Code\", \"Indicator_Name\", \"Indicator_Code\", \"2015\")\n",
    "    nb_rows = df.count()\n",
    "    print(f'*****         Loading {nb_rows} rows')\n",
    "    print(f'*****         Display the Schema')\n",
    "    df.printSchema()\n",
    "    print(f'*****         Display few rows')\n",
    "    df.show(3, truncate = False)\n",
    "    return df, nb_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****         Loading 886930 rows\n",
      "*****         Display the Schema\n",
      "root\n",
      " |-- Country_Name: string (nullable = true)\n",
      " |-- Country_Code: string (nullable = true)\n",
      " |-- Indicator_Name: string (nullable = true)\n",
      " |-- Indicator_Code: string (nullable = true)\n",
      " |-- 2015: double (nullable = true)\n",
      "\n",
      "*****         Display few rows\n",
      "+------------+------------+-----------------------------------------------------------------------+--------------+----+\n",
      "|Country_Name|Country_Code|Indicator_Name                                                         |Indicator_Code|2015|\n",
      "+------------+------------+-----------------------------------------------------------------------+--------------+----+\n",
      "|Arab World  |ARB         |Adjusted net enrolment rate, lower secondary, both sexes (%)           |UIS.NERA.2    |null|\n",
      "|Arab World  |ARB         |Adjusted net enrolment rate, lower secondary, female (%)               |UIS.NERA.2.F  |null|\n",
      "|Arab World  |ARB         |Adjusted net enrolment rate, lower secondary, gender parity index (GPI)|UIS.NERA.2.GPI|null|\n",
      "+------------+------------+-----------------------------------------------------------------------+--------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'education-statistics/EdStatsData.csv'\n",
    "df_educ_data, rows_educ = load_Educ_data(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Path to file Parquet is:   \"../../output/education-statistics/EdStatsData_staging.parquet_20200526\"\n",
      " Done for \"education-statistics/EdStatsData_staging.parquet_20200526\" !!!\n"
     ]
    }
   ],
   "source": [
    "pattern = re.search(r'(.*)\\.csv$', file).group(1)\n",
    "df = df_educ_data\n",
    "\n",
    "name_file = pattern + '_staging.parquet_' + date_time\n",
    "print(f' Path to file Parquet is:   \"{output_parquet}{name_file}\"')\n",
    "df.write.mode(\"overwrite\").parquet(f'{output_parquet}{name_file}')\n",
    "print(f' Done for \"{name_file}\" !!!')\n",
    "\n",
    "path_educ_data = output_parquet + name_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
      " \n",
      "There are 583 rows in i94port.parquet\n",
      "There are 3 rows in i94visa.parquet\n",
      "There are 55 rows in i94addr.parquet\n",
      "There are 289 rows in i94cit_i94res.parquet\n",
      "There are 4 rows in i94mode.parquet\n",
      " \n",
      "***** Make i94 labels files is done!\n"
     ]
    }
   ],
   "source": [
    "### Create Parquet Files \n",
    "# Parse I94_SAS_Labels_Description.SAS and save in parquet format in '../../data/'\n",
    "file = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "!python parse_file.py $path $file\n",
    "#### Read Parquet files create from 'I94_SAS_Labels_Descriptions.SAS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Description Labels  Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here some data extract from '../../data/I94_SAS_Labels_Description.SAS'.    \n",
    "To explain code in I94-immigration, I create 5 files and read here. The file was cleaned and parsed with the scrip *parse_file.py*. see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/'\n",
    "file = 'I94_SAS_Labels_Descriptions.SAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/I94_SAS_Labels_Descriptions.SAS\n"
     ]
    }
   ],
   "source": [
    "print(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat parse_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
      " \n",
      "There are 583 rows in i94port.parquet\n",
      "There are 3 rows in i94visa.parquet\n",
      "There are 55 rows in i94addr.parquet\n",
      "There are 289 rows in i94cit_i94res.parquet\n",
      "There are 4 rows in i94mode.parquet\n",
      " \n",
      "***** Make i94 labels files is done!\n"
     ]
    }
   ],
   "source": [
    "%run -i parse_file.py $path $file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_dataset_information.ipynb  2_exploration_spark.ipynb\t metastore_db\r\n",
      "1_Exploration_python.ipynb   capstone_exploration.ipynb  parse_file.py\r\n",
      "2_data_dictionnary.ipynb     derby.log\t\t\t postgresql-42.2.10.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la ../../output_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet files\n",
    "path = '../data/'\n",
    "df_immigration = spark.read.parquet(path+path_i94_immigration)\n",
    "df_temperature = spark.read.parquet(path+path_temperature)\n",
    "df_airport_code = spark.read.parquet(path+path_aiport_code)\n",
    "df_global_airports = spark.read.parquet(path+path_global_airports)\n",
    "df_iso_country = spark.read.parquet(path+path_iso_country)\n",
    "df_demograph = spark.read.parquet(path+path_demograph)\n",
    "df_indicator_dev = spark.read.parquet(path+path_indicator_dev)\n",
    "df_educ_data = spark.read.parquet(path+path_educ_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/../../output/i94_apr16_staging.parquet_20200526\n"
     ]
    }
   ],
   "source": [
    "print(path+path_i94_immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_airport_code: There are 55075 rows from parquet file, 55075 before staging'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df_demograph : There are 2891 rows from parquet file, 2891 before staging'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df_educ_data : There are 886930 rows from parquet file, 886930 before staging'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df_global_airports : There are 10667 rows from parquet file, 10667 before staging'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df_immigration : There are 3096313 rows from parquet file, 3096313 before staging'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df_indicator_dev : There are 422136 rows from parquet file, 422136 before staging'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df_iso_country : There are 246 rows from parquet file, 246 before staging'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df_temperature : There are 8599212 rows from parquet file, 8599212 before staging'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f'df_airport_code: There are {df_airport_code.count()} rows from parquet file, {rows_code} before staging')\n",
    "display(f'df_demograph : There are {df_demograph.count()} rows from parquet file, {rows_demo} before staging')\n",
    "display(f'df_educ_data : There are {df_educ_data.count()} rows from parquet file, {rows_educ} before staging')\n",
    "display(f'df_global_airports : There are {df_global_airports.count()} rows from parquet file, {rows_global} before staging')\n",
    "display(f'df_immigration : There are {df_immigration.count()} rows from parquet file, {rows_immig} before staging')\n",
    "display(f'df_indicator_dev : There are {df_indicator_dev.count()} rows from parquet file, {rows_dev} before staging')\n",
    "display(f'df_iso_country : There are {df_iso_country.count()} rows from parquet file, {rows_iso} before staging')\n",
    "display(f'df_temperature : There are {df_temperature.count()} rows from parquet file, {rows_temp} before staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Dataframe i94_mode *****\n",
      "There are 4 rows.\n",
      " \n",
      "***** Dataframe i94_ctry *****\n",
      "There are 289 rows.\n",
      " \n",
      "***** Dataframe i94_addr *****\n",
      "There are 55 rows.\n",
      " \n",
      "***** Dataframe i94_visa *****\n",
      "There are 3 rows.\n",
      " \n",
      "***** Dataframe i94_port *****\n",
      "There are 583 rows.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "i94_mode = pd.read_parquet(output_parquet+'i94mode.parquet')\n",
    "print(f'***** Dataframe i94_mode *****')\n",
    "print(\"There are {} rows.\".format(len(i94_mode)))\n",
    "print(' ')\n",
    "\n",
    "i94_ctry = pd.read_parquet(output_parquet+'i94cit_i94res.parquet')\n",
    "print(f'***** Dataframe i94_ctry *****')\n",
    "print(\"There are {} rows.\".format(len(i94_ctry)))\n",
    "print(' ')\n",
    "\n",
    "i94_addr = pd.read_parquet(output_parquet+'i94addr.parquet')\n",
    "print(f'***** Dataframe i94_addr *****')\n",
    "print(\"There are {} rows.\".format(len(i94_addr)))\n",
    "print(' ')\n",
    "\n",
    "i94_visa = pd.read_parquet(output_parquet+'i94visa.parquet')\n",
    "print(f'***** Dataframe i94_visa *****')\n",
    "print(\"There are {} rows.\".format(len(i94_visa)))\n",
    "print(' ')\n",
    "\n",
    "i94_port = pd.read_parquet(output_parquet+'i94port.parquet')\n",
    "print(f'***** Dataframe i94_port *****')\n",
    "print(\"There are {} rows.\".format(len(i94_port)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../output/\n",
      "../../data/\n"
     ]
    }
   ],
   "source": [
    "print(output_parquet)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df',\n",
       " 'df_airport_code',\n",
       " 'df_demograph',\n",
       " 'df_educ_data',\n",
       " 'df_global_airports',\n",
       " 'df_immigration',\n",
       " 'df_indicator_dev',\n",
       " 'df_iso_country',\n",
       " 'df_temperature',\n",
       " 'i94_addr',\n",
       " 'i94_ctry',\n",
       " 'i94_mode',\n",
       " 'i94_port',\n",
       " 'i94_visa']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%who_ls DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94 Immigration Data\n",
    "* i94addr, missing 152592 values (code State US, 2 letters)\n",
    "    * fill by Port_id from the dataframe 'i94port' \n",
    "    * join on 'df_immigration.i94port == port_state_dic.Port_id', with no missing values\n",
    "    * nul value replace by State_id\n",
    "* int_col = ['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94mode', 'i94bir', 'i94visa']\n",
    "    * fill null by default value from dictionnary and cast the int_col in Integer\n",
    "* str_cols = ['i94addr', 'i94port', 'gender', 'airline', 'visatype']\n",
    "    * fill null by default value from dictionnary\n",
    "* date_col = ['arrdate'(double sas format),'dtadfile'(string YYYYMMDD), 'dtaddto'(string DDMMYYYY)]\n",
    "    * 'arrdate' in SAS date format, a value represents the number of days between January 1, 1960, and a other date.\n",
    "    * cast the date and fill the null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../output/i94_apr16_staging.parquet_20200526\n"
     ]
    }
   ],
   "source": [
    "print(path_i94_immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/output/i94_apr16_staging.parquet_20200526;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o188.parquet.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/output/i94_apr16_staging.parquet_20200526;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-21646ff0f9d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimmigration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath_i94_immigration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath_i94_immigration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/output/i94_apr16_staging.parquet_20200526;'"
     ]
    }
   ],
   "source": [
    "immigration = spark.read.parquet(path+path_i94_immigration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns with a lot of null values and not useful\n",
    "drop_col = ['depdate', 'count', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', \\\n",
    "            'insnum','visapost', 'fltno', 'admnum', 'insnum']\n",
    "immigration = immigration.drop(*drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>airline</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid  i94yr  i94mon  i94cit  i94res  i94port  arrdate  i94mode  i94addr  \\\n",
       "0      0      0       0       0       0        0        0        0        0   \n",
       "\n",
       "   i94bir  i94visa  dtadfile  dtaddto  gender  airline  visatype  \n",
       "0       0        0         0        0       0        0         0  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration.createOrReplaceTempView(\"temp_immig\")\n",
    "#Count of Missing values of dataframe in pyspark \n",
    "df_immigration.select([count(when(isnan(c), c)).alias(c) for c in df_immigration.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>airline</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "      <td>152592</td>\n",
       "      <td>802</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>477</td>\n",
       "      <td>414269</td>\n",
       "      <td>83627</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid  i94yr  i94mon  i94cit  i94res  i94port  arrdate  i94mode  i94addr  \\\n",
       "0      0      0       0       0       0        0        0      239   152592   \n",
       "\n",
       "   i94bir  i94visa  dtadfile  dtaddto  gender  airline  visatype  \n",
       "0     802        0         1      477  414269    83627         0  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of null values of dataframe in pyspark \n",
    "immigration.select([count(when(col(c).isNull(), c)).alias(c) for c in immigration.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152079"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of null value in df_immigration.i94port\n",
    "immigration.filter(immigration.i94port.rlike('[A-Z]{3}')) \\\n",
    "              .filter(immigration.i94addr.isNull()) \\\n",
    "              .select(immigration.i94port, df_immigration.i94addr).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[i94port: string, i94addr: string]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean df_immigration.i94addr by df_immigration.i94port == port_state_dic.Port_id\n",
    "# port_state_dic = [{'Port_id': 'alc', 'State_id': 'ak'}]\n",
    "# Result: clean 148379 rows\n",
    "\n",
    "# create dictionnary from i94_port\n",
    "port_state_dic = dict([(i,a) for i, a in zip(i94_port.Port_id,i94_port.State_id)])\n",
    "# lamda function to get State_id\n",
    "\n",
    "user_func =  udf(lambda x: port_state_dic.get(x))\n",
    "# Take col i94addr when isNull(), replace by state_id by lambda otherwise keep null\n",
    "newdf = df_immigration.withColumn('i94addr', F.when((F.col('i94addr').isNull()),\n",
    "                                       user_func(df_immigration.i94port)).otherwise(F.col('i94addr')))\n",
    "# get number of null value in df_immigration.i94port\n",
    "newdf.filter(newdf.i94port.rlike('[A-Z]{3}')) \\\n",
    "     .filter(newdf.i94addr.isNull()) \\\n",
    "     .select(newdf.i94port, newdf.i94addr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the null value and cast the columns in integer\n",
    "# int_col = ['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94mode', 'i94bir', 'i94visa']\n",
    "null_int = {'cicid': -1, 'i94yr': -1, 'i94mon': -1,'i94cit': 239, 'i94res': 239, 'i94mode': 9, 'i94bir': -1, 'i94visa': -1}\n",
    "for k in null_int:\n",
    "        newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_int[k])\n",
    "                 .otherwise(F.col(k).cast(\"int\")))\n",
    "        \n",
    "# replace the null value for the string\n",
    "# str_cols = ['i94addr', 'i94port', 'gender', 'airline', 'visatype']\n",
    "null_str = {'i94addr': '99', 'i94port': '999', 'gender': 'U', 'airline': 'unknown', 'visatype': '99' }\n",
    "for k in null_str:\n",
    "        newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_str[k])\n",
    "                                 .otherwise(F.col(k)))\n",
    "        \n",
    "# date_col = ['arrdate'(double sas format),\n",
    "#             'dtadfile'(string YYYYMMDD), \n",
    "#             'dtaddto'(string DDMMYYYY)]\n",
    "null_date = {'arrdate': 'NA', 'dtadfile': 'NA', 'dtaddto': 'NA' }\n",
    "setup_date = udf(lambda x: (datetime(1960, 1, 1). date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "newdf = newdf.withColumn(\"arrdate\", setup_date(newdf.arrdate)) \\\n",
    "    .withColumn(\"dtadfile\",to_date(unix_timestamp(col(\"dtadfile\"),\"yyyyMMdd\").cast(\"timestamp\"))) \\\n",
    "    .withColumn(\"dtaddto\",to_date(unix_timestamp(col(\"dtaddto\"),\"MMddyyyy\").cast(\"timestamp\")))\n",
    "for k in null_date:\n",
    "        newdf = newdf.withColumn(k, F.when((F.col(k).isNull()), null_date[k])\n",
    "                                 .otherwise(F.col(k)))\n",
    "        \n",
    "newdf = newdf.withColumn('dtadfile', to_date(newdf.dtadfile, 'yyyyMMdd'))\n",
    "newdf = newdf.withColumn('dtaddto', to_date(newdf.dtadfile, 'yyyyMMdd'))\n",
    "newdf = newdf.withColumn('arrdate', to_date(newdf.dtadfile, 'yyyyMMdd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>airline</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3096313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid  i94yr  i94mon  i94cit  i94res  i94port  arrdate  i94mode  i94addr  \\\n",
       "0      0      0       0       0       0        0  3096313        0        0   \n",
       "\n",
       "   i94bir  i94visa  dtadfile  dtaddto  gender  airline  visatype  \n",
       "0       0        0   3096313  3096313       0        0         0  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf= newdf.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = (newdf.withColumnRenamed(\"cicid\", \"id_i94\") \\\n",
    "             .withColumnRenamed(\"i94yr\", \"year\") \\\n",
    "             .withColumnRenamed(\"i94mon\", \"month\") \\\n",
    "             .withColumnRenamed(\"i94cit\", \"country_born_iso\") \\\n",
    "             .withColumnRenamed(\"i94res\", \"country_res_iso\") \\\n",
    "             .withColumnRenamed(\"i94port\", \"iata_code\") \\\n",
    "             .withColumnRenamed(\"arrdate\", \"arr_date\") \\\n",
    "             .withColumnRenamed(\"i94mode\", \"arri_mode\") \\\n",
    "             .withColumnRenamed(\"i94addr\", \"state_id_arrival\") \\\n",
    "             .withColumnRenamed(\"i94bir\", \"age\") \\\n",
    "             .withColumnRenamed(\"i94visa\", \"arr_reason\") \\\n",
    "             .withColumnRenamed(\"dtadfile\", \"dt_add_i94\") \\\n",
    "             .withColumnRenamed(\"dtaddto\", \"depar_max\") \\\n",
    "             .withColumnRenamed(\"gender\", \"gender\") \\\n",
    "             .withColumnRenamed(\"airline\",\"airline\") \\\n",
    "             .withColumnRenamed(\"visatype:\", \"visatype\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_i94: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- country_born_iso: integer (nullable = true)\n",
      " |-- country_res_iso: integer (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- arr_date: date (nullable = true)\n",
      " |-- arri_mode: integer (nullable = true)\n",
      " |-- state_id_arrival: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- arr_reason: integer (nullable = true)\n",
      " |-- dt_add_i94: date (nullable = true)\n",
      " |-- depar_max: date (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration = newdf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Land Temperature Data\n",
    "\n",
    "\n",
    "\n",
    "* Path = '../../data/GlobalLandTemperaturesByCity.csv\n",
    "* There are 8599212 rows and 7 columns in *GlobalLandTemperaturesByCity.csv*.\n",
    "* Name of the dataFrame: df_temperature\n",
    "\n",
    "* As we see in [data exploration file](./0_dataset_information.ipynb), the first date is in 1743, and we find a row per day per town. So we will make aggregation for this data set and drop 'AverageTemperature' , 'Latitude' and 'Longitude' columns\n",
    "* Variables used:\n",
    "\n",
    "Column Name | Description \n",
    "-|-|\n",
    "**dt**|Date format YYYY-MM-DD| \n",
    "**AverageTemperature**|Average Temperature for the city to th date dt|\n",
    "**City**| City name| \n",
    "**Country**| Country name |\n",
    "\n",
    "\n",
    "\n",
    "* i94addr, missing 152592 values (code State US, 2 letters)\n",
    "    * fill by Port_id from the dataframe 'i94port' \n",
    "    * join on 'df_immigration.i94port == port_state_dic.Port_id', with no missing values\n",
    "    * nul value replace by State_id\n",
    "* int_col = ['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94mode', 'i94bir', 'i94visa']\n",
    "    * fill null by default value from dictionnary and cast the int_col in Integer\n",
    "* str_cols = ['i94addr', 'i94port', 'gender', 'airline', 'visatype']\n",
    "    * fill null by default value from dictionnary\n",
    "* date_col = ['arrdate'(double sas format),'dtadfile'(string YYYYMMDD), 'dtaddto'(string DDMMYYYY)]\n",
    "    * 'arrdate' in SAS date format, a value represents the number of days between January 1, 1960, and a other date.\n",
    "    * cast the date and fill the null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature = spark.read.parquet(path+path_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enbas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [UP](#enhaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_key(my): \n",
    "    for key, value in port_state_dic.items():\n",
    "        if my == key: \n",
    "            return(key, value)\n",
    "        if my == value:\n",
    "                return(key, value)\n",
    "print(get_key('XT'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i94_mode\n",
    "i94_mode = pd.read_parquet(path+'i94mode.parquet')\n",
    "#i94_mode.head(5)\n",
    "# dictionnary\n",
    "mode_dic = i94_mode.set_index(\"Mode_id\")[\"Mode\"].to_dict()\n",
    "\n",
    "#display(i94_mode.head())\n",
    "#i94_mode = pd.read_csv(path+'i94mode.csv')\n",
    "#display(mode_dic)\n",
    "#{'1': 'air', '2': 'sea', '3': 'land', '9': 'not reported'}\n",
    "\n",
    "# i94cit_i94res\n",
    "path = '../../data/'\n",
    "i94_ctry = pd.read_parquet(path+'i94cit_i94res.parquet')\n",
    "#i94_ctry.head(5)\n",
    "# dictionnary\n",
    "ctry_dic = i94_ctry.set_index(\"Country_id\")['Country'].to_dict()\n",
    "\n",
    "#display(ctry_dic)\n",
    "#{582: 'mexico',\n",
    "# 236: 'afghanistan',\n",
    "# 101: 'albania',...}\n",
    "#display(i94_ctry.head())\n",
    "\n",
    "# i94_addr\n",
    "path = '../../data/'\n",
    "i94_addr = pd.read_parquet(path+'i94addr.parquet')\n",
    "i94_addr.head(5)\n",
    "# dictionnary\n",
    "addr_dic = i94_addr.set_index(\"State_id\")['State'].to_dict()\n",
    "#display(addr_dic)\n",
    "#{'al': 'alabama',\n",
    "# 'ak': 'alaska',\n",
    "# 'az': 'arizona',...}\n",
    "#display(i94_addr.head())\n",
    "\n",
    "# i94_visa\n",
    "path = '../../data/'\n",
    "i94_visa = pd.read_parquet(path+'i94visa.parquet')\n",
    "#i94_visa.head(5)\n",
    "# dictionnary\n",
    "visa_dic = i94_visa.set_index('Code_visa')['Visa'].to_dict()\n",
    "\n",
    "#display(visa_dic)\n",
    "# {1: 'business', 2: 'pleasure', 3: 'student'}\n",
    "#display(i94_visa.head())\n",
    "\n",
    "# i94_port\n",
    "path = '../../data/'\n",
    "i94_port = pd.read_parquet(path+'i94port.parquet')\n",
    "#i94_port.head(5)\n",
    "# dictionnary\n",
    "port_dic= dict([(i,[a,b]) for i, a,b in zip(i94_port.Port_id, i94_port.Port_city,i94_port.State_id)])\n",
    "port_state_dic = dict([(i,a) for i, a in zip(i94_port.Port_id,i94_port.State_id)])\n",
    "\n",
    "# each row becomes a dictionary where key is column name and value is the data in the cell\n",
    "# [{'Port_id': 'alc', 'Port_city': 'alcan', 'State_id': 'ak'},\n",
    "# {'Port_id': 'anc', 'Port_city': 'anchorage', 'State_id': 'ak'},...]\n",
    "#port_dic = i94_port.to_dict('records')\n",
    "#display(port_dic)\n",
    "#display(i94_port.head())\n",
    "\n",
    "#display(port_state_dic)\n",
    "#display(port_dic)\n",
    "#{'alc': ['alcan', 'ak'],\n",
    "# 'anc': ['anchorage', 'ak'],\n",
    "# 'bar': ['baker aaf - baker island', 'ak'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not duplicates, the twice values are the same\n",
    "display(f\"There are {df_immigration.distinct().count()} distinc values\")\n",
    "display(f\"There are {df_immigration.count()} values\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of null values of dataframe in pyspark \n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_columns = ['cicid', 'i94yr', 'i94mon', 'i94visa', 'gender', 'airline', 'visatype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.where(col(\"i94addr\").isNull()).filter(newdf.i94port =='MIA').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of null values of dataframe in pyspark \n",
    "newdf.select([count(when(col(c).isNull(), c)).alias(c) for c in newdf.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesA = [('',1, 'lun'),('Monkey',2, 'mar'),('',3, 'mer'),('Darth Vader',4, 'jeu')]\n",
    "TableA = spark.createDataFrame(valuesA,['name','myid', 'day'])\n",
    "# Put null in empty cell\n",
    "TableA = TableA.withColumn('name', when(col('name') == '', None).otherwise(col('name')))\n",
    "\n",
    "\n",
    "valuesB = [('Rutabaga',1),('Pirate',2),('Ninja',3),('Darth Vader',4)]\n",
    "TableB = spark.createDataFrame(valuesB,['name','myid'])\n",
    " \n",
    "TableA.show()\n",
    "TableB.show()\n",
    "TableA.createOrReplaceTempView(\"TableA\")\n",
    "TableB.createOrReplaceTempView(\"TableB\")\n",
    "\n",
    "TableA.alias('A').join(TableB.alias('B'), on='myid', how='left') \\\n",
    "                 .select(\n",
    "                        'myid',\n",
    "                        'day',\n",
    "                        F.when(\n",
    "                            F.isnull(F.col('A.name')),\n",
    "                            F.col('B.name')\n",
    "                        ).otherwise(F.col('A.name')).alias('name')\n",
    "                    ) \\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Basically below line of code check all 5 SAL fields and if it is null, replace it with 0. \n",
    "If not keep the original value.\n",
    "\n",
    "df1 = df.withColumn(\"SAL1\", when(df.SAL1.isNull(), lit(0)).otherwise(df.SAL1))\\\n",
    ".withColumn(\"SAL2\", when(df.SAL2.isNull(), lit(0)).otherwise(df.SAL2))\\\n",
    ".withColumn(\"SAL3\", when(df.SAL3.isNull(), lit(0)).otherwise(df.SAL3))\\\n",
    ".withColumn(\"SAL4\", when(df.SAL4.isNull(), lit(0)).otherwise(df.SAL4))\\\n",
    ".withColumn(\"SAL5\", when(df.SAL5.isNull(), lit(0)).otherwise(df.SAL5))\\\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta.join(tb, ta.myid == tb.myid, 'left').select(tb.myid, coalesce(ta.name, tb.name)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_col = ['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94mode', 'i94bir', 'i94visa']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#I94_SAS_Labels_Description.SAS\n",
    "#def SAS_parser(file_parse, item, columns):\n",
    "import re\n",
    "import io\n",
    "\n",
    "def parse_file(path_file, key):\n",
    "    \"\"\"\n",
    "    fonction to parse file and create csv file\n",
    "    return dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    #file_parse = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "    with open(path_file, 'r') as f:\n",
    "        file = f.read()\n",
    "    sas_dict={}\n",
    "    key_name = ''\n",
    "\n",
    "    for line in file.split(\"\\n\"):\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        if '/* I94' in line :         \n",
    "            line = line.strip('/* ')\n",
    "            key_name = line.split('-')[0].replace(\"&\", \"_\").replace(\" \", \"\").strip(\" \").lower() \n",
    "            sas_dict[key_name] = []\n",
    "        elif '=' in line and key_name != '' :\n",
    "            #line_trans = re.sub(\"([A-Z]*?),(\\s*?[A-Z]{2}\\s)\",\"\\\\1=\\\\2\", line)\n",
    "            #print(line_trans)\n",
    "            sas_dict[key_name].append([item.strip(' ').strip(\" ';\") for item in line.split('=')])\n",
    "        \n",
    "\n",
    "    if key is \"i94port\":\n",
    "        #pattern = r'[^()]*\\s*\\([^()]*\\)'\n",
    "        columns = [\"Port_id\", \"Port_city\", \"State_id\"]\n",
    "        swap = sas_dict[key]          \n",
    "        sas_dict[key] = []\n",
    "        for x in swap:           \n",
    "            if \",\" in x[1]:\n",
    "                mylist=[]\n",
    "                a = x[1].rsplit(\",\", 1)\n",
    "                b = a[0]\n",
    "                c = a[1].strip()\n",
    "                mylist.extend([x[0], b, c])\n",
    "                sas_dict[key].append(item for item in mylist)\n",
    "\n",
    "                \n",
    "                \n",
    "    if key is \"i94cit_i94res\":\n",
    "        columns = [\"Country_id\", \"Country\"]\n",
    "        swap = sas_dict[key]\n",
    "        for x in swap:\n",
    "            #x[0] = int(x[0])\n",
    "            if \"mexico\" in x[1]:\n",
    "                x[1] = \"mexico\"        \n",
    "        \n",
    "    if key is \"i94mode\":\n",
    "        columns = [\"Mode_id\", \"Mode\"]\n",
    "        #swap = sas_dict[key]\n",
    "        #for x in swap:\n",
    "        #    x[0] = int(x[0])\n",
    "            \n",
    "    if key is \"i94addr\":\n",
    "        columns = [\"State_id\", \"State\"]\n",
    "        \n",
    "    if key is \"i94visa\":\n",
    "        columns = [\"Code_visa\", \"Visa\"]\n",
    "        #swap = sas_dict[key]\n",
    "        #for x in swap:\n",
    "        #    x[0] = int(x[0])\n",
    "            \n",
    "    df = \"\"           \n",
    "            \n",
    "\n",
    "    if key in sas_dict.keys():\n",
    "        if len(sas_dict[key]) > 0:\n",
    "            df = pd.DataFrame(sas_dict[key], columns = columns)\n",
    "            df.sort_values(df.columns[0], inplace=True)\n",
    "        with io.open(f\"../../data/{key}.csv\", \"w\") as f:\n",
    "            df.to_csv(f, index=False) \n",
    "           \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make i94port.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94port\"\n",
    "parse_file(path_file, key)\n",
    "\n",
    "# make i94visa.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94visa\"\n",
    "parse_file(path_file, key)\n",
    "\n",
    "# make i94addr.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94addr\"\n",
    "parse_file(path_file, key) \n",
    "\n",
    "# make i94cit_i94res.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94cit_i94res\"\n",
    "parse_file(path_file, key)\n",
    "\n",
    "# make i94mode.csv\n",
    "path_file = \"../../data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "key = \"i94mode\"\n",
    "parse_file(path_file, key)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def explore(df):\n",
    "    \"\"\"\n",
    "    Counts number of nulls and nans in each column\n",
    "    \"\"\"\n",
    "    #df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "    df1 = df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns])\n",
    "    df2 = df.select([(count(when(isnan(c) | col(c).isNull(), c))/count(lit(1))).alias(c) for c in df.columns])\n",
    "    df3 = df1.union(df2)\n",
    "    col_counts = df3.agg(*(countDistinct(col(c)).alias(c) for c in df3.columns)).collect()[0].asDict()\n",
    "    # select the cols with count=1 in an array\n",
    "    cols_to_drop = [col for col in df3.columns if col_counts[col] == 1 ]\n",
    "    # drop the selected column\n",
    "    df3.drop(*cols_to_drop).show(truncate=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = '../../data/'\n",
    "i94_visa_spark = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(path+'i94visa.csv')\n",
    "\n",
    "\n",
    "i94_addr_spark = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(path+\"i94addr.csv\")\n",
    "\n",
    "\n",
    "i94_mode_spark = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(path+\"i94mode.csv\")\n",
    "\n",
    "\n",
    "i94_cntry_spark = spark.read.format(\"csv\") \\\n",
    "                 .option(\"header\", \"true\") \\\n",
    "                 .load(path+\"i94cit_i94res.csv\") \\\n",
    "                    \n",
    "\n",
    "\n",
    "i94_port_spark = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(path+\"i94port.csv\")\n",
    "\n",
    "#todo mettre en int ce qu'il faut. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# put all in lowercase \n",
    "for col in df.columns:\n",
    "    df = df.withColumn(col, F.lower(F.col(col)))\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: ray in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (0.8.5)\n",
      "Requirement already satisfied, skipping upgrade: redis<3.5.0,>=3.3.2 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (3.6.2)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: grpcio in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (1.28.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (5.3)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.6.0 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (3.11.3)\n",
      "Requirement already satisfied, skipping upgrade: google in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: py-spy>=0.2.0 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from ray) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from aiohttp->ray) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from aiohttp->ray) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: multidict<5.0,>=4.5 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from aiohttp->ray) (4.7.5)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from aiohttp->ray) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from aiohttp->ray) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5; python_version < \"3.7\" in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from aiohttp->ray) (3.7.4.2)\n",
      "Requirement already satisfied, skipping upgrade: idna-ssl>=1.0; python_version < \"3.7\" in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from aiohttp->ray) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.2 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from grpcio->ray) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from protobuf>=3.8.0->ray) (45.2.0.post20200210)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from google->ray) (4.8.2)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from jsonschema->ray) (0.15.7)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from jsonschema->ray) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.0 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from yarl<2.0,>=1.0->aiohttp->ray) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>=1.2 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from beautifulsoup4->google->ray) (1.9.5)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/anthelix/anaconda3/envs/psyco/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema->ray) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install -U ray\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "# set AWS variables\n",
    "os.environ['AWS_ACCESS_KEY_ID']    = config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['SECRET']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope TODO\n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data TODO\n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1,2G\r\n",
      "510M airports_us.csv\r\n",
      "509M GlobalLandTemperaturesByCity.csv\r\n",
      "205M WDIData.csv\r\n",
      "5,8M airport-codes_csv.csv\r\n",
      "248K us-cities-demographics.csv\r\n",
      "144K immigration_data_sample.csv\r\n",
      " 36K I94_SAS_Labels_Descriptions.SAS\r\n",
      "4,0K ./\r\n",
      "4,0K ../\r\n",
      "4,0K 20-years-us-university-dataset/\r\n",
      "4,0K airline-delay-and-cancellation-data-2009-2018/\r\n",
      "4,0K eda-for-data-sciene-job-market-in-us/\r\n",
      "4,0K education-statistics/\r\n",
      "4,0K sas_data/\r\n"
     ]
    }
   ],
   "source": [
    "# Read in the data here\n",
    "!ls -1FSash ./dataset\n",
    "path = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UDACITY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"__________________________________________________________________________\")\n",
    "# airport-codes_csv\n",
    "df_airport_world = pd.read_csv(path+'airport-codes_csv.csv')\n",
    "# print(df_airport.head(2))\n",
    "#print(\"__________________________________________________________________________\")\n",
    "# immigration_data_sample\n",
    "df_immigration = pd.read_csv(path+'immigration_data_sample.csv')\n",
    "# print(df_immigration.head(2))\n",
    "#print(\"__________________________________________________________________________\")\n",
    "# us-cities-demographics\n",
    "df_demograph = pd.read_csv(path+'us-cities-demographics.csv', sep=\";\")\n",
    "# print(df_demograph.head(2))\n",
    "#print(\"__________________________________________________________________________\")\n",
    "# download from kaggle the GlobalLandTemperaturesByCity.csv\n",
    "df_temperature = pd.read_csv(path+'GlobalLandTemperaturesByCity.csv', sep=\",\")\n",
    "#print(df_temperature.head(2))\n",
    "#print(\"__________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sas=spark.read.parquet(\"./dataset/sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 55075 rows and 12 columns in df_airport_world\n",
      "There are 1000 rows and 29 columns in df_immigration\n",
      "There are 2891 rows and 12 columns in df_demograph\n",
      "There are 8599212 rows and 7 columns in df_temperature\n",
      "There are 3096313 rows and 28 columns in df_sas\n"
     ]
    }
   ],
   "source": [
    "nRow, nCol = df_airport_world.shape\n",
    "print(\"There are {} rows and {} columns in df_airport_world\".format(nRow, nCol))\n",
    "nRow, nCol = df_immigration.shape\n",
    "print(\"There are {} rows and {} columns in df_immigration\".format(nRow, nCol))\n",
    "nRow, nCol = df_demograph.shape\n",
    "print(\"There are {} rows and {} columns in df_demograph\".format(nRow, nCol))\n",
    "nRow, nCol = df_temperature.shape\n",
    "print(\"There are {} rows and {} columns in df_temperature\".format(nRow, nCol))\n",
    "\n",
    "print(\"There are {} rows and {} columns in df_sas\".format(df_sas.count(), len(df_sas.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KAGGLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 15 columns in df_airport_us\n"
     ]
    }
   ],
   "source": [
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "df_airport_us = pd.read_csv(path+'airports_us.csv', nrows = nRowsRead)\n",
    "nRow, nCol = df_airport_us.shape\n",
    "print(\"There are {} rows and {} columns in df_airport_us\".format(nRow, nCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 64 columns in df_indicator_dev\n"
     ]
    }
   ],
   "source": [
    "# WDIData.csv Indicators developpement\n",
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "df_indicator_dev = pd.read_csv(path+'WDIData.csv', nrows = nRowsRead)\n",
    "nRow, nCol = df_indicator_dev.shape\n",
    "print(\"There are {} rows and {} columns in df_indicator_dev\".format(nRow, nCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/education-statistics:\r\n",
      "EdStatsSeries.csv    EdStatsData.csv\t EdStatsCountry-Series.csv\r\n",
      "EdStatsFootNote.csv  EdStatsCountry.csv\r\n",
      "\r\n",
      "./dataset/eda-for-data-sciene-job-market-in-us:\r\n",
      "data-scientist-job-market-in-the-us/\r\n",
      "\r\n",
      "./dataset/eda-for-data-sciene-job-market-in-us/data-scientist-job-market-in-the-us:\r\n",
      "fulltimeRM.csv\t fulltimeSU.csv   fulltimeLA.csv  fulltimeAL.csv\r\n",
      "fulltimeSD.csv\t fulltimeBOS.csv  fulltimeMA.csv  fulltimeAT.csv\r\n",
      "fulltimeSEA.csv  fulltimeCHI.csv  fulltimeMV.csv  fulltimeBO.csv\r\n",
      "fulltimeSF.csv\t fulltimeDC.csv   fulltimeNY.csv  alldata.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -tRFh ./dataset/ed*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 613 rows and 4 columns in df_country_series\n",
      "There are 241 rows and 32 columns in df_country\n",
      "There are 1000 rows and 70 columns in df_data\n",
      "There are 1000 rows and 5 columns in df_foot_note\n",
      "There are 1000 rows and 21 columns in df_series\n"
     ]
    }
   ],
   "source": [
    "# /dataset/education-statistics\n",
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "df_country_series = pd.read_csv(path+'education-statistics/EdStatsCountry-Series.csv', nrows = nRowsRead)\n",
    "nRow, nCol = df_country_series.shape\n",
    "print(\"There are {} rows and {} columns in df_country_series\".format(nRow, nCol))\n",
    "\n",
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "df_country = pd.read_csv(path+'education-statistics/EdStatsCountry.csv', nrows = nRowsRead)\n",
    "nRow, nCol = df_country.shape\n",
    "print(\"There are {} rows and {} columns in df_country\".format(nRow, nCol))\n",
    "\n",
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "df_data = pd.read_csv(path+'education-statistics/EdStatsData.csv', nrows = nRowsRead)\n",
    "nRow, nCol = df_data.shape\n",
    "print(\"There are {} rows and {} columns in df_data\".format(nRow, nCol))\n",
    "\n",
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "df_foot_note = pd.read_csv(path+'education-statistics/EdStatsFootNote.csv', nrows = nRowsRead)\n",
    "nRow, nCol = df_foot_note.shape\n",
    "print(\"There are {} rows and {} columns in df_foot_note\".format(nRow, nCol))\n",
    "\n",
    "\n",
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "df_series = pd.read_csv(path+'education-statistics/EdStatsSeries.csv', nrows = nRowsRead)\n",
    "nRow, nCol = df_series.shape\n",
    "print(\"There are {} rows and {} columns in df_series\".format(nRow, nCol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

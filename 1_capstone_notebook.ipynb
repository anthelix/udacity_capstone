{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!{sys.executable} -m pip install -U ray\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "pd.set_option(\"display.precision\", 2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "# set AWS variables\n",
    "os.environ['AWS_ACCESS_KEY_ID']    = config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['SECRET']\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "download_url = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/nba-elo/nbaallelo.csv\"\n",
    "target_csv_path = \"nba_all_elo.csv\"\n",
    "\n",
    "response = requests.get(download_url)\n",
    "response.raise_for_status()    # Check that the request was successful\n",
    "with open(target_csv_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "print(\"Download ready.\")\n",
    ">>> import pandas as pd\n",
    ">>> nba = pd.read_csv(\"nba_all_elo.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope TODO\n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data TODO\n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1,2G\r\n",
      "510M airports_us.csv\r\n",
      "509M GlobalLandTemperaturesByCity.csv\r\n",
      "205M WDIData.csv\r\n",
      "5,8M airport-codes_csv.csv\r\n",
      "248K us-cities-demographics.csv\r\n",
      "144K immigration_data_sample.csv\r\n",
      " 36K I94_SAS_Labels_Descriptions.SAS\r\n",
      "4,0K ./\r\n",
      "4,0K ../\r\n",
      "4,0K 20-years-us-university-dataset/\r\n",
      "4,0K airline-delay-and-cancellation-data-2009-2018/\r\n",
      "4,0K education-statistics/\r\n",
      "4,0K sas_data/\r\n"
     ]
    }
   ],
   "source": [
    "# Read in the data here\n",
    "!ls -1FSash ./dataset\n",
    "#!ls -tRFh ./dataset/\n",
    "path = './dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * U.S. immigration officers\n",
    " * I-94 Form (Arrival/Departure Record) to foreign visitors \n",
    " * Explanation about the Visitor Arrivals Program (I-94 Form) [here](https://travel.trade.gov/research/programs/i94/description.asp)\n",
    " * data about arrival and why and during\n",
    " * dataset = 1 file per month\n",
    " * here: april 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "* immigration_data_sample\n",
    "    * revoir l'orignine du fichier. chercher data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat(pd.read_sas(immigration_fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "    ==> 1000 rows fourni par udac, beaucoup plus dans 1 seul mois\n",
    "    * faire un dictionnaire, recuperer les colonnes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRowsRead = None # change and set to None for the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 29 columns in dataset immigration provide by Udacity.\n"
     ]
    }
   ],
   "source": [
    "description    = \"dataset immigration provide by Udacity\"\n",
    "name           = \"df_immigration\"\n",
    "file           = \"immigration_data_sample.csv\"\n",
    "n_df           = pd.read_csv(path+file, nrows = nRowsRead)\n",
    "nRow, nCol     = n_df.shape\n",
    "print(\"There are {} rows and {} columns in {}.\".format(nRow, nCol, description))\n",
    "df_immigration = n_df\n",
    "#print(df_immigration.head(1))\n",
    "#print(df_immigration.info())\n",
    "\n",
    "\n",
    "#dic_6 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Dictionary\n",
    "\n",
    "Column Name | Description | Example | Type\n",
    "-|-|-|-|\n",
    "cicid|     ID uniq per record in the dataset | 4.08e+06 | float64\n",
    "i94yr|     4 digit year  | 2016.0 | float64\n",
    "i94mon|    Numeric month |  4.0 | float64\n",
    "i94ci|     3 digit code of source city for immigration (Born country) | 209.0 | float64\n",
    "i94res|    3 digit code of source country for immigration (Residence country) | 209.0 | float64\n",
    "i94port|   Port addmitted through | HHW | object\n",
    "arrdate|   Arrival date in the USA | 20566.0 | float64\n",
    "i94mode|   Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) | 1.0 | float\n",
    "i94addr|   State of arrival | HI | object\n",
    "depdate|   Departure date | 20573.0 | float\n",
    "i94bir|    Age in years | 61.0 | float\n",
    "i94visa|   Visa Code - 1 = Business / 2 = Pleasure / 3 = Student |2.0 | float\n",
    "count|     Used for summary statistics | 1.0 | float\n",
    "dtadfile|  Character Date Field |20160422| int 64 \n",
    "visapost|  Department of State where where Visa was issued | | object\n",
    "occup|     Occupation that will be performed in U.S. || object\n",
    "entdepa|   Arrival Flag - Whether admitted or paroled into the US |G| object\n",
    "entdepd|   Departure Flag. Whether departed, lost visa, or deceased |O|  object\n",
    "entdepu|   Update Flag - Either apprehended, overstayed, adjusted to perm residence || float64\n",
    "matflag|   Match flag |M|  object\n",
    "biryear|   4 digit year of birth |1955.0| float64\n",
    "dtaddto|   Character date field to when admitted in the US |07202016| object\n",
    "gender|    Gender|M| object\n",
    "insnum|    INS number || float64\n",
    "airline|   Airline used to arrive in U.S.|JL| object\n",
    "admnum|    Admission number, should be unique and not nullable |5.66e+10| float\n",
    "fltno|     Flight number of Airline used to arrive in U.S. |00782| object\n",
    "visatype|  Class of admission legally admitting the non-immigrant to temporarily stay in U.S.|WT|object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3606803 rows and 15 columns in DataFrame airports_us.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin_airport</th>\n",
       "      <th>Destination_airport</th>\n",
       "      <th>Origin_city</th>\n",
       "      <th>Destination_city</th>\n",
       "      <th>Passengers</th>\n",
       "      <th>Seats</th>\n",
       "      <th>Flights</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Fly_date</th>\n",
       "      <th>Origin_population</th>\n",
       "      <th>Destination_population</th>\n",
       "      <th>Org_airport_lat</th>\n",
       "      <th>Org_airport_long</th>\n",
       "      <th>Dest_airport_lat</th>\n",
       "      <th>Dest_airport_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MHK</td>\n",
       "      <td>AMW</td>\n",
       "      <td>Manhattan, KS</td>\n",
       "      <td>Ames, IA</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>254</td>\n",
       "      <td>2008-10-01</td>\n",
       "      <td>122049</td>\n",
       "      <td>86219</td>\n",
       "      <td>39.14</td>\n",
       "      <td>-96.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EUG</td>\n",
       "      <td>RDM</td>\n",
       "      <td>Eugene, OR</td>\n",
       "      <td>Bend, OR</td>\n",
       "      <td>41</td>\n",
       "      <td>396</td>\n",
       "      <td>22</td>\n",
       "      <td>103</td>\n",
       "      <td>1990-11-01</td>\n",
       "      <td>284093</td>\n",
       "      <td>76034</td>\n",
       "      <td>44.12</td>\n",
       "      <td>-123.21</td>\n",
       "      <td>44.25</td>\n",
       "      <td>-121.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EUG</td>\n",
       "      <td>RDM</td>\n",
       "      <td>Eugene, OR</td>\n",
       "      <td>Bend, OR</td>\n",
       "      <td>88</td>\n",
       "      <td>342</td>\n",
       "      <td>19</td>\n",
       "      <td>103</td>\n",
       "      <td>1990-12-01</td>\n",
       "      <td>284093</td>\n",
       "      <td>76034</td>\n",
       "      <td>44.12</td>\n",
       "      <td>-123.21</td>\n",
       "      <td>44.25</td>\n",
       "      <td>-121.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EUG</td>\n",
       "      <td>RDM</td>\n",
       "      <td>Eugene, OR</td>\n",
       "      <td>Bend, OR</td>\n",
       "      <td>11</td>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>103</td>\n",
       "      <td>1990-10-01</td>\n",
       "      <td>284093</td>\n",
       "      <td>76034</td>\n",
       "      <td>44.12</td>\n",
       "      <td>-123.21</td>\n",
       "      <td>44.25</td>\n",
       "      <td>-121.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MFR</td>\n",
       "      <td>RDM</td>\n",
       "      <td>Medford, OR</td>\n",
       "      <td>Bend, OR</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>147300</td>\n",
       "      <td>76034</td>\n",
       "      <td>42.37</td>\n",
       "      <td>-122.87</td>\n",
       "      <td>44.25</td>\n",
       "      <td>-121.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Origin_airport Destination_airport    Origin_city Destination_city  \\\n",
       "0            MHK                 AMW  Manhattan, KS         Ames, IA   \n",
       "1            EUG                 RDM     Eugene, OR         Bend, OR   \n",
       "2            EUG                 RDM     Eugene, OR         Bend, OR   \n",
       "3            EUG                 RDM     Eugene, OR         Bend, OR   \n",
       "4            MFR                 RDM    Medford, OR         Bend, OR   \n",
       "\n",
       "   Passengers  Seats  Flights  Distance    Fly_date  Origin_population  \\\n",
       "0          21     30        1       254  2008-10-01             122049   \n",
       "1          41    396       22       103  1990-11-01             284093   \n",
       "2          88    342       19       103  1990-12-01             284093   \n",
       "3          11     72        4       103  1990-10-01             284093   \n",
       "4           0     18        1       156  1990-02-01             147300   \n",
       "\n",
       "   Destination_population  Org_airport_lat  Org_airport_long  \\\n",
       "0                   86219            39.14            -96.67   \n",
       "1                   76034            44.12           -123.21   \n",
       "2                   76034            44.12           -123.21   \n",
       "3                   76034            44.12           -123.21   \n",
       "4                   76034            42.37           -122.87   \n",
       "\n",
       "   Dest_airport_lat  Dest_airport_long  \n",
       "0               NaN                NaN  \n",
       "1             44.25            -121.15  \n",
       "2             44.25            -121.15  \n",
       "3             44.25            -121.15  \n",
       "4             44.25            -121.15  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# airports_us.csv KAGGLE\n",
    "description   = \" Dataset from KAGGLE, usefule to follow where go aliens\"\n",
    "name          = \"df_airport_us\"\n",
    "file          = \"airports_us.csv\"\n",
    "\n",
    "n_df          = pd.read_csv(path+file, nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, file))\n",
    "df_airport_us = n_df\n",
    "df_airport_us.head()\n",
    "\n",
    "#dic_1 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8599212 rows and 7 columns in DataFrame df_temp.\n"
     ]
    }
   ],
   "source": [
    "# download from kaggle the GlobalLandTemperaturesByCity.csv KAGGLE/UDACITY\n",
    "description   = \"download from kaggle the GlobalLandTemperaturesByCity.csv KAGGLE/UDACITY\"\n",
    "name          = \"df_temp\"\n",
    "file          = \"GlobalLandTemperaturesByCity.csv\"\n",
    "\n",
    "n_df          = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_temp       = n_df\n",
    "\n",
    "dic_2 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 64 columns in DataFrame df_indicator_dev.\n"
     ]
    }
   ],
   "source": [
    "# WDIData.csv Indicators developpement KAGGLE\n",
    "description      = \"WDIData.csv country Indicators developpment KAGGLE\"\n",
    "name             = \"df_indicator_dev\"\n",
    "file             = \"WDIData.csv\"\n",
    "\n",
    "n_df             = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol       = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_indicator_dev = n_df\n",
    "\n",
    "dic_3 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "# dic_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 12 columns in DataFrame df_airport_world.\n"
     ]
    }
   ],
   "source": [
    "# airport-codes_csv UDACITY\n",
    "description      = \"airport-codes_csv provide by UDACITY\"\n",
    "name             = \"df_airport_world\"\n",
    "file             = \"airport-codes_csv.csv\"\n",
    "\n",
    "n_df             = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol       = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_airport_world = n_df\n",
    "\n",
    "dic_4 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 12 columns in DataFrame df_demograph.\n"
     ]
    }
   ],
   "source": [
    "# us-cities-demographics USACITY\n",
    "description   = \"us-cities-demographics provide by UDACITY\"\n",
    "name          = \"df_demograph\"\n",
    "file          = \"us-cities-demographics.csv\"\n",
    "\n",
    "n_df          = pd.read_csv(path+file, sep=\";\", nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_demograph  = n_df\n",
    "\n",
    "dic_5 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-fe2b6f915f96>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-fe2b6f915f96>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    description   = <>\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#I94 DESCRIPTION\n",
    "description   = \"I94_SAS_Labels_Descriptions.SAS\"\n",
    "name          = \"df_label_I94\"\n",
    "file          = \"I94_SAS_Labels_Descriptions.SAS\"\n",
    "\n",
    "n_df          = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_<>       = n_df\n",
    "\n",
    "dic_<> = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "dic_<>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3096313 rows and 28 columns in DataFrame df_sas.\n"
     ]
    }
   ],
   "source": [
    "# SAS\n",
    "description   = \" Some parquet files in directory 'sas_data'\"\n",
    "name          = \"df_sas\"\n",
    "file          = \"./dataset/sas_data\"\n",
    "\n",
    "n_df          = spark.read.parquet(\"./dataset/sas_data\")\n",
    "nRow, nCol    = n_df.count(), len(n_df.columns)\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_sas        = n_df\n",
    "\n",
    "dic_8 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./dataset/20-years-us-university-dataset\n",
    "# decide to not use it for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 28 columns in DataFrame df_airline_delay.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ./dataset/airline-delay-and-cancellation-data-2009-2018/2016.csv\n",
    "description   = \".Data about us flight in 2016\"\n",
    "name          = \"df_airline_delay\"\n",
    "file          = \"airline-delay-and-cancellation-data-2009-2018/2016.csv\"\n",
    "\n",
    "n_df          = pd.read_csv(path+file, nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_airline_delay       = n_df\n",
    "\n",
    "dic_10 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_10\n",
    "type(dic_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-30f9d1c4f2db>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-30f9d1c4f2db>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    description   =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list_dic = [dic_1, dic_2, dic_3, dic_4, dic_5, dic_6, dic_8, dic_9, dic_10]\n",
    "\n",
    "description   =\n",
    "name          =\n",
    "file          =\n",
    "\n",
    "n_df          = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_temp       = n_df\n",
    "\n",
    "dic_n = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "dic_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 613 rows and 4 columns in DataFrame df_Educ_country_series.\n",
      "There are 241 rows and 32 columns in DataFrame df_Educ_country.\n",
      "There are 1000 rows and 70 columns in DataFrame df_Educ_data.\n",
      "There are 1000 rows and 5 columns in DataFrame df_Educ_foot_note.\n",
      "There are 1000 rows and 21 columns in DataFrame df_Educ_series.\n"
     ]
    }
   ],
   "source": [
    "# /dataset/education-statistics \n",
    "\n",
    "description           = \"Data from education-statistics\"\n",
    "name                  = \"df_Educ_country_series\"\n",
    "file                  = \"education-statistics/EdStatsCountry-Series.csv\"\n",
    "n_df                  = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol            = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_country_series = n_df\n",
    "dic_11 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_11\n",
    "\n",
    "description   = \"Data from education-statistics\"\n",
    "name          = \"df_Educ_country\"\n",
    "file          = \"education-statistics/EdStatsCountry.csv\"\n",
    "n_df          = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_country = n_df\n",
    "dic_12 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_12\n",
    "\n",
    "description   = \"Data from education-statistics\"\n",
    "name          = \"df_Educ_data\"\n",
    "file          = \"education-statistics/EdStatsData.csv\"\n",
    "n_df          = pd.read_csv(path+file, nrows = nRowsRead)\n",
    "nRow, nCol    = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_data  = n_df\n",
    "dic_13 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_13\n",
    "\n",
    "description       = \"Data from education-statistics\"\n",
    "name              = \"df_Educ_foot_note\"\n",
    "file              = \"education-statistics/EdStatsFootNote.csv\"\n",
    "n_df              = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol        = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_foot_note = n_df\n",
    "dic_14 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_14\n",
    "\n",
    "description    = \"Data from education-statistics\"\n",
    "name           = \"df_Educ_series\"\n",
    "file           = \"education-statistics/EdStatsSeries.csv\"\n",
    "n_df           = pd.read_csv(path+file, sep=\",\", nrows = nRowsRead)\n",
    "nRow, nCol     = n_df.shape\n",
    "print(\"There are {} rows and {} columns in DataFrame {}.\".format(nRow, nCol, name))\n",
    "df_Educ_series = n_df\n",
    "dic_15 = {'name': name, 'path': (path+file), 'nLines': nRow, 'nColomn': nCol, 'Description': description}\n",
    "#dic_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "list_dic1 = [dic_1, dic_2, dic_3, dic_4, dic_5, dic_6, dic_8, dic_10, dic_11, dic_12, dic_13]\n",
    "for dic in list_dic1:\n",
    "    print(type(dic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dic_1',\n",
       " 'dic_10',\n",
       " 'dic_11',\n",
       " 'dic_12',\n",
       " 'dic_13',\n",
       " 'dic_14',\n",
       " 'dic_15',\n",
       " 'dic_2',\n",
       " 'dic_3',\n",
       " 'dic_4',\n",
       " 'dic_5',\n",
       " 'dic_6',\n",
       " 'dic_8']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%xdel n_df\n",
    "%who_ls dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%whos DataFrame\n",
    "all_df = %who_ls DataFrame\n",
    "all_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_sas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_indicator_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_country_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_foot_note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_df_demograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_airport_world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_airport_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_airline_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df_airline_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_airline_delay.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_airline_delay.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_delay.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
